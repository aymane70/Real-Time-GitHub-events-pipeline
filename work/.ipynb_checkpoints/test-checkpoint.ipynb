{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edf96030-ac2f-4bae-9c21-4d5b8a02254c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import praw \n",
    "import json\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"testing my skills\") \\\n",
    "    .config(\n",
    "        \"spark.jars.packages\",\n",
    "        \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,\"\n",
    "        \"org.apache.kafka:kafka-clients:3.5.1,\"\n",
    "        \"org.apache.spark:spark-token-provider-kafka-0-10_2.12:3.5.0,\"\n",
    "        \"org.apache.commons:commons-pool2:2.11.1,\"\n",
    "        \"org.postgresql:postgresql:42.6.0\"\n",
    "    ) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"xqPYLpWNQmYN92tOv5-BYQ\",\n",
    "    client_secret=\"ZCWInQM9d_nJHSUwNoO0duBpFbRiSw\",\n",
    "    user_agent=\"aymane69\"\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e937f297-dd18-4472-977e-ebe224f9d0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    reading streams of data from kafka producer ,\n",
    "    \n",
    "\"\"\"\n",
    "from kafka import KafkaProducer\n",
    "import json\n",
    "\n",
    "\n",
    "producer = KafkaProducer(\n",
    "                bootstrap_servers='kafka:9092',\n",
    "                value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    "                )\n",
    "\n",
    "subreddit=reddit.subreddit(\"dataengineering\")\n",
    "\n",
    "list=[]\n",
    "for p in subreddit.hot(limit=1000):\n",
    "    item={\n",
    "        \"id\": p.id ,\n",
    "        \"name\": p.name,\n",
    "        \"content\": p.selftext ,\n",
    "        \"date_utc\": p.created_utc ,\n",
    "        \"title\": p.title ,\n",
    "        \"upvote_ratio\": p.upvote_ratio ,\n",
    "        \"score\": p.score,\n",
    "        \"url\": p.url\n",
    "    }\n",
    "    list.append(item)\n",
    "    producer.send(\"reddit\",list)\n",
    "\n",
    "df = spark.readStream.format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"reddit\") \\\n",
    "    .load()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eff7942b-24fa-4f92-8143-be8e90624aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits=[\"news\",\n",
    "            \"worldnews\",\n",
    "            \"politics\",\n",
    "            \"UpliftingNews\",\n",
    "            \"technology\",\n",
    "            \"programming\",\n",
    "            \"science\",\n",
    "            \"books\",\n",
    "            \"dataengineering\"]\n",
    "\n",
    "for sub in subreddits : \n",
    "    subreddit=reddit.subreddit(sub)\n",
    "    posts=[]\n",
    "    for post in subreddit.hot(limit=500):\n",
    "        item={\n",
    "            \"id\": post.id ,\n",
    "            \"name\": post.name,\n",
    "            \"content\": post.selftext ,\n",
    "            \"date_utc\": post.created_utc ,\n",
    "            \"title\": post.title ,\n",
    "            \"upvote_ratio\": post.upvote_ratio ,\n",
    "            \"score\": post.score,\n",
    "            \"url\": post.url\n",
    "            }\n",
    "        posts.append(item)\n",
    "    \n",
    "    with open(f\"{sub}.json\",\"w\") as f :\n",
    "        json.dump(posts,f)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30dc9010-33d2-4c13-b11f-1186e187bcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_unixtime \n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dfs = {}\n",
    "for sub in subreddits: \n",
    "    df = spark.read \\\n",
    "        .format(\"json\") \\\n",
    "        .load(f\"{sub}.json\") \\\n",
    "        .withColumn(\"created_date\", from_unixtime(col(\"date_utc\"))) \\\n",
    "        .drop(col(\"date_utc\"))\n",
    "\n",
    "    dfs[sub] = df  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb8567cf-ffa1-45e2-b2d8-0e9eb951cdda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>score</th>\n",
       "      <th>title</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>url</th>\n",
       "      <th>created_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This thread is a place where you can share thi...</td>\n",
       "      <td>1olrgye</td>\n",
       "      <td>t3_1olrgye</td>\n",
       "      <td>5</td>\n",
       "      <td>Monthly General Discussion - Nov 2025</td>\n",
       "      <td>0.86</td>\n",
       "      <td>https://www.reddit.com/r/dataengineering/comme...</td>\n",
       "      <td>2025-11-01 16:00:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://preview.redd.it/ia7kdykk8dlb1.png?widt...</td>\n",
       "      <td>1n5slp3</td>\n",
       "      <td>t3_1n5slp3</td>\n",
       "      <td>34</td>\n",
       "      <td>Quarterly Salary Discussion - Sep 2025</td>\n",
       "      <td>0.91</td>\n",
       "      <td>https://www.reddit.com/r/dataengineering/comme...</td>\n",
       "      <td>2025-09-01 16:00:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Happy to get help from experts in the house.</td>\n",
       "      <td>1ozegd4</td>\n",
       "      <td>t3_1ozegd4</td>\n",
       "      <td>10</td>\n",
       "      <td>What is your current Enterprise Cloud Storage ...</td>\n",
       "      <td>0.92</td>\n",
       "      <td>https://www.reddit.com/r/dataengineering/comme...</td>\n",
       "      <td>2025-11-17 12:02:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hey! I’m curious how other teams manage feedba...</td>\n",
       "      <td>1ozdbcx</td>\n",
       "      <td>t3_1ozdbcx</td>\n",
       "      <td>6</td>\n",
       "      <td>How do your teams handle UAT + releases for ne...</td>\n",
       "      <td>0.88</td>\n",
       "      <td>https://www.reddit.com/r/dataengineering/comme...</td>\n",
       "      <td>2025-11-17 10:57:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I work for a small food and bev company. 200mm...</td>\n",
       "      <td>1oz6ckb</td>\n",
       "      <td>t3_1oz6ckb</td>\n",
       "      <td>22</td>\n",
       "      <td>Director of IT or DE</td>\n",
       "      <td>0.89</td>\n",
       "      <td>https://www.reddit.com/r/dataengineering/comme...</td>\n",
       "      <td>2025-11-17 03:56:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>Hi, I'm new to databricks (Please go easy) and...</td>\n",
       "      <td>1o88lg6</td>\n",
       "      <td>t3_1o88lg6</td>\n",
       "      <td>1</td>\n",
       "      <td>Suggestion needed with Medallion Architecture</td>\n",
       "      <td>0.56</td>\n",
       "      <td>https://www.reddit.com/r/dataengineering/comme...</td>\n",
       "      <td>2025-10-16 15:04:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td></td>\n",
       "      <td>1o88isc</td>\n",
       "      <td>t3_1o88isc</td>\n",
       "      <td>1</td>\n",
       "      <td>Every company is a data company, but most don'...</td>\n",
       "      <td>0.60</td>\n",
       "      <td>https://taleshape.com/blog/getting-started-bui...</td>\n",
       "      <td>2025-10-16 15:01:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td></td>\n",
       "      <td>1o85f8n</td>\n",
       "      <td>t3_1o85f8n</td>\n",
       "      <td>0</td>\n",
       "      <td>Intelligent Applications: The Next Step in Dat...</td>\n",
       "      <td>0.43</td>\n",
       "      <td>https://news.sap.com/2025/10/intelligent-appli...</td>\n",
       "      <td>2025-10-16 12:56:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>Hi everyone! I'm doing data collection for a c...</td>\n",
       "      <td>1o7tcyz</td>\n",
       "      <td>t3_1o7tcyz</td>\n",
       "      <td>6</td>\n",
       "      <td>Data Collecting</td>\n",
       "      <td>0.88</td>\n",
       "      <td>https://www.reddit.com/r/dataengineering/comme...</td>\n",
       "      <td>2025-10-16 01:30:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>I run a huge data product that gives informati...</td>\n",
       "      <td>1o83jwp</td>\n",
       "      <td>t3_1o83jwp</td>\n",
       "      <td>0</td>\n",
       "      <td>Best way to run a detailed global market model...</td>\n",
       "      <td>0.40</td>\n",
       "      <td>https://www.reddit.com/r/dataengineering/comme...</td>\n",
       "      <td>2025-10-16 11:25:08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               content       id        name  \\\n",
       "0    This thread is a place where you can share thi...  1olrgye  t3_1olrgye   \n",
       "1    https://preview.redd.it/ia7kdykk8dlb1.png?widt...  1n5slp3  t3_1n5slp3   \n",
       "2         Happy to get help from experts in the house.  1ozegd4  t3_1ozegd4   \n",
       "3    Hey! I’m curious how other teams manage feedba...  1ozdbcx  t3_1ozdbcx   \n",
       "4    I work for a small food and bev company. 200mm...  1oz6ckb  t3_1oz6ckb   \n",
       "..                                                 ...      ...         ...   \n",
       "495  Hi, I'm new to databricks (Please go easy) and...  1o88lg6  t3_1o88lg6   \n",
       "496                                                     1o88isc  t3_1o88isc   \n",
       "497                                                     1o85f8n  t3_1o85f8n   \n",
       "498  Hi everyone! I'm doing data collection for a c...  1o7tcyz  t3_1o7tcyz   \n",
       "499  I run a huge data product that gives informati...  1o83jwp  t3_1o83jwp   \n",
       "\n",
       "     score                                              title  upvote_ratio  \\\n",
       "0        5              Monthly General Discussion - Nov 2025          0.86   \n",
       "1       34             Quarterly Salary Discussion - Sep 2025          0.91   \n",
       "2       10  What is your current Enterprise Cloud Storage ...          0.92   \n",
       "3        6  How do your teams handle UAT + releases for ne...          0.88   \n",
       "4       22                               Director of IT or DE          0.89   \n",
       "..     ...                                                ...           ...   \n",
       "495      1      Suggestion needed with Medallion Architecture          0.56   \n",
       "496      1  Every company is a data company, but most don'...          0.60   \n",
       "497      0  Intelligent Applications: The Next Step in Dat...          0.43   \n",
       "498      6                                    Data Collecting          0.88   \n",
       "499      0  Best way to run a detailed global market model...          0.40   \n",
       "\n",
       "                                                   url         created_date  \n",
       "0    https://www.reddit.com/r/dataengineering/comme...  2025-11-01 16:00:49  \n",
       "1    https://www.reddit.com/r/dataengineering/comme...  2025-09-01 16:00:36  \n",
       "2    https://www.reddit.com/r/dataengineering/comme...  2025-11-17 12:02:07  \n",
       "3    https://www.reddit.com/r/dataengineering/comme...  2025-11-17 10:57:19  \n",
       "4    https://www.reddit.com/r/dataengineering/comme...  2025-11-17 03:56:24  \n",
       "..                                                 ...                  ...  \n",
       "495  https://www.reddit.com/r/dataengineering/comme...  2025-10-16 15:04:36  \n",
       "496  https://taleshape.com/blog/getting-started-bui...  2025-10-16 15:01:50  \n",
       "497  https://news.sap.com/2025/10/intelligent-appli...  2025-10-16 12:56:37  \n",
       "498  https://www.reddit.com/r/dataengineering/comme...  2025-10-16 01:30:27  \n",
       "499  https://www.reddit.com/r/dataengineering/comme...  2025-10-16 11:25:08  \n",
       "\n",
       "[500 rows x 8 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs[\"dataengineering\"].toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5b76e2e-3c45-4b87-baf0-55ca469437cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[postgres_config]\n",
    "host = \"postgres\"\n",
    "port = 5432\n",
    "username = \"reddit_user\"\n",
    "password = \"reddit_pass\"\n",
    "database = \"reddit_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a14a544b-8c47-4768-a9d0-8df4f72e301f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2 \n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    \"dbname=\" + database\n",
    "    + \" user=\" + username\n",
    "    + \" password=\" + password\n",
    "    + \" host=\" + host,\n",
    "    port = port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bca76276-f7c7-4b5e-ad22-d00d4b791b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "subreddits=[\"news\",\n",
    "            \"worldnews\",\n",
    "            \"politics\",\n",
    "            \"UpliftingNews\",\n",
    "            \"technology\",\n",
    "            \"programming\",\n",
    "            \"science\",\n",
    "            \"books\",\n",
    "            \"dataengineering\"]\n",
    "\n",
    "for sub in subreddits : \n",
    "    m_query = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {sub} (\n",
    "        id VARCHAR(20) PRIMARY KEY,\n",
    "        name VARCHAR(40),\n",
    "        content VARCHAR(500),\n",
    "        title VARCHAR(200),\n",
    "        upvote_ratio FLOAT,\n",
    "        score INT,\n",
    "        url VARCHAR(200),\n",
    "        created_date TIMESTAMP\n",
    "    )\n",
    "    \"\"\"\n",
    "    \n",
    "    m_cursor = conn.cursor()\n",
    "    m_cursor.execute(m_query)\n",
    "    conn.commit()\n",
    "    m_cursor.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "023d4ca1-cc30-42b2-8819-3aa3a3d343c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o152.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 11.0 failed 1 times, most recent failure: Lost task 0.0 in stage 11.0 (TID 11) (56fee3ae383c executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO news (\"content\",\"id\",\"name\",\"score\",\"title\",\"upvote_ratio\",\"url\",\"created_date\") VALUES ('','1ozfysl','t3_1ozfysl',2231,'Gunmen abduct 25 girls from a high school in Nigeria and kill one of the staff',0.97,'https://apnews.com/article/nigeria-girls-abducted-school-9d288758bf1a57f3390c962f0a68596b?utm_source=onesignal&utm_medium=push&utm_campaign=2025-11-17-Girls+kidnapped','2025-11-17 13:14:56') was aborted: ERROR: column \"created_date\" is of type timestamp without time zone but expression is of type character varying\n  Hint: You will need to rewrite or cast the expression.\n  Position: 123  Call getNextException to see other errors in the batch.\n\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:880)\n\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:919)\n\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1685)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.postgresql.util.PSQLException: ERROR: column \"created_date\" is of type timestamp without time zone but expression is of type character varying\n  Hint: You will need to rewrite or cast the expression.\n  Position: 123\n\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2713)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2401)\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:368)\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:327)\n\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:877)\n\t... 19 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1036)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1034)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:901)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.sql.BatchUpdateException: Batch entry 0 INSERT INTO news (\"content\",\"id\",\"name\",\"score\",\"title\",\"upvote_ratio\",\"url\",\"created_date\") VALUES ('','1ozfysl','t3_1ozfysl',2231,'Gunmen abduct 25 girls from a high school in Nigeria and kill one of the staff',0.97,'https://apnews.com/article/nigeria-girls-abducted-school-9d288758bf1a57f3390c962f0a68596b?utm_source=onesignal&utm_medium=push&utm_campaign=2025-11-17-Girls+kidnapped','2025-11-17 13:14:56') was aborted: ERROR: column \"created_date\" is of type timestamp without time zone but expression is of type character varying\n  Hint: You will need to rewrite or cast the expression.\n  Position: 123  Call getNextException to see other errors in the batch.\n\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:880)\n\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:919)\n\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1685)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\nCaused by: org.postgresql.util.PSQLException: ERROR: column \"created_date\" is of type timestamp without time zone but expression is of type character varying\n  Hint: You will need to rewrite or cast the expression.\n  Position: 123\n\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2713)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2401)\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:368)\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:327)\n\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:877)\n\t... 19 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m table_name, df \u001b[38;5;129;01min\u001b[39;00m dfs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      2\u001b[0m     \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjdbc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjdbc:postgresql://postgres:5432/reddit_db\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdbtable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreddit_user\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpassword\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreddit_pass\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdriver\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.postgresql.Driver\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m---> 10\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:1461\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1459\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m   1460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1461\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1463\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave(path)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o152.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 11.0 failed 1 times, most recent failure: Lost task 0.0 in stage 11.0 (TID 11) (56fee3ae383c executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO news (\"content\",\"id\",\"name\",\"score\",\"title\",\"upvote_ratio\",\"url\",\"created_date\") VALUES ('','1ozfysl','t3_1ozfysl',2231,'Gunmen abduct 25 girls from a high school in Nigeria and kill one of the staff',0.97,'https://apnews.com/article/nigeria-girls-abducted-school-9d288758bf1a57f3390c962f0a68596b?utm_source=onesignal&utm_medium=push&utm_campaign=2025-11-17-Girls+kidnapped','2025-11-17 13:14:56') was aborted: ERROR: column \"created_date\" is of type timestamp without time zone but expression is of type character varying\n  Hint: You will need to rewrite or cast the expression.\n  Position: 123  Call getNextException to see other errors in the batch.\n\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:880)\n\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:919)\n\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1685)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.postgresql.util.PSQLException: ERROR: column \"created_date\" is of type timestamp without time zone but expression is of type character varying\n  Hint: You will need to rewrite or cast the expression.\n  Position: 123\n\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2713)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2401)\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:368)\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:327)\n\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:877)\n\t... 19 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1036)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1034)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:901)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.sql.BatchUpdateException: Batch entry 0 INSERT INTO news (\"content\",\"id\",\"name\",\"score\",\"title\",\"upvote_ratio\",\"url\",\"created_date\") VALUES ('','1ozfysl','t3_1ozfysl',2231,'Gunmen abduct 25 girls from a high school in Nigeria and kill one of the staff',0.97,'https://apnews.com/article/nigeria-girls-abducted-school-9d288758bf1a57f3390c962f0a68596b?utm_source=onesignal&utm_medium=push&utm_campaign=2025-11-17-Girls+kidnapped','2025-11-17 13:14:56') was aborted: ERROR: column \"created_date\" is of type timestamp without time zone but expression is of type character varying\n  Hint: You will need to rewrite or cast the expression.\n  Position: 123  Call getNextException to see other errors in the batch.\n\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:880)\n\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:919)\n\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1685)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\nCaused by: org.postgresql.util.PSQLException: ERROR: column \"created_date\" is of type timestamp without time zone but expression is of type character varying\n  Hint: You will need to rewrite or cast the expression.\n  Position: 123\n\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2713)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2401)\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:368)\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:327)\n\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:877)\n\t... 19 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import DecimalType, TimestampType\n",
    "\n",
    "for table_name, df in dfs.items():\n",
    "    # Ensure types match PostgreSQL\n",
    "    df_fixed = df.withColumn(\"upvote_ratio\", df[\"upvote_ratio\"].cast(DecimalType(10,3))) \\\n",
    "                 .withColumn(\"created_date\", df[\"created_date\"].cast(TimestampType()))\n",
    "    \n",
    "    # Optional: replace nulls if upvote_ratio or other non-nullable columns\n",
    "    df_fixed = df_fixed.fillna({\"upvote_ratio\": 0.0, \"score\": 0, \"content\": \"\", \"title\": \"\", \"url\": \"\"})\n",
    "    \n",
    "    # Write to PostgreSQL\n",
    "    df_fixed.write \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", \"jdbc:postgresql://postgres:5432/reddit_db\") \\\n",
    "        .option(\"dbtable\", table_name) \\\n",
    "        .option(\"user\", \"reddit_user\") \\\n",
    "        .option(\"password\", \"reddit_pass\") \\\n",
    "        .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248a240c-7177-44ca-9321-9cefbfd3f117",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
