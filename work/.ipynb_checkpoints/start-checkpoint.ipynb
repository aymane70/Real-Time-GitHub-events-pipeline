{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43e04a81-a3a9-4343-ab32-1bb831b63d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"GitHubEventsPipeline\") \\\n",
    "    .config(\n",
    "        \"spark.jars.packages\",\n",
    "        \",\".join([\n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0\",\n",
    "            \"org.apache.kafka:kafka-clients:3.5.1\",\n",
    "            \"org.apache.spark:spark-token-provider-kafka-0-10_2.12:3.5.0\",\n",
    "            \"org.apache.commons:commons-pool2:2.11.1\",\n",
    "            \"org.postgresql:postgresql:42.6.0\"\n",
    "        ])\n",
    "    ) \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a368059-fc4e-41bb-9543-8c0116290dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark\\\n",
    "      .readStream \\\n",
    "      .format(\"kafka\") \\\n",
    "      .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "      .option(\"subscribe\", \"git_logs\") \\\n",
    "      .option(\"startingOffsets\", \"latest\") \\\n",
    "      .load()\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e62ffd9-3d07-4054-904a-a90ba8abfb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import col, from_json, to_timestamp\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# 3. Define schema for events\n",
    "schema = StructType([\n",
    "    StructField(\"id\", StringType()),\n",
    "    StructField(\"type\", StringType()),\n",
    "    StructField(\"repo\", StructType([\n",
    "        StructField(\"name\", StringType())\n",
    "    ])),\n",
    "    StructField(\"actor\", StructType([\n",
    "        StructField(\"login\", StringType())\n",
    "    ])),\n",
    "    StructField(\"created_at\", StringType())\n",
    "])\n",
    "\n",
    "# 4. Parse JSON and rename fields to match Postgres table\n",
    "parsed = df.select(\n",
    "    from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\")\n",
    ").withColumn(\"created_at\", to_timestamp(\"data.created_at\")).select(\n",
    "    col(\"data.id\").alias(\"id\"),\n",
    "    col(\"data.type\").alias(\"type\"),\n",
    "    col(\"data.repo.name\").alias(\"repo_name\"),\n",
    "    col(\"data.actor.login\").alias(\"actor_login\"),\n",
    "    col(\"created_at\")\n",
    ")\n",
    "\n",
    "# 5. Function to write each microbatch to Postgres\n",
    "def write_to_postgres(batch_df, batch_id):\n",
    "    batch_df.write \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", \"jdbc:postgresql://postgres:5432/reddit_db\") \\\n",
    "        .option(\"dbtable\", \"github_events\") \\\n",
    "        .option(\"user\", \"reddit_user\") \\\n",
    "        .option(\"password\", \"reddit_pass\") \\\n",
    "        .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .save()\n",
    "\n",
    "# 6. Write streaming DataFrame to Postgres\n",
    "query = parsed.writeStream \\\n",
    "    .foreachBatch(write_to_postgres) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f039fb7c-0a41-4382-b334-d3dfa73111b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
