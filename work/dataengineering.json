[{"id": "1olrgye", "name": "t3_1olrgye", "content": "This thread is a place where you can share things that might not warrant their own thread. It is automatically posted each month and you can find previous threads in the collection.\n\nExamples:\n\n* What are you working on this month?\n* What was something you accomplished?\n* What was something you learned recently?\n* What is something frustrating you currently?\n\nAs always, sub rules apply. Please be respectful and stay curious.\n\n**Community Links:**\n\n* [Monthly newsletter](https://dataengineeringcommunity.substack.com/)\n* [Data Engineering Events](https://dataengineering.wiki/Community/Events)\n* [Data Engineering Meetups](https://dataengineering.wiki/Community/Meetups)\n* [Get involved in the community](https://dataengineering.wiki/Community/Get+Involved)", "date_utc": 1762012849.0, "title": "Monthly General Discussion - Nov 2025", "upvote_ratio": 0.86, "score": 5, "url": "https://www.reddit.com/r/dataengineering/comments/1olrgye/monthly_general_discussion_nov_2025/"}, {"id": "1n5slp3", "name": "t3_1n5slp3", "content": "https://preview.redd.it/ia7kdykk8dlb1.png?width=500&format=png&auto=webp&s=5cbb667f30e089119bae1fcb2922ffac0700aecd\n\nThis is a recurring thread that happens quarterly and was created to help increase transparency around salary and compensation for Data Engineering.\n\n# [Submit your salary here](https://tally.so/r/nraYkN)\n\nYou can view and analyze all of the data on our [DE salary page](https://dataengineering.wiki/Community/Salaries) and get involved with this open-source project [here](https://github.com/data-engineering-community/data-engineering-salaries).\n\n&#x200B;\n\nIf you'd like to share publicly as well you can comment on this thread using the template below but it will not be reflected in the dataset:\n\n1. Current title\n2. Years of experience (YOE)\n3. Location\n4. Base salary & currency (dollars, euro, pesos, etc.)\n5. Bonuses/Equity (optional)\n6. Industry (optional)\n7. Tech stack (optional)", "date_utc": 1756742436.0, "title": "Quarterly Salary Discussion - Sep 2025", "upvote_ratio": 0.91, "score": 34, "url": "https://www.reddit.com/r/dataengineering/comments/1n5slp3/quarterly_salary_discussion_sep_2025/"}, {"id": "1ozegd4", "name": "t3_1ozegd4", "content": "Happy to get help from experts in the house.", "date_utc": 1763380927.0, "title": "What is your current Enterprise Cloud Storage solution and why did you choose them?", "upvote_ratio": 0.92, "score": 10, "url": "https://www.reddit.com/r/dataengineering/comments/1ozegd4/what_is_your_current_enterprise_cloud_storage/"}, {"id": "1ozdbcx", "name": "t3_1ozdbcx", "content": "Hey! I\u2019m curious how other teams manage feedback and releases when building new data pipelines.\n\nRight now, after an initial requirements-gathering phase, my team builds the *entire* pipeline end-to-end (raw \u2192 curated \u2192 presentation) and only then sends everything for UAT. The problem is that when feedback comes in, it\u2019s often late in the process and can cause delays or rework.\n\nI\u2019ve been told (by ChatGPT) that a more common approach is to deliver pipelines in stages, like:\n\n* Raw/Bronze\n* Curated/Silver\n* Presentation/Gold\n* Dashboards / metrics / ML models\n\nThis is so you can get business feedback earlier in the process and avoid \u201cbig bang\u201d releases + potential rework.\n\nSo I\u2019m wondering:\n\n* Does your team deliver pipelines incrementally like this?\n* What does UAT look like for you?\n\nWould really appreciate hearing how other teams handle this. Thanks!", "date_utc": 1763377039.0, "title": "How do your teams handle UAT + releases for new data pipelines? Incremental delivery vs full pipeline?", "upvote_ratio": 0.88, "score": 6, "url": "https://www.reddit.com/r/dataengineering/comments/1ozdbcx/how_do_your_teams_handle_uat_releases_for_new/"}, {"id": "1oz6ckb", "name": "t3_1oz6ckb", "content": "I work for a small food and bev company. 200mm revenue per year. I joined as an analyst and worked my up to Data Analytics manager. Huge salary jump from 60k to 160k in less than 4 years. This largely comes from being able to handle ALL things ERP / SQL / Analytics / Decision making (I understand core accounting concepts and strategy). Anyway, the company is finally maturing and recognizing that I cannot keep wearing a million hats. I told my boss I am okay not going the finance route, and he is suggesting Director of IT. Super flattering but I feel under qualified! Also I constantly consider leaving the company for greener pastures as it pertains to cloud tech. I want to work somewhere that has a modern stack for modern data products (not food and bev). Ultimately I am considering the management track versus keeping my head down in the weeds of analytics. Also I am super early in my career (under 30) . What would you do?\n", "date_utc": 1763351784.0, "title": "Director of IT or DE", "upvote_ratio": 0.89, "score": 22, "url": "https://www.reddit.com/r/dataengineering/comments/1oz6ckb/director_of_it_or_de/"}, {"id": "1oz5bni", "name": "t3_1oz5bni", "content": "For those working as analytics engineers, or data engineers who involves alot in analytics activities, I\u2019d like to understand how your role looks in practice.\n\nA few questions:\n\nHow much of your day goes into data engineering tasks, and how much goes into analytics or modeling work?\n\nAs they say analytics engineering bridges the gap between data engineering and data analysis so I would love to know how exactly you guys are doing it IRL? \n\nWhat tools do you use most often?\n\nDo you build and maintain pipelines, or is your work mainly inside the warehouse?\n\nHow much responsibility do you have for data quality and modeling?\n\nHow do you work with analysts and data engineers?\n\nWhat skills matter most in this kind of hybrid role?\n\n\nI\u2019m also interested in where you see this role heading. As AI makes pipeline work and monitoring easier, do you think the line between data engineering and analytics work will narrow?\n\nAny insight from your experience would help. Thank you for your time! ", "date_utc": 1763348718.0, "title": "For Analytics Engineers or DEs doing analytics work, what does your role look like?", "upvote_ratio": 0.82, "score": 24, "url": "https://www.reddit.com/r/dataengineering/comments/1oz5bni/for_analytics_engineers_or_des_doing_analytics/"}, {"id": "1ozd7o5", "name": "t3_1ozd7o5", "content": "**Introduction**\n\ni am based in Switzerland and have been working in the field of data & analytics as a consultant for a little over 5 years. I worked mostly within the SAP analytics ecosystem with some exposure to GCP. I did a bunch of e learning courses over the years and realized it is more or less a waste of time unless you actually get to apply that knowledge in a real project, better sooner than later.\n\nTechnical skill-wise: mostly SQL, Python here and there and a lot of ABAP 3 years ago. The rest of the time just using GUIs (SAP users will know what i am talking about)\n\n  \n**Expectations / Priorities:**\n\n1. I would like to switch from consultant to inhouse. \n2. I would like to diversify my skill set and add some non-SAP tools and technologies to my skill set.\n3. I would like to strike a better balance between pure data engineering (as in coding, SQL, data analysis, data cleansing etc.) vs. other parts of the job: doing workshops, communication, collaborating with team members. Wouldnt mind gaining some managerial responsiblity either. Past 3 years i felt like a \"only\" data analyst, writing mostly SQL and analyzing data.\n4. Over the course of these 5 years i never really felt like i was part of a team working on a mission with a any degree of purpose whatsoever. Would like to have more of that in my life.\n5. I would like to stay located in Switzerland but open to work remotely.\n\n\n\nI applied to a decent amount of jobs and having a tough time to find an entry point with my starting position. I would be more than happy to prepare before starting a new position through online courses in case there it is expected to have knowledge around certains tools / products / technologies.\n\n\n\nI am also considering to do freelancing, but i am unsure how much of the above list would actually improve in that setting. Also i wouldnt really know where and how to start / get clients and would require some networking i suppose.  \n\n\nI am reducing my working hours next year to introduce more flexibility to my daily life and foster my search for a more fulfilling job setup. I am also aware that the above wish list is asking for a lot and most likely i will have to make some sort of compromise and will never check all the boxes.\n\n  \nLooking for any advice and happy to connect with people who are in a similar spot or share the same priorities as me. ", "date_utc": 1763376668.0, "title": "Time for change", "upvote_ratio": 0.75, "score": 4, "url": "https://www.reddit.com/r/dataengineering/comments/1ozd7o5/time_for_change/"}, {"id": "1oysp7j", "name": "t3_1oysp7j", "content": "I am in a tough, stressful position right now. I've been tasked with taking over a large project that a previous engineer was working on, but left the company. There are several problems with the output. There are no comments in the code, no documentation on what it means, and no one understands why they did what they did in the code and what it means. I'm being forced to fix something I didn't break, explain things I didn't create, all while the end users don't even have a great sense of what \"done\" looks like. And on top of that, they want it done yesterday. What do you do in these situations?", "date_utc": 1763316674.0, "title": "Tech Debt", "upvote_ratio": 0.86, "score": 37, "url": "https://www.reddit.com/r/dataengineering/comments/1oysp7j/tech_debt/"}, {"id": "1ozdksr", "name": "t3_1ozdksr", "content": "A few major challenges that I faced.\n\n* Phase 2 of KSA e-invoicing brings stricter compliance, requiring businesses to upgrade systems to meet new integration and reporting standards.\n* Many companies struggle with API readiness, real-time data sharing, and aligning ERP/GST tools with ZATCA\u2019s technical specs.\n* Managing security requirements, certification, and large-scale data validation adds additional complexity during implementation.", "date_utc": 1763377930.0, "title": "What are the implementation challenges of Phase 2 KSA e-invoicing?", "upvote_ratio": 0.5, "score": 0, "url": "https://www.reddit.com/r/dataengineering/comments/1ozdksr/what_are_the_implementation_challenges_of_phase_2/"}, {"id": "1ozdem1", "name": "t3_1ozdem1", "content": "Hi guys, I am new to this field and have a question regarding legacy system decommissioning. Is it necessary, and why/how do we do it? I am well out of my depth with this one. ", "date_utc": 1763377333.0, "title": "Why is following the decommissioning process important?", "upvote_ratio": 0.4, "score": 0, "url": "https://www.reddit.com/r/dataengineering/comments/1ozdem1/why_is_following_the_decommissioning_process/"}, {"id": "1oz6zsg", "name": "t3_1oz6zsg", "content": "Hello everybody, I'm new here!  \nYep, based on the title I'm enough desperate that I could pay for a SQLMesh solution, well. \n\nhttps://preview.redd.it/svoce309vq1g1.png?width=580&format=png&auto=webp&s=831984c134916fda055c23cc1bc75c2892021abf\n\n  \nI'm trying to create a table in my silver layer (it's a university project) where I'm trying to clean information in order to show clear information to BI/Data Analyst, however I chose SQLMesh on DBT (Now I'm crying..).  \nWhen I try to create a table because of \"FULL\" it ends up creating a View... for me it doesn't make sense (because it's in silve layer, and the table is created on sqlmes\\_silver (idk why...)\n\nIf you know how to create it correctly you can be in touch (DM as you wish).\n\nhttps://preview.redd.it/unzq7c1wvq1g1.png?width=418&format=png&auto=webp&s=825abb8428232e34016b260c1ef0a6324b924ed6\n\nI'll be veeeery gratefull if you can help me.\n\nOhh..annnd...don't judge my english (thanks XD)", "date_utc": 1763353775.0, "title": "Asking for help with SQLMesh (I could pay T.T)", "upvote_ratio": 0.81, "score": 3, "url": "https://www.reddit.com/r/dataengineering/comments/1oz6zsg/asking_for_help_with_sqlmesh_i_could_pay_tt/"}, {"id": "1oz2e05", "name": "t3_1oz2e05", "content": "Hi everyone!\n\nI\u2019m a Data Science student, and for one of my co-op projects I need to chat with a professional working in Canada in a data-related role (data analyst, data scientist, BI analyst, ML engineer, etc.).\n\nIt\u2019s just a short 10\u201315 minute informational chat and the goal is simply to understand the Canadian labour market and learn more about different career paths in data.\n\nIf anyone here is currently working in Canada in a data/analytics/ML role and wouldn\u2019t mind helping a student out, I\u2019d really appreciate it. Even one person would make a huge difference.\n\nThanks so much in advance, and no worries at all if you\u2019re busy!", "date_utc": 1763340477.0, "title": "Looking for a Canadian Data Professional for a 10\u201315 Min Informational Chat", "upvote_ratio": 0.8, "score": 6, "url": "https://www.reddit.com/r/dataengineering/comments/1oz2e05/looking_for_a_canadian_data_professional_for_a/"}, {"id": "1oz9vbi", "name": "t3_1oz9vbi", "content": "I\u2019m 27, based in Ahmedabad (India), and have been stuck at the same crossroads for over 3 years. I want some guidance related to job vs freelancing and salesforce vs data career\n\nMy Background\n\nEducation:\n\nBachelors: Mechanical Engineering\nMasters #1: Engineering Management\nMasters #2: Data Science (most aligned with my interests)\n\nExperience:\n\n2 years as a Salesforce Admin (laid off in Sep 2024)\nFreelancing since Mar 2024 in Salesforce Admin + Excel\nHave 1 long-term client and want to keep earning in USD remotely\n\nUncertain about: sales/business development; haven\u2019t explored deeply yet.\n\nThe 3 Paths I Keep Bouncing Between\n\n1. Salesforce (Admin \u2192 Developer \u2192 Consultant)\n2. Data Engineering (ETL, pipelines, cloud, dbt, Airflow, Spark)\n3. AI/ML (LLMs, MLOps, applied ML, generative AI)\n\nI feel stuck because these options each look viable, but the time, cost, switching friction, and long-term payoff are very different. What should i upskill into if i want to keep doing freelancing or should i drop freelancing and get a job? ", "date_utc": 1763363662.0, "title": "Stuck for 3 years choosing between Salesforce, Data Engineering, and AI/ML \u2014 need a rational, market-driven direction", "upvote_ratio": 0.5, "score": 0, "url": "https://www.reddit.com/r/dataengineering/comments/1oz9vbi/stuck_for_3_years_choosing_between_salesforce/"}, {"id": "1oy9jq4", "name": "t3_1oy9jq4", "content": "What's up, r/dataengineering. We all know SQL is the bedrock, but practicing it is... well, boring.\n\nI made a tool called SQL Case Files. It's a detective game that runs in your browser (or offline as a PWA) and teaches you SQL by having you solve crimes. It's 100% free, no sign-up. Just a solid way to practice queries.\n\nCheck it out:\u00a0[https://sqlcasefiles.com](https://sqlcasefiles.com/)", "date_utc": 1763258630.0, "title": "I built a free PWA to make SQL practice less of a chore. (100+ levels)", "upvote_ratio": 0.98, "score": 142, "url": "https://www.reddit.com/r/dataengineering/comments/1oy9jq4/i_built_a_free_pwa_to_make_sql_practice_less_of_a/"}, {"id": "1oz6jt2", "name": "t3_1oz6jt2", "content": "I built a small tool that generates ready-to-use Apache Beam + GCP Dataflow project templates with one command both via CLI and MCP Server. The idea is to avoid wasting time on folder structure, CI/CD, Docker setup, and deployment boilerplate so teams can focus on actual pipeline logic. Would love feedback on whether this is useful, overkill, or needs different features.\n\nRepo:\u00a0[https://github.com/bharath03-a/gcp-dataflow-template-kit](https://github.com/bharath03-a/gcp-dataflow-template-kit?utm_source=chatgpt.com)", "date_utc": 1763352403.0, "title": "I built a CLI + Server to instantly bootstrap standardized GCP Dataflow templates (Apache Beam)", "upvote_ratio": 0.67, "score": 1, "url": "https://www.reddit.com/r/dataengineering/comments/1oz6jt2/i_built_a_cli_server_to_instantly_bootstrap/"}, {"id": "1oysczy", "name": "t3_1oysczy", "content": "Hey,\n\nFor context, I just graduated from a good NY state school with a high GPA in Mechanical Engineering and took a full time role at Lockheed Martin as a Systems Engineer (mostly test and integration stuff).\n\nI have never particularly enjoyed any work specifically, and I chose mechanical because I was an 18 year old who knew nothing and heard it was a solid degree. My main goal is to find a high paying job in NYC, and I think that data engineering seems like a good track to go down. \n\nCurrently, I don\u2019t have too much coding experience; during college, I took one class on python and SQL, and I also have a solid amount of Matlab experience. I am a quick learner and remember finding myself picking up python rather quickly when I took the class freshman year.\n\nBasically, I just want to know what I have to do to make this career change as quickly as possible, i.e. get a masters in data analytics somewhere, certifications online, etc. It doesn\u2019t seem that my job will be providing too much experience in the field so I want to know what I should do to get quantifiable metrics on my r\u00e9sum\u00e9. ", "date_utc": 1763315910.0, "title": "Mechanical Engineering BA to Data Engineering career", "upvote_ratio": 0.88, "score": 6, "url": "https://www.reddit.com/r/dataengineering/comments/1oysczy/mechanical_engineering_ba_to_data_engineering/"}, {"id": "1oysxhj", "name": "t3_1oysxhj", "content": "Hi, there are some interesting books in latest bundle in humble: https://www.humblebundle.com/books/data-engineering-science-oreilly-books ", "date_utc": 1763317209.0, "title": "data engineering & science oreilly humble bundle books set", "upvote_ratio": 1.0, "score": 3, "url": "https://www.reddit.com/r/dataengineering/comments/1oysxhj/data_engineering_science_oreilly_humble_bundle/"}, {"id": "1oymygm", "name": "t3_1oymygm", "content": "Hi. \n\nSorry if this is the wrong sub in advance. \n\nI have the chance to do an internship as a Data Governance Specialist for six months in an international project but it won't follow up with a job offer. \n\nI am pursuing already an internship as a Data Analyst which should finalize with a job offer. \n\nI am super entry level (it's my first job experience), should I give up the DA job to pursue this? Is it good CV wise? Will I get a job afterwards if I have this limited experience in Data Governance? \u200b\u200b\u200b", "date_utc": 1763302950.0, "title": "Data Governance Specialist internship or more stable option [EU] ?", "upvote_ratio": 0.81, "score": 3, "url": "https://www.reddit.com/r/dataengineering/comments/1oymygm/data_governance_specialist_internship_or_more/"}, {"id": "1oyt5h8", "name": "t3_1oyt5h8", "content": "23M currently in community college planning to transfer to a university for an economics degree to hopefully land a data analyst position. The reason i am doing economics is because if i want to do any other degree like computer science/engineering, stats, math, etc. i would need to stay in community college for 3 years instead of 2 which would limit 1 year of not being able to network and find internships when i transfer to a well-known school. I am also a military veteran using my post 9/11 Gi bill which basically gives me a free bachelor's degree but if i stay in community college for 3 years the gi bill benefits would cut before i get the bachelor's degree costing me a lot more time and money in the long run. My plan was to get an economic degree do a bunch of courses, self-teach myself, projects, etc in order to break into the data world to eventually get into data engineering or MLOps/AI Engineer. Do you think this would be a good decision? i wouldn't mind getting a master's later on if need be but i would be 29-30 by then and wondering if i should just bit the bullet change in CS or CE now and get it over with. what do you think? ", "date_utc": 1763317725.0, "title": "Am i shooting myself in the foot for getting an economics degree in order to go from data analyst to data engineer?", "upvote_ratio": 0.48, "score": 0, "url": "https://www.reddit.com/r/dataengineering/comments/1oyt5h8/am_i_shooting_myself_in_the_foot_for_getting_an/"}, {"id": "1oy6dn0", "name": "t3_1oy6dn0", "content": "Hi everyone,  \nI\u2019m working on a data privacy project and my team uses BigQuery as our lakehouse. I need to anonymize sensitive data, and from what I\u2019ve seen, Google provides some native masking options \u2014 but they seem to rely heavily on policy tags and Data Catalog policies.\n\nMy challenge is the following: I *don\u2019t* want to mask data in the original (raw/silver) tables. I only want masking to happen in the **consumption views** that are built on top of those tables. However, it looks like BigQuery doesn\u2019t allow applying policy tags or masking policies directly to views.\n\nHas anyone dealt with a similar situation or has suggestions on how to approach this?\n\nThe goal is to leverage Google\u2019s built-in tools instead of maintaining our own custom anonymization logic, which would simplify ongoing maintenance. If anyone has alternative ideas, I\u2019d really appreciate it.\n\n**Note:** I only need the data to be anonymized in the final consumption/refined layer.", "date_utc": 1763249765.0, "title": "How do you handle data privacy in BigQuery?", "upvote_ratio": 0.94, "score": 27, "url": "https://www.reddit.com/r/dataengineering/comments/1oy6dn0/how_do_you_handle_data_privacy_in_bigquery/"}, {"id": "1oxspqy", "name": "t3_1oxspqy", "content": "Hey folks,\n\n  \nI am a Data Engineering Leader (15+ yrs experience) and I have been thinking about how fast AI is changing our field, especially Data Engineering.\n\nBut here\u2019s a question that\u2019s been bugging me lately:  \nWhen students graduate with a B.E./B.Tech in Computer Science or an MCA,  \nhow much of their syllabus today actually covers Data Engineering?\n\nWe keep hearing about Data Engineering, *AI integrated courses* & *curriculum reforms*,  \nbut on the ground, how much of it is real vs. just marketing?", "date_utc": 1763215977.0, "title": "How Much of Data Engineering Is Actually Taught in Engineering or MCA Courses?", "upvote_ratio": 0.94, "score": 71, "url": "https://www.reddit.com/r/dataengineering/comments/1oxspqy/how_much_of_data_engineering_is_actually_taught/"}, {"id": "1oy1wuh", "name": "t3_1oy1wuh", "content": "I've been working with BigQuery for about 3 years, but cost control only became my responsibility 6 months ago. Our spend is north of $100K/month, and frankly, this has been an exhausting experience.\n\nWe recently started experimenting with reservations. That's helped give us more control and predictability, which was a huge win. But we still have the occasional f\\*\\*\\* up.\n\nEvery new person who touches BigQuery has no idea what they're doing. And I don't blame them: understanding optimization techniques and cost control took me a long time, especially with no dedicated FinOps in place. We'll spend days optimizing one workload, get it under control, then suddenly the bill explodes again because someone in a completely different team wrote some migration that uses up all our on-demand slots.\n\nBased on what I read in this thread and other communities, this is a common issue.\n\nHow do you handle this? Is it just constant firefighting, or is there actually a way to get ahead of it? Better onboarding? Query governance? \n\nI put together a quick survey to see how common this actually is: [https://forms.gle/qejtr6PaAbA3mdpk7](https://forms.gle/qejtr6PaAbA3mdpk7)", "date_utc": 1763238221.0, "title": "6 months of BigQuery cost optimization...", "upvote_ratio": 0.8, "score": 14, "url": "https://www.reddit.com/r/dataengineering/comments/1oy1wuh/6_months_of_bigquery_cost_optimization/"}, {"id": "1oy4lie", "name": "t3_1oy4lie", "content": "Hi y\u2019all,\n\nI\u2019m trying to build my first project using Airflow and been having difficulty setting up the correct combo of my Dockerfile, docker-compose.yaml, .env, requirements.txt, etc.\n\nProject contains one simple DAG.\n\nOriginally been using latest 3.1.3 airflow version but gave up and now trying 2.9.3 but having new issues with matching the right versions of all my other tools.\n\nAm I best off just switching back to 3.1.3 and duking it out?\n\nEDIT: switched to 3.0.6 and got the DAG to work at least to a level where I can manually test it (still breaks on task 1). Used to break with no logs so debugging was hard but now more descriptive error logs appear so will get right on with attacking that.\n\nThanks for all that replied before the edit \u2764\ufe0f", "date_utc": 1763245060.0, "title": "Which Airflow version is best for beginners?", "upvote_ratio": 0.82, "score": 7, "url": "https://www.reddit.com/r/dataengineering/comments/1oy4lie/which_airflow_version_is_best_for_beginners/"}, {"id": "1oxu38x", "name": "t3_1oxu38x", "content": "I\u2019m just toying around with a new toolset to feel it out. \n\nI have an always on EC2 that periodically calls some python code which,\n\nLoads incrementally where it left off from Postgres to a persistent duckdb.  ( Postgres is a read replica of my primary application db )\n\nRuns transforms within duckdb. \n\nLoads incrementally the changes of that transform into a separate Postgres. ( my data warehouse )\n\nKinda scratching my head with edge cases of DLT \u2026 but I really like how it seems like if the schema evolves then DLT handles it by itself without the need for me to change code.  The transform part could break though.  No getting around that. ", "date_utc": 1763219526.0, "title": "Experimenting with DLT and DuckDb", "upvote_ratio": 0.94, "score": 23, "url": "https://www.reddit.com/r/dataengineering/comments/1oxu38x/experimenting_with_dlt_and_duckdb/"}, {"id": "1oxto9o", "name": "t3_1oxto9o", "content": "For about past 6 months, I have been working regularly with confluent (Kafka) and databricks (AutoLoader) for building and running some streaming pipelines (all that run either on file arrivals in s3 or pre-configured frequency in the order of minute(s), with size of data being just 1-2 GBs per day at max.\n\nI have read all the cost optimisation docs by them and by Claude. Yet still the cost is pretty high.\n\nIs there any way to cut down the costs while still using managed services? All suggestions would be highly appreciated.", "date_utc": 1763218471.0, "title": "How to setup budget real-time pipelines?", "upvote_ratio": 0.86, "score": 19, "url": "https://www.reddit.com/r/dataengineering/comments/1oxto9o/how_to_setup_budget_realtime_pipelines/"}, {"id": "1oxoqj9", "name": "t3_1oxoqj9", "content": "Currently writing Python scripts to pull data from Stripe, Shopify, etc.. in our data lake and it's getting old.\n\nWhat's everyone using for this? Seen people mention Airbyte but curious what else is out there that's free or at least not crazy expensive.\n\nAnd if you're running something in production, does it actually work reliably? Like what breaks? Schema ? Rate limits? Random API timeouts? And how do you actually deal with it?", "date_utc": 1763203679.0, "title": "Good free tools for API ingestion? How do they actually run in production?", "upvote_ratio": 0.91, "score": 22, "url": "https://www.reddit.com/r/dataengineering/comments/1oxoqj9/good_free_tools_for_api_ingestion_how_do_they/"}, {"id": "1ox84x9", "name": "t3_1ox84x9", "content": "Obvious throwaway account is obvious.\n\nMy job is a data engineer for a medium-ish sized company, been here for just over 4 years.  This is my first \"data\" job, but I learned a good bit about SQL in previous roles.  Our department / my team manages our BI data warehouse, and we have a couple of report developers as well.  When I read and study about modern data engineering practices, or modern development practices / AI usage, I feel like I'm a caveman rubbing sticks together while watching flying cars go by me every day.  I'm considering switching to a DevOps position in my company because I enjoy working with Linux and smaller applications, but also because I feel like this position is a complete dead end - I have no room to exert creativity or really learn anything on the job because of the reasons I'll detail below.\n\nUntil about 2 years ago, our data warehouse was basically one large SQL database (MS SQL).  Standard Kimball-style facts/dimensions, with a handful of other nonstandard tables scattered here and there.  We also have a few separate databases that act as per-department \"sandboxes\" for business analysts to build their own stuff, but that's a whole separate story.  The whole thing is powered by SSIS packages; OLTP data transformed to a star schema in most cases.  Most of it appears to be developed by people who learned SSIS before SQL, because in almost every process, the business logic is baked into transformations instead of scripts or code.  I expected this from a legacy setup, and shortly after I started working here it became known that we were going to be migrating to the cloud and away from this legacy stuff, so I thought it was a temporary problem that we'd be walking away from.\n\nHow naive I was.\n\nProblem #1:  We have virtually no documentation, other than the occasional comment within code if I'm lucky.  We have no medallion architecture.  We have no data dictionary.  Pretty much all the knowledge of how a majority of our data interacts is tribal knowledge within my department and the business analysts who have been here for a long time.  Even the business logic of our reports that go to the desks of the C-levels gets argued about sometimes because it's not written down anywhere.  We've had no standard code practices (ever) so one process to the next could employ a totally different design approach.\n\nProblem #2:  Enter the cloud migration phase.  At first, this sounded like the lucky break I was hoping for - a chance to go hands-on with Snowflake and employ real data engineering tools and practices and rebuild a lot of the legacy stuff that we've dealt with since our company's inception.  Sadly, that would have been way too easy... Orders came down from the top that we needed to get this done as a lift-and-shift, so we paid a consulting company to use machine learning to convert all of our SSIS packages into Azure Data Factory pipelines en masse.  Since we don't have a data dictionary or any real documentation, we really had no way to offer test cases for validating data after the fact.  We spent months manually validating table data against table data, row by row.  Now we're completely vendor-locked with ADF, which is a massive pile of shit for doing surgical-level transformations like we do. \n\nProblem #2A:  Architecture.  Our entire architecture was decided by one person - a DBA who, by their own admission, has never been a developer of any sort, so they had no idea how complex some of our ETL processes were.  Our main OLTP system is staying on-prem, and we're replicating its database up to Snowflake using a third-party tool as our source.  Then our ADF processes transform the data and deposit it back to Snowflake in a separate location. I feel like we could have engineered a much simpler solution than this if we were given a chance, but this decision was made before my team was even involved.  (OneLake?  Dynamic Tables?)\n\nProblem #3:  Project management, or the lack thereof.  At this inception of this migration, the decision to use ADF was made without consulting anyone in my department, including our manager.  Similarly, the decision to just convert all of our stuff was made without input from our department.  We were also never given a chance to review any of our existing stuff to determine if anything was deprecated; we paid for all of it to be converted, debugged it, and half of it is defunct.  Literal months of manpower wasted.  \n\nProblem #4:  Looking ahead.  If I fast forward to the end of this migration phase and look at what my job is going to be on a daily basis, it boils down to wrestling with Azure Data Factory every day and dissecting tiny bits of business logic that are baked into transformations, with layers upon layers of unnecessary complexity, let alone the aforementioned lack of code standardization.\n\nThis doesn't feel like data engineering, this feels like janitorial code cleanup as a result of poor project planning and no foresight.  I'm very burned out and it feels hopeless to think there's any real data engineering future here.  I recently picked up the Snowflake SnowPro Core certification in my downtime because I really enjoy working with the platform, and I've also been teaching myself a bit about devops in my spare time at home (built a homelab / NAS, stood up some containers, gonna be playing with K3S this weekend).  \n\nThe saving grace is my team of fellow developers.  We've managed to weed out the turds over the past year, so the handful of us on the team all work really well together, collaborate often, and genuinely enjoy each other while being in the trenches.  At the moment, I'm staying for the clowns and not the circus.  \n\nAm I crazy, or is this a shitshow?  Would anybody else stay here, or how would anyone else proceed in this situation?  Any input is welcomed.\n\nedit:  for clarity, current architecture boils down to:  source OLTP > replicated to Snowflake via third-party tool > ADF for ETL/ELT > destination Snowflake", "date_utc": 1763153374.0, "title": "Sanity check: am I crazy for feeling like my \"data engineering\" position is a dead end?", "upvote_ratio": 0.9, "score": 84, "url": "https://www.reddit.com/r/dataengineering/comments/1ox84x9/sanity_check_am_i_crazy_for_feeling_like_my_data/"}, {"id": "1oxggmk", "name": "t3_1oxggmk", "content": "My main question is at what point and for what aggregations should I switch from SQL to Python?\n\nMy goals being:\n\n1. Not writing endless amount of repeated tedious code (or having AI write endless repeating tedious code for me). What I mean is all of the CTEs I need to write for each bucket/feature requested, so like CTE\\_a\\_category\\_last\\_month with a where clause on category and timeframe. My first thought was doing the buckets in Python would help but upon research everyone recommends to use SQL for pretty much everything up until machine learning.\n2. Run-time. Because of the sheer amount of features that were requested of me (400 for now, but they want to go more granular with categories so it's gonna be like 1000 more), the 400 take a while to run, about 15 minutes. Maybe 15 minutes isn't that bad? Idk but the non-technical people above me aren't happy with it.\n\nPre-Context:\n\nI am not the one coming up with the asks, I am a junior, I have very little power or say or access. This means no writing to PROD, only reading, and I have to use PROD. Yes I can use AI but I am not looking for AI suggestions because I know how to use AI and I'm already using it. I want human input on the smartest most elegant solution.\n\nAlso to preface I have a bunch of experience with SQL, but not so much experience with Python beyond building machine learning algorithms and doing basic imputation/re-expression, which is why I'm not sure what tool is better.\n\nContext-context:\n\nI work with transaction data. We have tables with account info, customer info, transaction code info, etc. I've already aggregated all of the basic data and features, runs pretty fast. But once I add the 400 buckets/features, it runs slow. For each transaction category and a bunch of time frames (ie. month buckets for the past two years, so you'll have a\\_category\\_last\\_month, a\\_category\\_last\\_last\\_month, b\\_category\\_last\\_month, etc) I need to do a bunch of heavy aggregations ie minimum amount spent on a single day during given month.\n\nRight now it's all done in SQL. I'm working on optimizing the query, but there is only so much I can do and I dread working on the new 1000 categories they want. What is the best way to go about my task? What would SQL handle better and be better/more elegant for code written vs Python? AI suggested to create a row for each feature instead of column for every single customer and then have Python pivot it, is this a good option? I feel like more rows would take even longer to run.", "date_utc": 1763175004.0, "title": "Need to scale feature engineering, only Python and SQL (SQL Server & SSIS) available as tools (no dbt etc.)", "upvote_ratio": 0.94, "score": 16, "url": "https://www.reddit.com/r/dataengineering/comments/1oxggmk/need_to_scale_feature_engineering_only_python_and/"}, {"id": "1oxf79l", "name": "t3_1oxf79l", "content": "I have a need to output all rows in a partition to just one file, while still maintain parallelism for PySpark writes. The dataframes that I have can range up to 65+ million rows.\n\nAll of my googling gave me two options: `df.coalesce(1).write.partitionBy(...)` or `df.repartition(1).write.partitionBy(...)`.\n\nThe `coalesce` option seems to be the least preferred by most because it reduces the executors down to 1 and effectively becomes single threaded. The `repartition` option combines everything back into one partition and while there may still be multiple executors, the write seems to be single, and it takes a long time.\n\nI have tried `df.repartition(*cols).write.partitionBy(*cols)...`, but this produces multiple files for some partitions.\n\nI would like the output of `coalesce(1) / repartition(1)`, but the parallelism of regular `df.write`.\n\nIs this possible to do, or will I have to rethink about wanting one file?", "date_utc": 1763171403.0, "title": "Writing PySpark partitions to one file each in parallel?", "upvote_ratio": 0.92, "score": 19, "url": "https://www.reddit.com/r/dataengineering/comments/1oxf79l/writing_pyspark_partitions_to_one_file_each_in/"}, {"id": "1oxubz8", "name": "t3_1oxubz8", "content": "Started a new job and have a $700 professional development stipend I need to use before the end of the year.\n\nI have 8YOE and own and have done most of the books and courses recommended on this sub. So I have no idea what to spend it on would love some suggestions. The only requirement indicated is that it has to be in some way related to my job as a SWE/DE and increase my skills/career growth in some way. Any creative ideas?", "date_utc": 1763220129.0, "title": "Suggestions on what to spend $700 professional development stipend before EOY?", "upvote_ratio": 0.57, "score": 1, "url": "https://www.reddit.com/r/dataengineering/comments/1oxubz8/suggestions_on_what_to_spend_700_professional/"}, {"id": "1oxyssl", "name": "t3_1oxyssl", "content": "I have about 1.5 years of experience in data engineering, based in NYC. I worked in data analytics before giving me roughly 4 years of total professional experience. I\u2019ll be looking for a new job soon and I\u2019m wondering how realistic it is to find a remote position. \n\nIdeally, I\u2019d like to stay salary-tied to the NYC metro area while potentially living somewhere with a lower cost of living.\n\nAm i being delusional? I've only worked hybrid schedules.", "date_utc": 1763230725.0, "title": "how common is it to find remote jobs in DE?", "upvote_ratio": 0.38, "score": 0, "url": "https://www.reddit.com/r/dataengineering/comments/1oxyssl/how_common_is_it_to_find_remote_jobs_in_de/"}, {"id": "1owzjys", "name": "t3_1owzjys", "content": "I honestly don\u2019t know what to do. I\u2019ve been working my butt off on a major project since last year, pushing myself so hard that I basically burned out. I\u2019ve consistently shown updates, shared my progress, and even showed my manager the actual impact I made.\n\nBut in my end-of-year review, he said my performance was \u201cinconsistent\u201d and even called me \u201cdependent,\u201d just because I asked questions when I needed clarity. Then he said he\u2019s only been watching my work for the past 1\u20132 months\u2026 which makes it feel like the rest of my effort just didn\u2019t count.\n\nI feel so unfairly judged, and it honestly makes me want to cry. I didn\u2019t coast or slack off. I put everything into this project, and it feels like it was dismissed in two sentences.\n\nI also met with him to explain why I didn\u2019t deserve the review, but he stayed firm on his decision and said the review can\u2019t be changed.\n\nI\u2019m torn on what to do. Should I go to HR? Has anyone dealt with a manager who overlooks months of work and gives feedback that doesn\u2019t match reality?\n\nAny advice would really help.", "date_utc": 1763134181.0, "title": "Got an unfair end-of-year review after burning myself out", "upvote_ratio": 0.93, "score": 60, "url": "https://www.reddit.com/r/dataengineering/comments/1owzjys/got_an_unfair_endofyear_review_after_burning/"}, {"id": "1oxf3fk", "name": "t3_1oxf3fk", "content": "Hey, fellow engineers\n\nI've been staring at the monitor a lot lately, my eyes are all dry and feel like my vision is dropping. \n\nI cant just not look at it, you know, to do my job. How do yall take care of your overworked eyes? ", "date_utc": 1763171104.0, "title": "Eye care", "upvote_ratio": 0.68, "score": 6, "url": "https://www.reddit.com/r/dataengineering/comments/1oxf3fk/eye_care/"}, {"id": "1owxk6b", "name": "t3_1owxk6b", "content": "In an enterprise setting we spend  $100k+, in bigger orgs even $xM+ for transforming data at scale. To create the perfect data source for our business partners. Which often or most of the time is under utilized. To do this we use a data warehouses (Redshift, Snowflake) or lakehouse (Databricks, ADF, \u2026). The new platform made it easier to handle the data, but it comes with a cost. They are designed for big data (TB\u2019s to PB\u2019s of data), but arguably in most organization most data sources are a fraction of this size. Those solutions are also designed to lock you in with proprietary compute and data formats as they say necessary to provide the best performance. Whenever our Redshift Datawarehouse struggled to keep up AWS\u2019s answer was, \u201coh your cluster head node is not keeping up with the demand you should upgrade to the next bigger instance type\u201d problem solved and cost was doubled. \n\nBut now with cheap object storage and open data formats like iceberg it should be possible to get the same performance than Snowflake, Redshift and Databricks at a fraction of the cost. But in order to transform your data you need compute and your data need to be ingested into the compute, transformed and written back in the transformed format to your datalake. The object storage and network speed between storage and compute is usually your bottleneck here. \n\nI made some experiments with different EC2 instances and duckdb (just saying I am not affiliated with the product). I had a 85GB timeseries data stream (iceberg) that needed to be pivoted and split into 100 individual tables. On a regular general purpose compute instance t3g.2xlarge that took about 6-7 hours to complete. Then I used i4g memory optioned instances with more memory and network bandwidth up 25 Gbps and it halfed the time. Then I found these new instances network optimized c8gn and they managed to do all 100 tables in 20 mins. Compare this to databricks (Databricks was reading from s3), which took 3 hours. Databricks cost for this transform was $9.38 and the EC2 instance did it for $1.70. So huge savings with a bit of engineering.\n\nWanted to share this and wanted to hear some stories from others in their pursuit of cheaper data transformation options \n\nEDIT: just to clarify. I am not proposing get rid of data warehouse or lakehouse, just saying you can save by \u201coutsourcing\u201d compute for batch transformations to much cheaper compute options so you can keep your actual warehouse/lakehouse small.", "date_utc": 1763129587.0, "title": "Why is transforming data still so expensive", "upvote_ratio": 0.9, "score": 67, "url": "https://www.reddit.com/r/dataengineering/comments/1owxk6b/why_is_transforming_data_still_so_expensive/"}, {"id": "1ox2dhd", "name": "t3_1ox2dhd", "content": "I took a new job building greenfield data architecture for a company that has never had a data engineering team before.  \n\nI have only ever worked for giant boomer non tech companies with infinite budget for redundant tools who were okay with lighting $100k+ on fire MONTHLY to do fuck all in Foundry/Databricks.\n\nHow big is your team, how much data do you move around, what is your stack, and what do you spend per month?", "date_utc": 1763140334.0, "title": "What are your monthly costs?", "upvote_ratio": 0.89, "score": 35, "url": "https://www.reddit.com/r/dataengineering/comments/1ox2dhd/what_are_your_monthly_costs/"}, {"id": "1owsh0m", "name": "t3_1owsh0m", "content": "We built real time monitoring for factory equipment across 15 facilities, started with 100 machines, now over 500 each sends vibration, power usage, errors every 5 seconds it\u2019s about 2 million data points per day.\n\nFirst attempt was mqtt brokers at each site pushing to aws IoT core, worked with 10 machines in testing, fell apart at scale all brokers kept crashing, lost data everywhere just lasted 4 months. Second try was kafka clusters at each site but management became a full time job, needed way more hardware than budgeted, configuration issues between sites I spent more time fighting it than building features. We went way simpler didn't need maximum speed, just reliable data collection with minimal babysitting that handles network failures because factory internet sucks. Found messaging that runs on cheap hardware, handles hub and spoke setup, saves data so we don't lose anything when connections drop, small servers at each factory, machines connect locally. If the site loses internet local monitoring keeps working and syncs when back we are using nats now it handles 500 machines without drama.\n\nLearned edge first matters way more than raw speed for iot, devices should work independently and sync when they can, not depend on constant cloud, simpler beats complex sometimes.", "date_utc": 1763114370.0, "title": "streaming telemetry from 500+ factory machines to cloud in real time, lessons from 2 years running this setup", "upvote_ratio": 0.73, "score": 89, "url": "https://www.reddit.com/r/dataengineering/comments/1owsh0m/streaming_telemetry_from_500_factory_machines_to/"}, {"id": "1owy8wn", "name": "t3_1owy8wn", "content": "Currently running a simple ETL: Postgres -> minor transforms -> S3 (Iceberg) using pyiceberg in a single Python script on Lambda (daily). Analysts query it with DuckDB for ad-hoc stuff. Works great.\n\nBut everywhere I look online, everyone's using Spark for this kind of workflow instead of pyiceberg. I'm a solo data engineer (small team), so managing a Spark cluster feels way beyond my bandwidth.\n\nAm I missing something critical by not using Spark? Is my setup too \"hacky\" or unprofessional? Just want to make sure I'm not shooting myself in the foot long-term.", "date_utc": 1763131224.0, "title": "When does Spark justify itself for Postgres to S3 ETL using Iceberg format? Sorry, I'm noob here.", "upvote_ratio": 0.91, "score": 34, "url": "https://www.reddit.com/r/dataengineering/comments/1owy8wn/when_does_spark_justify_itself_for_postgres_to_s3/"}, {"id": "1oxamdy", "name": "t3_1oxamdy", "content": "TLDR\n\nDBA here, in many years of career, my biggest drama to fight were always metrics or lack of.\n\nPlaces always had a bare minimum monitoring scripts/applications and always reactive. Meaning only if it\u2019s broken, it alerts.\n\nI\u2019m super lazy and I don\u2019t want to be awake 3am to fix something that I knew was going to break hours, days ahead. So as a side gig, I always tried to create meaning metrics. Today my company relies a lot on a grafana+prometheus setup I created because the our application as a black box. Devs would rely on reading logs and hoping for the best to justify a behaviour that maybe was normal, maybe was always like that. So grafana just proved it right or wrong.\n\nDecisions are now made by people \u201cwatching grafana\u201d. This metric here means this, this other means that. And both together means that.\n\nWhile it still a very small side project, now I have been given people to help me to leverage that to the entire pipeline, which is fairly complex from the business perspective, and time consuming, given I don\u2019t have a deep knowledge of any of these tools and infrastructure behind it and I learn as I find challenges.\n\nI was just a DBA with a side project hahaa.\n\n\nFinally my question:\nWhere do I start? I mean, I already started, but I wonder if I can make use of ML to create meaning alerts/metrics. Because people can look at 2 - 3 charts and make sense of what is going on, but leveraging this to the whole pipeline will be too much for humans and probably too noise. \n\nIt a topic I have quite a lot interest but no much background experience.\n\n\n\n", "date_utc": 1763159275.0, "title": "Monitoring: Where do I start?", "upvote_ratio": 0.9, "score": 8, "url": "https://www.reddit.com/r/dataengineering/comments/1oxamdy/monitoring_where_do_i_start/"}, {"id": "1owt21l", "name": "t3_1owt21l", "content": "At the company I work at the data flow is much more complex and something more like ELTLTLTL. Or do we even generally count intermediary 'staging tables' when deciding whether a pipeline falls into ETL or ELT?", "date_utc": 1763116455.0, "title": "Is the difference between ETL and ELT purely theoretical or is there some sort of objective way to determine in which category a pipeline falls?", "upvote_ratio": 0.97, "score": 62, "url": "https://www.reddit.com/r/dataengineering/comments/1owt21l/is_the_difference_between_etl_and_elt_purely/"}, {"id": "1owwdnx", "name": "t3_1owwdnx", "content": "Curious to know from folks based in the US / Europe if you guys still use Cloudera (Hive, Impala, HDFS) in your DE stack.\n\nJust moved to Asia from Australia as a DE consultant and was shocked at how widely adopted it still is in countries like Singapore, Thailand, Malaysia, Philippines, etc", "date_utc": 1763126631.0, "title": "Is Cloudera still Alive in US/EU?", "upvote_ratio": 0.96, "score": 22, "url": "https://www.reddit.com/r/dataengineering/comments/1owwdnx/is_cloudera_still_alive_in_useu/"}, {"id": "1ox5kta", "name": "t3_1ox5kta", "content": "I work for a small organization and we have built an ETL pipeline with Python and SQL for Power BI dashboards. Here is the current process:\n\nThere are multiple python scripts connected to each other by importing in-memory dataframes. One script runs multiple complex SQL queries concurrently and there are other scripts for transforming the data and uploading to SQL server. The pipeline transfers 3 MB of data each time since it queries the most recent data and takes 2 to 3 minutes to execute each day.\n\nThis is hard to automate because the databases require VPN which needs 2fa. So we have been working with the IT solutions team to automate the pipeline.\n\nThe easiest way to automate this would be to deploy the code onto a VM and have it run on a schedule. However, the solutions team has proposed a different approach with Azure Data Factory:\n\n* ADF orchestrator invokes \"Copy Data\" activity via self-hosted IR via to the source DB\n* Data is copied into Azure Blob Storage\n* Function App executes transformations in the Python scripts\n* Self-hosted IR invokes \"Copy Data\" with Source as transformed data and the SQL Server as the sink\n\nThe IT solutions deparment said this is the best approach because Microsoft supports PaaS over IaaS and there would be overhead of managing the VM.\n\nI am just wondering if this solution would be overkill because our pipeline is very small scale (only 3 MB of data transferred on each run) and we are not a large company.\n\nThe other problem is that nobody on the team knows Azure. Even though the IT solutions team will implement everything, it will still need to be maintained. The team consists of a business analyst who only knows SQL and not Python, a co-op student who changes every 4 months and myself. I am just a student who has worked here on many co-op and part time roles (currently part time). The business analyst delegates all the major technical tasks to the co-op students so when I leave, the pipeline will be managed by another co-op student who will only be there for 4 months.\n\nManagement currently support the ADF approach because it is Microsoft best practice. They believes that using a VM will not be best practice and they will need to hire another person to fix everything if it breaks. They also want to move to Fabric in the future for its AI/ML capabilities even though we can just build ML pipelines in Python.\n\nI am not sure if I am overthinking this or the ADF solution is truly overkill. I am fine with learning Azure technologies and not opposed to it but I want to build something that can be maintained.", "date_utc": 1763147454.0, "title": "Would using Azure Data Factory in this Context be Overkill?", "upvote_ratio": 0.84, "score": 4, "url": "https://www.reddit.com/r/dataengineering/comments/1ox5kta/would_using_azure_data_factory_in_this_context_be/"}, {"id": "1ox4uvz", "name": "t3_1ox4uvz", "content": "Hello everyone,\n\nI\u2019m a CTO of a small startup in South America (limited budget, of course) with a background in software development. While I have academic knowledge in Machine Learning, AI explicability, and related topics, I\u2019ve never worked on a professional data team or project. In most academic projects, we work with ready-to-use datasets, so I\u2019ve never had to think about creating datasets from scratch.\n\nWe\u2019re a 60-person company, with only 5 in tech, working in the accounting industry. We have four main applications, each with its own transactional Postgres database:\n- Backend: Serves a hybrid mobile/web app for customers and a back-office application for employees. It handles resources for customer enterprises and our in-house CRM.\n- Tasks: An internal task and process orchestration app (using Camunda).\n- CMS: A content system for website campaigns, offers, landing pages, etc.\n- Docs: An internal Wiki with markdown files documenting processes, laws, rules, etc.\n\nThe databases are relatively small for now: Backend has 120 tables, Tasks has 50, and most tables have around 500k rows from 4 years of operation. We\u2019ve plugged all of them into Metabase for BI reporting. \n\nWe have some TVs around the office with real-time dashboards refreshing every 30s (for example for the sales team tracks daily goals and our fiscal team tracking new urgent due tasks). Employees also use detailed tables for their day-to-day needs, often filtering and exporting to Excel.\n\nWe\u2019ve hit some bumps in our performance and need advice on how to scale efficiently. Most BI reports go through a view in the Backend database that consolidates all customer data, which contains many joins (20+) and CTEs. This setup works well enough for now, but I\u2019m starting to worry as we scale. On top of that, we have some needs to keep track tasks in our Camunda system that are late but only for delinquent customers, so I have to join the data from our Backend database. I've tried Trino/Presto for that but it had a really bad performance and now we are using a Postgres Foreign Data Wrapper and its working well so far... Joining data from our Camunda system with the Backend database to track late tasks, the query performance takes a big hit since it's going through the same consolidated view (it was either that or repeat the same joins over and over again). \n\nTo address this, we\u2019ve decided it\u2019s time to create a Data Warehouse to offload these heavy queries from the databases. We\u2019re using read replicas, indexes, etc., but I want to create a robust structure for us to grow.\n\nAdditionally, we\u2019re planning to integrate data from other sources like Google Analytics, Google Ads, Meta Ads, partner APIs (e.g., WhatsApp vendor), and PDF content (tax guides, fiscal documents, bank reports, etc.). We\u2019d like to use this data for building ML models and RAG (Retrieval-Augmented Generation), etc.\n\nWe\u2019ve also been exploring the idea of a Data Lake to handle the raw, unstructured data. I\u2019m leaning toward a medallion architecture (Bronze-Silver-Gold layers) and pushing the \"Gold\" datasets into an OLAP database for BI consumption. The goal would be to also create ML-ready datasets in Parquet format.\n\nCost is a big factor for us. Our current AWS bill is under USD 1K/month, which covers virtual machines, databases, cloud containers, etc. We\u2019re open to exploring other cloud providers and potentially multi-cloud solutions, but cost-effectiveness is key.\n\nI\u2019m studying a lot about this but am unsure of the best path forward, both in terms of architecture and systems to use. Has anyone dealt with a similar scenario, especially on a budget? Should we focus on building a Data Warehouse first, or would implementing a Data Lake be more beneficial for our use case? What tools or systems would you recommend for building a scalable, cost-efficient data pipeline? Any other advice or best practices for someone with an academic background but limited hands-on experience in data engineering?\n\nThanks in advance for any tip", "date_utc": 1763145844.0, "title": "Need tips on a hybrid architecture for both real-time BI and ML", "upvote_ratio": 0.8, "score": 6, "url": "https://www.reddit.com/r/dataengineering/comments/1ox4uvz/need_tips_on_a_hybrid_architecture_for_both/"}, {"id": "1owxx7s", "name": "t3_1owxx7s", "content": "How common is working with EDI for you guys?  I've been in data engineering for about 10 yrs, but only started seeing it at my current company when I joined about a year ago.  Training resources are a pain.  Curious how I've made it this long without seeing it or really even hearing about it until now?", "date_utc": 1763130449.0, "title": "EDI in DE", "upvote_ratio": 0.86, "score": 10, "url": "https://www.reddit.com/r/dataengineering/comments/1owxx7s/edi_in_de/"}, {"id": "1ox1s0b", "name": "t3_1ox1s0b", "content": "Hello all. I am working for a bank where we collect financial statements from our borrowers (Balance Sheet, P&L), in the format of spreadsheet, every quarter. \n\nI would like to\n\n  \n1. Standardize those statements, like aggregating some sub-items into more generic line items (ex. some companies have their own specific expenses, but just aggregating them into \"other operational expense\")\n\n2. load those standardized statements to some central place\n\n3. And do time series analyses within one company\n\n4. or comparing one company's performance to that of the other or that of a group of others.\n\nAny good ideas how to do this?\n\nRight now,\n\nI am just using Excel, one sheet has all the columns for line items for financial statements and some columns for quarter, year and company name, and I input borrowers' financial statements line item matching those columns, and have another sheet to bring those data and do some analysis. It does its job, but I am pretty sure there is a better way.\n\n", "date_utc": 1763139040.0, "title": "Best way to store financial statements and do some timeseries / benchmark analyses", "upvote_ratio": 0.88, "score": 6, "url": "https://www.reddit.com/r/dataengineering/comments/1ox1s0b/best_way_to_store_financial_statements_and_do/"}, {"id": "1owx9ut", "name": "t3_1owx9ut", "content": "Each year there is a new playlist for this course. As someone who's just getting started, would you recommend a particular playlist (2022,2023) or should I just watch the latest (2025). Or has the quality remained the same throughout?\n\n  \nIt's possible 2025 would be the latest and most updated version so I'm going to stick with it", "date_utc": 1763128890.0, "title": "CMU Intro to Database Systems", "upvote_ratio": 0.75, "score": 6, "url": "https://www.reddit.com/r/dataengineering/comments/1owx9ut/cmu_intro_to_database_systems/"}, {"id": "1ox02g8", "name": "t3_1ox02g8", "content": "I\u2019m attempting to standardize warehouse column names. Picking a clean and consistent way to name\u00a0date-only\u00a0vs\u00a0datetime fields is clashing with my OCD.\n\nOptions I\u2019m considering:\n\n* \\*\\_date\u00a0and\u00a0\\*\\_datetime\u00a0(most symmetrical)\n* \\*\\_on\u00a0and\u00a0\\*\\_at\u00a0(reads nicely but less standard)\n* \\*\\_date\u00a0and\u00a0\\*\\_at\u00a0(common but mismatched\n\nThank you!", "date_utc": 1763135303.0, "title": "[Naming Conventions] Date & Datetime Fields", "upvote_ratio": 1.0, "score": 5, "url": "https://www.reddit.com/r/dataengineering/comments/1ox02g8/naming_conventions_date_datetime_fields/"}, {"id": "1ox5wiq", "name": "t3_1ox5wiq", "content": "Hello Data Lords,\n\nBecoming a data engineer has been on my mind long enough, it\u2019s time to ask the community.\n\nI am a GIS consultant for a civil engineering firm earning 81k/year in a MCOL city. The job is steady but it seldom challenges me anymore. While I understand data engineers tend to earn more than me, I also get a yearly raise around 7% and a new title every 2 years or so that constitutes around a 12% raise. Would my salary keep up in the data engineering industry? My perspective is more long term. For additional context, I am fully vested in my company as a regular full time employee. \n\nAlmost every project I work on, I use Python to automate data workflows, manipulate data, etc. so I have a background working with data.\n", "date_utc": 1763148204.0, "title": "GIS Consulting to Data Engineering Salary", "upvote_ratio": 0.67, "score": 2, "url": "https://www.reddit.com/r/dataengineering/comments/1ox5wiq/gis_consulting_to_data_engineering_salary/"}, {"id": "1ox173p", "name": "t3_1ox173p", "content": "I don't expect it to work 100% i am looking for user assisted mode but i am wondering if there is some literature on strategies to do it?  \nI have some heuristics like type of column, number of columns, header name etc. to limit the choice and but looking for something better.\n\nBackground is i have created app for small data (less than million rows) and it makes dashboard creation from data by doing lot of magic behind the scenes. It also allows multiple sources but currently they are disjoint despite in same dashboard and i am getting lot of requests to support defining relations unfortunately lot of users are non technical and will be confused when asked to define data model. ", "date_utc": 1763137779.0, "title": "Is there a way to auto create data model from schemas of sources?", "upvote_ratio": 0.78, "score": 5, "url": "https://www.reddit.com/r/dataengineering/comments/1ox173p/is_there_a_way_to_auto_create_data_model_from/"}, {"id": "1ox9ps0", "name": "t3_1ox9ps0", "content": "As head of data engineering, for years I am working with Iceberg in Both Chase UK and Revolut, but integrating for non-critical projects meant dealing with Java dependencies and complex infrastructure that I don't want to waste time on. I wanted something that would work in pure Python without all the overhead, please take a look at it, you may find it useful:\n\n## links:\n\n- source: [github.com/rodmena-limited/DataShard](https://github.com/rodmena-limited/DataShard)\n- docs: [datashard.readthedocs.io](https://datashard.readthedocs.io/en/latest/index.html)\n\n## install\n`pip install datashard`\n\n\n## Contribute\nI am also looking for a maintainer, so don't be shy to DM me.", "date_utc": 1763157118.0, "title": "Iceberg-Inspired Safe Concurrent Data Operations for Python / DataShard", "upvote_ratio": 0.67, "score": 1, "url": "https://www.reddit.com/r/dataengineering/comments/1ox9ps0/iceberginspired_safe_concurrent_data_operations/"}, {"id": "1owaz2q", "name": "t3_1owaz2q", "content": "I've been seeing mention of \"data products\" and \"data contracts\" for some time.  I think I get the concepts, but... \ud83e\udd37\u200d\u2642\ufe0f\n\nHow far off am I?\n\nData product:  Something valuable using data?  Tangible?  Physical? What's \"physical\" when we're talking about virtual, digital things?  Is it a dataset/model, report, or something more?  Is this just a different word for \"solution\"?  Is it just the terminology for those things nowadays?\n\nData contract:  This is some kind of agreement that data producer/provider doesn't change a data structure/schema without due process involving the data consumer?  Do people actually do this, to good effect?  I deal with source data where the vendor changes shit willy-nilly.  And other sources where business users can create the dreaded custom field.  Maybe I'm cynical, but I can't see these parties changing those practices readily.\n\n  \nEDIT: I was prompted to post, because a little while ago I looked over this older post about data products (archived, now).  \n[https://www.reddit.com/r/dataengineering/comments/1flolf6/what\\_is\\_a\\_data\\_product\\_in\\_your\\_experience/](https://www.reddit.com/r/dataengineering/comments/1flolf6/what_is_a_data_product_in_your_experience/)\n\n  \nThanks for all the responses so far!", "date_utc": 1763063317.0, "title": "Explain like I'm 5: What are \"data products\" and \"data contracts\"", "upvote_ratio": 0.95, "score": 86, "url": "https://www.reddit.com/r/dataengineering/comments/1owaz2q/explain_like_im_5_what_are_data_products_and_data/"}, {"id": "1ox7jsv", "name": "t3_1ox7jsv", "content": "Hey folks,  \nI've just published my first medium article with the topic how to scale relational databases:  \n[https://medium.com/@ysacherer/postgres-scalability-scaling-reads-c13162c58eaf](https://medium.com/@ysacherer/postgres-scalability-scaling-reads-c13162c58eaf)\n\nI am open for discussions, feedback and a like ;)", "date_utc": 1763151974.0, "title": "Postgres Scalability \u2014 Scaling Reads", "upvote_ratio": 0.33, "score": 0, "url": "https://www.reddit.com/r/dataengineering/comments/1ox7jsv/postgres_scalability_scaling_reads/"}, {"id": "1ox1e9h", "name": "t3_1ox1e9h", "content": "I've seen a lot of articles talking about how one can absolutely optimize their streaming pipelines by using Parquet as the input format. We all know that the advantage of Parquet is that a parquet file stores data in columns, so each column can be decompressed individually and that makes for very fast and efficient access.\n\nOK, but Kafka doesn't care about that. As far as I know, if you send a Parquet file through Kafka, you cannot modify anything in that file before it is deserialized. So you cannot do column pruning or small reads. You essentially lose every single benefit of Parquet.\n\nSo why do these articles and guides insist about using Parquet with Kafka?", "date_utc": 1763138201.0, "title": "Is it not pointless to transfer Parquet data with Kafka?", "upvote_ratio": 0.53, "score": 1, "url": "https://www.reddit.com/r/dataengineering/comments/1ox1e9h/is_it_not_pointless_to_transfer_parquet_data_with/"}, {"id": "1owo25o", "name": "t3_1owo25o", "content": "working on a b2b marketplace for industrial equipment. need to aggregate product catalogs from supplier sites. 40 suppliers, about 50k products total.\n\nevery supplier structures their data differently. some use tables, some bullet points, some put specs in pdfs. one supplier has dimensions as \"10x5x3\", another has separate fields. pricing is worse - volume discounts, member pricing, regional stuff all over the place.\n\nbeen building custom parsers but doesnt scale. supplier redesigns their site, parser breaks. spent 3 days last week on one who moved everything to js tabs.\n\ntried gpt4 for extraction. works ok but expensive and hallucinates. had it make up a weight spec that wasnt there. cant have that.\n\ncurrent setup is beautifulsoup for simple sites, playwright for js ones, manual csv for suppliers who block us. its messy.\n\nalso struggling with change detection. some suppliers update daily, others weekly. reprocessing 50k products when maybe 200 changed is wasteful.\n\nhow do you guys handle multi-source data aggregation when schemas are all different? especially curious about change detection strategies", "date_utc": 1763097917.0, "title": "scraping 40 supplier sites for product data - schema hell", "upvote_ratio": 0.7, "score": 5, "url": "https://www.reddit.com/r/dataengineering/comments/1owo25o/scraping_40_supplier_sites_for_product_data/"}, {"id": "1ow73mi", "name": "t3_1ow73mi", "content": "I\u2019ve been learning Kafka recently and got curious about how it works under the hood. Two things are confusing me:\n\n1. Kafka stores all messages in an append-only log, right? But if I want to \u201creplay\u201d millions of messages from the past, how does it do that efficiently without slowing down new writes or consuming huge memory? Is it just sequential disk reads, or is there some smart indexing happening?\n\n2. I get that Kafka can distribute topics across multiple brokers, and consumers can scale horizontally. But if I\u2019m only working with a single node, or a small dataset, what real benefits does Kafka give me over just using a database table as a queue? Are there other patterns or advantages I\u2019m missing beyond multi-node scaling?\n\nI\u2019d love to hear from people who\u2019ve used Kafka in production \u2014 how it manages these log mechanics, replaying messages, and what practical scenarios make Kafka truly excels.", "date_utc": 1763054687.0, "title": "If Kafka is a log-based system, how does it \u201creplay\u201d messages efficiently \u2014 and what makes it better than just a database queue?", "upvote_ratio": 0.92, "score": 44, "url": "https://www.reddit.com/r/dataengineering/comments/1ow73mi/if_kafka_is_a_logbased_system_how_does_it_replay/"}, {"id": "1owkwp9", "name": "t3_1owkwp9", "content": "I am the Dean of STEM at a HS in Chicago. We're an independent charter school and since we'd just split with our previous network we are rebuilding. \n\nThough the admin doesn't seem to understand the amount of repetitive, mindless, and repetitious work that is done on a daily basis for everything from the lack of basic workflows, automations, and the consolidation of all of the data we acquire on attendance, grades, standardized test scores, behavior, etc. could both benefit our school and alleviate a lot of work for a lot of individuals. \n\nDoes anyone know of any resources, information, or quite literally any helpful ideas for determining where to begin? \n\nI am well versed in excel and sheets, I'm moderately capable with basic automations and workflows, although I haven't spent much time yet learning how to use app scripts, API's, nor how to go about developing a system of data consolidation in which the data is being collected using different platforms. \n\nFor instance our LMS is Powerschool which also serves as our SIS although we use a platform called Dean's list for behavioral monitoring. Additionally our standardized test scores come from 2 different sources. \n\n  \nAny help, direction, etc would be incredibly helpful. If I wasn't swamped and overwhelmed with all of my other duties I would take the time to learn it all on my own but we operate so stupidly and in such disorganization most hours of my day are spent doing things that could easily be incorporated into workflows, if I could figure out how to use the API's to allow data to be shared with various platforms(google workspace, Powerschool, Dean's list, etc). ", "date_utc": 1763088387.0, "title": "In need of info/support/direction for high school data engineering system", "upvote_ratio": 0.71, "score": 4, "url": "https://www.reddit.com/r/dataengineering/comments/1owkwp9/in_need_of_infosupportdirection_for_high_school/"}, {"id": "1ows3en", "name": "t3_1ows3en", "content": "Hello All, I am a complete beginner and I need help with the following process please. \n\nGoal - Build a dashboard in Power BI \n\nBackground - Client has a retail business and has 25 branches in the country. Each branch uses a POS and we get three files for each branch. Invoice, Invoice Line and Invoice Customer. Initially client was sending excel files with three tabs in it. May be because their Intern or Junior was working on creating these files the files were very erroneous. We had a meeting discussed a few solutions and decided that the client will upload sales data files to the FTP server. \n\nCurrent Process - \n\u2022 Download files from FTP to Local folder named Raw. \n\u2022 Use Python script to add two new columns - Branch Name and Branch Code. \n\u2022 We achieve this by including a dictionary in python code that adds these columns based on file names. For example - file name 045_inv.csv then Manhattan since code for Manhattan is 045. We \nrepeat this for invoice line and invoice customer. \n\u2022 Save these to a new local folder - Processed \n\u2022 Use Python script to read files from Processed load them to PGSql db containing three tables - invoice, invoice_line, invoice_customer\n\u2022 Three python scripts for three tables\n\nMy Request - \n\n1) How can I make this process smoother and more seamless? \n2) What is the best way to automate this? \n3) what checks can I perform to ensure that data health and accuracy is maintained \n", "date_utc": 1763112907.0, "title": "Need help with the following process - I\u2019m a complete beginner", "upvote_ratio": 0.5, "score": 0, "url": "https://www.reddit.com/r/dataengineering/comments/1ows3en/need_help_with_the_following_process_im_a/"}, {"id": "1owjt0b", "name": "t3_1owjt0b", "content": "Anyone here used or built a text to sql ai agent?\n\nA lot of talk at the moment in my shop about it. The issue is that we have a data swamp. Trying to wrangle docs, data contracts, lineage and all that stuff but wondering is anyone done this and have it working?\n\nMy thinking is that the LLM given the right context can generate the sql, but not from the raw logs or some of the downstream tables\n", "date_utc": 1763085296.0, "title": "Text to SQL Agents?", "upvote_ratio": 0.62, "score": 5, "url": "https://www.reddit.com/r/dataengineering/comments/1owjt0b/text_to_sql_agents/"}, {"id": "1ow6dzm", "name": "t3_1ow6dzm", "content": "I guess there are way too many of them for designing data warehouse based on that book, but in my job I mostly work on transactional DBs like Postgres", "date_utc": 1763053109.0, "title": "Any playlist suggestions for mastering data modelling for transactional databases?", "upvote_ratio": 0.93, "score": 12, "url": "https://www.reddit.com/r/dataengineering/comments/1ow6dzm/any_playlist_suggestions_for_mastering_data/"}, {"id": "1owfezf", "name": "t3_1owfezf", "content": "I currently serve as a Data Engineer at this well-funded startup. I am nearing completion of my software engineering degree, and my net salary is $1,500 USD per month, which is a competitive salary for a Junior role in my country.\nThe CDO recently informed me that the company plans to hire either a Director of Business Intelligence (BI) or a Senior Data Scientist. Crucially, the final hiring decision is contingent upon the career path I choose to pursue within the company, based on my current responsibilities.\nTeam Structure and Responsibilities\nOur current technical data team consists of three individuals: the CDO, myself, and a colleague focused on dashboarding and visualization, who will soon be transitioning to another sector within the organization.\nFor the past four months, I have been solely responsible for the conception and implementation of our data infrastructure architecture, including the deployment of all initial ETL pipelines. A substantial amount of work remains, with numerous ETL pipelines still needing to be developed. If I choose to handle this volume of work entirely on my own and maintain my current pace, there is a risk of significant burnout.\n\nTo elevate my expertise and ensure I am making robust technical decisions, I plan to obtain the GCP Data Engineer Certification in the coming months.\nI am proficient in programming, system integration, problem-solving, and I am growing confident in pipeline implementation. However, I occasionally question this confidence, wondering if it stems from the repetitive nature of the process or the current absence of a direct manager to provide supervision and critical technical oversight.\nI was quite concerned when the CDO asked me to define the role I should assume starting next month, given the upcoming senior hire.\n\n \n* Should I assume the leadership risk and position myself to manage the new senior hire (e.g., as a Team Lead or BI Manager)?\n * Should I explore an alternative career trajectory, such as transitioning toward a Data Scientist role?\n * What critical internal questions should I ask myself to ensure I make the most informed decision about my future path?\n*Should I ask for a salary update? of how much? 15%?\n\nI think they see me with leadership potential but I definitely think that I need to improve as a DE to have more confidence in myself. The CDO is a really nice boss and I really enjoy to work at my own pace.", "date_utc": 1763073692.0, "title": "What are my options", "upvote_ratio": 0.63, "score": 2, "url": "https://www.reddit.com/r/dataengineering/comments/1owfezf/what_are_my_options/"}, {"id": "1owgac5", "name": "t3_1owgac5", "content": "Hi All, I wrote a medium article about saving 90% on Data Warehouse and Lakehouses. Would like to get some feedback if the article is clear, useful or suggestions for improvements.\n\nHere the link: [https://medium.com/@klaushofenbitzer/save-up-to-90-on-your-data-warehouse-lakehouse-with-an-in-process-database-duckdb-63892e76676e?postPublishedType=initial](https://medium.com/@klaushofenbitzer/save-up-to-90-on-your-data-warehouse-lakehouse-with-an-in-process-database-duckdb-63892e76676e?postPublishedType=initial)\n\nI wanted to address the problem that data warehouses and lakehouses like Databricks, Snowflake or even AWS Athena are quite expensive at scale and that by using an in-process database for certain use cases like batch transformation or data pipeline workloads can done with cheaper solutions like DuckDB. Through open-data formats like parquet or iceberg the created tables can still be served in your data warehosue without needing to move on transform the data.", "date_utc": 1763075868.0, "title": "Medium Article: Save up to 90% on your Data - Warehouse/Lakehouse", "upvote_ratio": 0.57, "score": 2, "url": "https://www.reddit.com/r/dataengineering/comments/1owgac5/medium_article_save_up_to_90_on_your_data/"}, {"id": "1ow33mg", "name": "t3_1ow33mg", "content": "Hi everyone,\n\nI\u2019m working with dbt and Snowflake, and I have an incremental model (materialized='incremental', incremental_strategy='insert_overwrite') that selects from a source table. One of the columns, MONTH_START_DATE, is currently TIMESTAMP_NTZ(9) in Snowflake.\nI changed the source model and the column MONTH_START_DATE is now DATE datatype \n\nAfter doing this I am getting an error:\n\nSQL compilation error: cannot change column MONTH_START_DATE from type TIMESTAMP_NTZ(9) to DATE\n\nHow can I fix this?", "date_utc": 1763045552.0, "title": "Snowflake + dbt incremental model: error cannot change type from TIMESTAMP_NTZ(9) to DATE", "upvote_ratio": 0.76, "score": 8, "url": "https://www.reddit.com/r/dataengineering/comments/1ow33mg/snowflake_dbt_incremental_model_error_cannot/"}, {"id": "1owe39q", "name": "t3_1owe39q", "content": "Hi guys, can we ingest data from MongoDB(self-hosted on EC2) collections and store it in S3?. The collection has around 430million documents but I'll be extracting new data on daily basis which will be around 1.5 Gb. Can I do it using visual, notebook or script? Thanks", "date_utc": 1763070502.0, "title": "Data ingestion using AWS Glue", "upvote_ratio": 0.76, "score": 2, "url": "https://www.reddit.com/r/dataengineering/comments/1owe39q/data_ingestion_using_aws_glue/"}, {"id": "1ow7ngl", "name": "t3_1ow7ngl", "content": "Hey Everyone,\n\nI have written several scheduled queries in BigQuery that run daily. I now intend to preprocess this data using PySpark and store the output in Google Cloud Storage (GCS). There are eight distinct datasets in BigQuery table that need to be stored separately within the same folder in GCS. \n\nI am uncertain which tool to use in this scenario, as this is my first time building a data pipeline. Should I use Dataproc, or is there a more suitable alternative?\n\nI plan to run the above process on a daily basis, if that context is helpful. I have tested the entire workflow locally, and everything appears to be functioning correctly. I am now looking to deploy this process to the cloud.\n\nThank you!", "date_utc": 1763055905.0, "title": "Building a Data Pipeline from BigQuery to Google Cloud Storage", "upvote_ratio": 0.57, "score": 1, "url": "https://www.reddit.com/r/dataengineering/comments/1ow7ngl/building_a_data_pipeline_from_bigquery_to_google/"}, {"id": "1ovykw8", "name": "t3_1ovykw8", "content": "I've written a Medium article going through the 5 Snowflake features I'm most excited about and those which I think will have the biggest impact on how we use Snowflake:   \n\u2705Openflow  \n\u2705Managed dbt  \n\u2705Workspaces  \n\u2705Snowflake Intelligence  \n\u2705Pandas Hybrid Execution", "date_utc": 1763033016.0, "title": "Do you know what the 5 most important Snowflake features are for 2026?", "upvote_ratio": 0.59, "score": 8, "url": "https://medium.com/@tom.bailey.courses/the-5-snowflake-features-that-will-define-2026-a1b720111a0b"}, {"id": "1ow6z4t", "name": "t3_1ow6z4t", "content": "I've been working on creating a database from scratch for a month or two.\n\nIt started out as a JSON-based database with the data persisting in-memory and updates being written to disk on every update. I soon realized how unrealistic the implementation of it was, especially if you have multiple collections with millions of records each. That's when I started the journey of learning how databases are implemented.\n\nAfter a few weeks of research and coding, I've completed the first version of my file-based database. This version is append-only, using LSN to insert, update, delete, and locate records. It also uses a B+ Tree for collection entries, allowing for fast ID:LSN lookup. When the B+ Tree reaches its max size (I've set it to 1500 entries), the tree will be encoded (using my custom encoder) and atomically written to disk before an empty tree takes the old one's place in-memory.\n\nI'm sure I'm there are things that I'm doing wrong, as this is my first time researching how databases work and are optimized. So, I'd like feedback on the code or even the concept of this library itself.\n\nJust wanna state that this wasn't vibe-coded at all. I don't know whether it's my pride or the fear that AI will stunt my growth, but I make a point to write my code myself. I did bounce ideas off of it, though. So there's bound to be some mistakes made while I tried to implement some of them.", "date_utc": 1763054414.0, "title": "Feedback on JS/TS class-driven file-based database", "upvote_ratio": 0.81, "score": 3, "url": "https://github.com/neisanworks/neisandb/pkgs/npm/neisandb"}, {"id": "1ovuncj", "name": "t3_1ovuncj", "content": "My company's chosen me (a data scientist) to set up an entire data pipeline to help with internal matters.\n\n  \nThey're looking for -  \n1. A data lake/warehouse where data from multiple integrated systems is to be consolidated  \n2. Data archiving/auditing  \n3. Automated invoice generation  \n4. Visualization and Alert generation  \n5. An API that can be used to send data outbound from the DWH  \n6. Web UI (For viewing data, generating invoices)\n\nMy company will only use self-hosted software.\n\n  \nWhat would be the most optimal pipeline to set this up considering the requirements above and also the fact that this is only my second time setting up a data pipeline (my first one being much less complex). What are the components I need to consider and what are some of the industry norms in terms of software for those components.\n\n  \nI'd appreciate any help. Thanks in advance", "date_utc": 1763017943.0, "title": "Looking for some guidance regarding a data pipeline", "upvote_ratio": 0.95, "score": 19, "url": "https://www.reddit.com/r/dataengineering/comments/1ovuncj/looking_for_some_guidance_regarding_a_data/"}, {"id": "1ow9xfq", "name": "t3_1ow9xfq", "content": "let's begin", "date_utc": 1763060921.0, "title": "Day - 5 Winter Arc (Becoming a Skilled Data Engineer)", "upvote_ratio": 0.4, "score": 0, "url": "https://youtube.com/watch?v=Lg34FzzGqNs&si=_1fT4arFakYS4IEr"}, {"id": "1ovxzl7", "name": "t3_1ovxzl7", "content": "I keep running into conflicting opinions on this, so I\u2019m curious how other teams actually handle it in practice.\n\nContext: think of a product with\u00a0**EU customers and non-EU engineers**, or generally a setup where data residency / GDPR matters and you still need to debug real issues in production.\n\nI\u2019d love to hear how your org does things around:\n\n**1. Where you are vs where the data is**\n\n* Which country/region are\u00a0*you*\u00a0working from?\n* Where is your main production DB / warehouse for that data (EU region, US, multiple regions, etc.)?\n\n**2. Who gets to touch production data**\n\n* Do individual engineers have\u00a0**direct access**\u00a0to prod DBs/warehouses/logs, or is it mostly through internal tools / dashboards?\n* Is access\u00a0**permanent**\u00a0(e.g. read-only role you always have) or\u00a0**on-demand / temporary**\u00a0(someone grants it when needed)?\n* Are credentials shared (team accounts, jump boxes) or strictly individual + SSO?\n\n**3. Debugging real issues**\n\nWhen you hit a bug that\u00a0*only*\u00a0shows up with real production data, what do you actually do?\n\n* Point a debug build at prod?\n* Query the prod DB/warehouse directly?\n* Ask a DBA / data / platform team to pull what you need? How often does this happen for you (roughly per week / month)?\n\n**4. Data residency / regional rules in practice**\n\nIf you\u2019re\u00a0**outside**\u00a0the region where the data \u201cshould\u201d live (e.g. you\u2019re in the US/UK, data is \u201cEU-only\u201d): what\u2019s the real process?\n\n* You still query it directly (VPN/bastion/etc.)\n* Someone\u00a0*in-region*\u00a0runs queries / exports for you\n* You rely on pre-built tooling / dashboards and never see raw rows Are there any \u201cunofficial\u201d workflows (Slack messages like \u201chey can you run this query for me from EU?\u201d)?\n\n**5. Guardrails & horror stories**\n\n* Do you have masking / RLS / separate views specifically for debugging?\n* Any guardrails like time-limited accounts, strict audit logs, approvals, etc.?\n* Have you seen any near-misses or incidents related to prod access (accidental UPDATE without WHERE, GDPR concerns, etc.)?\n\n**6. If you could change one thing**\n\n* If you had a magic wand, what\u2019s the first thing you\u2019d fix in your current \u201cdebugging with prod data\u201d setup? (Could be policy, tooling, process, anything.)\n\nFeel free to anonymize company names, but rough industry and team size (e.g. \u201cEU fintech, \\~50 engineers\u201d or \u201cUS B2B SaaS, mixed EU/US users\u201d) would be super helpful for context.\n\nReally curious how different teams balance \u201cwe need real prod data to fix this\u201d with \u201cwe don\u2019t want everyone to have God-mode on prod\u201d.", "date_utc": 1763030911.0, "title": "How does your team handle debugging with production data across regions (esp. EU vs non-EU)?", "upvote_ratio": 0.67, "score": 4, "url": "https://www.reddit.com/r/dataengineering/comments/1ovxzl7/how_does_your_team_handle_debugging_with/"}, {"id": "1ovm05s", "name": "t3_1ovm05s", "content": "Anyone have a mature data product practice within their organizations and willing to share how they operate?  I am curious how orgs are handling the release of new data assets and essentially marketing on behalf of the data org. My org is heading in this direction and I\u2019m not quite sure what will resonate with the business and our customers (Data Scientists, business intelligence, data savvy execs and leaders\u2026and now other business users who want to use datasets within MS copilot).  \n\nAlso curious if you\u2019ve found success with any governance tooling that has a \u201cmarketplace\u201d and how effective it is.\n\nIt all sounds good in theory and really changes the dynamic of the DE team as order takers and more of true partners, so I\u2019m motivated from that sense (cautiously optimistic overall). ", "date_utc": 1762992182.0, "title": "Data Product Management", "upvote_ratio": 0.81, "score": 9, "url": "https://www.reddit.com/r/dataengineering/comments/1ovm05s/data_product_management/"}, {"id": "1ovdd9k", "name": "t3_1ovdd9k", "content": "What\u2019s your personal growth hack? What are the things that folks overlook or you see as an impediment to career advancement?", "date_utc": 1762972517.0, "title": "What\u2019s your growth hack?", "upvote_ratio": 0.72, "score": 19, "url": "https://www.reddit.com/r/dataengineering/comments/1ovdd9k/whats_your_growth_hack/"}, {"id": "1ovb4ww", "name": "t3_1ovb4ww", "content": "Hey everyone, I\u2019m data scientinst and master\u2019s student in CS and have been maintaining, pretty much on my own, a research project that uses machine learning with climate data. The infrastructure is very \"do it yourself\", and now that I\u2019m near the end of my degree, the data volume has exploded and the organization has become a serious maintenance problem.\n\nCurrently, I have a Linux server with a /data folder (\\~800GB and growing) that contains:\n\n* Climate datasets (NetCDF4, HDF5, and Zarr) \u2014 mainly MERRA-2 and ERA5, handled through Xarray;\n* Tabular data and metadata (CSV, XLSX);\n* ML models (mostly Scikit-learn and PyTorch pickled models);\n* A relational database with experiment information.\n\nThe system works, but as it grew, several issues emerged:\n\n* Data ingestion and metadata standardization are fully manual (isolated Python scripts);\n* Subfolders for distributing the final application (e.g., a reduced /data subset with only one year of data, \\~10GB) are manually generated;\n* There\u2019s no version control for the data, so each new processing step creates new files with no traceability;\n* I\u2019m the only person managing all this \u2014 once I leave, no one will be able to maintain it.\n\nI want to move away from this \u201cmessy data folder\u201d model and build something more organized, readable, and automatable, but still realistic for an academic environment (no DevOps team, no cloud, just a decent local server with a few TB of storage).\n\nWhat I\u2019ve considered so far:\n\n* A full relational database, but converting NetCDF to SQL would be absurdly expensive in both cost and storage.\n* A NoSQL database like MongoDB, but it seems inefficient for multidimensional data like netcdf4 datasets.\n* The idea of a local data lake seems promising, but I\u2019m still trying to understand how to start and what tools make sense in a research (non-cloud) setting.\n\nI\u2019m looking for a structure that can:\n\n* Organize everything (raw, processed, outputs, etc.);\n* Automate data ingestion and subset generation (e.g., extract only one year of data);\n* Provide some level of versioning for data and metadata;\n* Be readable enough for someone else to understand and maintain after me.\n\nHas anyone here faced something similar with large climate datasets (NetCDF/Xarray) in a research environment?  \nShould I be looking into a non-relational database?\n\nAny advice on architecture, directory standards, or tools would be extremely welcome \u2014 I find this problem fascinating and I\u2019m eager to learn more about this area, but I feel like I need a bit of guidance on where to start.", "date_utc": 1762967753.0, "title": "Organizing a climate data + machine learning research project that grew out of control", "upvote_ratio": 0.89, "score": 19, "url": "https://www.reddit.com/r/dataengineering/comments/1ovb4ww/organizing_a_climate_data_machine_learning/"}, {"id": "1ovoa7q", "name": "t3_1ovoa7q", "content": "I was tasked to move a huge 50gb csv file from ADLS to on-prem sql server. I was using Self hosted IR in ADF and the target table was truncated before loading the data.\n\n  \nI tried and tested few configuration changes:\n\nIn first case I kept everything as default but immediately after 10 minutes I got an error: An existing connection was forcibly closed by the remote host\n\nIn second try, I enabled bulk insert and set the batch size to 20000, but still failed with same error.\n\n  \nIn third try, I kept all the settings same as 2, but this time changed the max concurrent connections from blank to 10 and it worked.\n\n  \nI can't figure out why changing max concurrent connections to 10 worked because adf automatically chooses the appropriate connections based on the data. Is it true or it only takes 1 until we explicitly provide it?", "date_utc": 1762998218.0, "title": "Why setting Max concurrent connections to 10 fixed my ADLS \u2192 On-Prem SQL copy\u201d", "upvote_ratio": 0.75, "score": 4, "url": "https://www.reddit.com/r/dataengineering/comments/1ovoa7q/why_setting_max_concurrent_connections_to_10/"}, {"id": "1ov19lh", "name": "t3_1ov19lh", "content": "I\u2019ve learn Spark today my manger ask me these two question and  i got a bit confused about how its \u201clazy evaluation\u201d actually works.\n\nIf Spark is lazy and transformations are lazy too, then how does it read a file and infer schema or column names when we set `inferSchema = true`?  \nFor example, say I\u2019m reading a 1 TB CSV file \u2014 Spark somehow figures out all the column names and types *before* I call any action like `show()` or `count()`.  \nSo how is that possible if it\u2019s supposed to be lazy? Does it partially read metadata or some sample of the file eagerly?\n\nAlso, another question that came to mind \u2014 both Python (Pandas) and Spark can store data in memory, right?  \nSo apart from distributed computation across multiple nodes, what else makes Spark special?  \nLike, if I\u2019m just working on a single machine, is Spark giving me any real advantage over Pandas?\n\nWould love to hear detailed insights from people who\u2019ve actually worked with Spark in production \u2014 how it handles schema inference, and what the \u201creal\u201d benefits are beyond just running on multiple nodes.", "date_utc": 1762941827.0, "title": "If Spark is lazy, how does it infer schema without reading data \u2014 and is Spark only useful for multi-node memory?", "upvote_ratio": 0.96, "score": 46, "url": "https://www.reddit.com/r/dataengineering/comments/1ov19lh/if_spark_is_lazy_how_does_it_infer_schema_without/"}, {"id": "1ow9fbn", "name": "t3_1ow9fbn", "content": "One of my core engineering principles is that building with no dependencies is faster, more reliable, and easier to maintain at scale. It\u2019s an aesthetic choice that also influences architecture and engineering.\u00a0\n\nOver the past year, I\u2019ve been developing my open source data transformation project, Hyperparam, from the ground up, depending on nothing else. That\u2019s why it\u2019s small, light, and fast. It\u2019s minimal software.\n\nI\u2019m interested how others approach this: do you optimize for simplicity or integration?", "date_utc": 1763059798.0, "title": "Anyone else building with zero dependencies?", "upvote_ratio": 0.37, "score": 0, "url": "https://www.reddit.com/r/dataengineering/comments/1ow9fbn/anyone_else_building_with_zero_dependencies/"}, {"id": "1ov1ug0", "name": "t3_1ov1ug0", "content": "Hi everyone,  \nI\u2019ve spent the last few weeks talking with a friend about the lack of a standard for data transformations.\n\nOur conversation started with the Fivetran + dbt merger (and the earlier acquisition of SQLMesh): what alternative tool is out there? And what would make me confident in such tool?\n\nSince dbt became popular, we can roughly define a transformation as:\n\n* a SELECT statement\n* a schema definition (optional, but nice to have)\n* some logic for materialization (table, view, incremental)\n* data quality tests\n* and other elements (semantics, unit tests, etc.)\n\nIf we had a standard we could move a transformation from one tool to another, but also have mutliple tools work together (interoperability).\n\nHonestly, I initially wanted to start building a tool, but I forced myself to sit down and first write a standard for data transformations. Quickly, I realized the specification also needed to include tests and UDFs (this is my pet peeve with transformation tools, UDF are part of my transformations).\n\nIt\u2019s just an initial draft, and I\u2019m sure it\u2019s missing a lot. But it\u2019s open, and I\u2019d love to get your feedback to make it better.\n\nI am also bulding my open source tool, but that is another story.", "date_utc": 1762944022.0, "title": "Introducing Open Transformation Specification (OTS) \u2013 a portable, executable standard for data transformations", "upvote_ratio": 0.83, "score": 33, "url": "https://github.com/francescomucio/open-transformation-specification"}, {"id": "1ov7fxk", "name": "t3_1ov7fxk", "content": "I am currently working as a data engineer and just started on migration and modernizing our data moving from sql server to databricks and dbt. I am about 3 months into learning and working with databricks and dbt and building pipelines. Recently I received a job offer from a government agency for an analytics manager. The pay is the same as I make and a better retirement pension if I stay long term. One the one hand I want to stay at my current job because doing a full migration will help me better my technical skills for long term. On the other hand this is my chance to step into management and ultimately I want to explore the management route because I am scared that AI will eventually make my mediocre DE skills obsolete and I don\u2019t want to be laid off at 50 without much prospects. Both the current job and the new job offer are remote. Would love your suggestions and thank you in advance. \n\nEdit - The new job has been described as overseeing a team of 5 that will start a migration to databricks and duck db from Oracle. They use microstrategy as their semantic layer. I would be initially learn the existing system and then work with vendors and work with the team to migrate the data. I am 42 with a family living in a MCOL area and financially doing alright with decent savings but definitely need to work till 60 unless I get an unexpected windfall. ", "date_utc": 1762959644.0, "title": "Worth it to move to a different job for same pay from DE to Analytics Manager?", "upvote_ratio": 0.79, "score": 13, "url": "https://www.reddit.com/r/dataengineering/comments/1ov7fxk/worth_it_to_move_to_a_different_job_for_same_pay/"}, {"id": "1ov9muj", "name": "t3_1ov9muj", "content": "In the past month, all my Bitnami-based image containers are no longer coming up. \nI read somewhere that the repositories are no longer public or something of the sort. \nDoes anyone know of any major changes to Bitnami. Apparently the acquisition by Broadcom is now finalized, I wonder if that\u2019s in any way material. \nAny insights/suggestions would be greatly appreciated. ", "date_utc": 1762964507.0, "title": "Bitnami gone?", "upvote_ratio": 0.67, "score": 6, "url": "https://www.reddit.com/r/dataengineering/comments/1ov9muj/bitnami_gone/"}, {"id": "1ov7pyc", "name": "t3_1ov7pyc", "content": "Hi, This is a kind of follow up post. The idea of migrating Glue jobs to Snowpark is on hold for now.\n\nNow, I am asked to explore ADF/Azure Databricks. For context, We'll be moving two Glue jobs away from AWS. They wanted to use snowflake. These jobs, responsible for replication from HANA to Snowflake, uses spark.\n\nWhat's the best approaches to achive this? Should I go for ADF only, Databricks only or ADF + Databricks? The HANA is on-prem.\n\nJobs overview-\n\nCurrently, we have a metadata-driven Glue-based ETL framework for replicating data from SAP HANA to Snowflake. The controller Glue job orchestrates everything - it reads control configurations from Snowflake, checks which tables need to run, plans partitioning with HANA, and triggers parallel Spark Glue jobs. The Spark worker jobs extract from HANA via JDBC, write to Snowflake staging, merge into target tables, and log progress back to Snowflake.\n\nHas anyone gone through this same thing?\u00a0**Please help.**", "date_utc": 1762960263.0, "title": "AWS Glue to Azure databricks/ADF", "upvote_ratio": 0.85, "score": 9, "url": "https://www.reddit.com/r/dataengineering/comments/1ov7pyc/aws_glue_to_azure_databricksadf/"}, {"id": "1ouz5fq", "name": "t3_1ouz5fq", "content": "Hi everyone,\n\nAt my company, we are currently reevaluating our data integration setup. Right now, we have several Docker containers running on various on-premise servers. These are difficult to access and update, and we also lack a clear overview of which pipelines are running, when they are running, and whether any have failed. We only get notified by the end users...\n\nWe\u2019re considering migrating to Azure Container Apps or Azure Container App Jobs.  The advantages we see are that we can easily set up a CI/CD pipeline using GitHub Actions to deploy new images and have a straightforward way to schedule runs.  However, one limitation is that we would still be missing a central overview of pipeline runs and their statuses. Does anyone have experience or recommendations for handling monitoring and failure tracking in such a setup? Is a tool like Sentry enough?\n\nWe have also looked into orchestration tools like Dagster and Airflow, but we are concerned about the operational overhead. These tools can add maintenance complexity, and the learning curve might make it harder for our first-line IT support to identify and resolve issues quickly.\n\nWhat do you think about this approach? Does migrating to Azure Container Apps make sense in this case? Are there other alternatives or lightweight orchestration tools you would recommend that provide better observability and management?\n\nThanks in advance for your input!", "date_utc": 1762933705.0, "title": "Re-evaluating our data integration setup: Azure Container Apps vs orchestration tools", "upvote_ratio": 0.89, "score": 7, "url": "https://www.reddit.com/r/dataengineering/comments/1ouz5fq/reevaluating_our_data_integration_setup_azure/"}, {"id": "1ovbqlq", "name": "t3_1ovbqlq", "content": "I\u2019m relaunching [my data podcast ](https://datatalks.rilldata.com/)next week \u2014 the newest episode with Joe Reis drops on Nov 18 \u2014 and I\u2019m looking for guest ideas.\n\nWho\u2019s a builder in data engineering you\u2019d like to hear from?  \n  \nPast guests have included Hannes Muhleisen (DuckDB), Guillermo Rauch (Vercel), Ryan Blue (Iceberg), Alexey Milovidov (ClickHouse), Erik Bernhardsson (Modal), and Lloyd Tabb (Looker).\n\n[\\(Thanks for the signed copy, Joe!\\)](https://preview.redd.it/zne7q65b3v0g1.png?width=1736&format=png&auto=webp&s=075078838c85b1cc1100e01ea98a6eef2ed65695)\n\n", "date_utc": 1762969067.0, "title": "Which data engineering builders do you want to hear from?", "upvote_ratio": 0.5, "score": 0, "url": "https://www.reddit.com/r/dataengineering/comments/1ovbqlq/which_data_engineering_builders_do_you_want_to/"}, {"id": "1ov0xue", "name": "t3_1ov0xue", "content": "", "date_utc": 1762940588.0, "title": "2025 State of Data Quality survey results", "upvote_ratio": 0.73, "score": 5, "url": "https://26725328.fs1.hubspotusercontent-eu1.net/hubfs/26725328/SYNQs%202025%20State%20of%20Data%20Quality%20Survey.pdf"}, {"id": "1ouxsb8", "name": "t3_1ouxsb8", "content": "How do you guys go about building and maintaining readable and easy to understand/access pyspark scripts? \n\nMy org is migrating data and we have to convert many SQL scripts to pyspark. Given the urgency of things, we are directly converting SQL to Python/pyspark and it is turning 'not so easy' to maintain/edit. We are not using sqlspark and assume we are not going to use it. \n\nWhat are some guidelines/housekeeping to build better scripts? \n\nAlso right now I just spend enough time on technical understanding/logic sql code but not the business logic cause that is going to lead to lots of questions and and more delays. Do you think it is not good to do this? ", "date_utc": 1762928639.0, "title": "Building and maintaining pyspark script", "upvote_ratio": 0.78, "score": 7, "url": "https://www.reddit.com/r/dataengineering/comments/1ouxsb8/building_and_maintaining_pyspark_script/"}, {"id": "1oudwoc", "name": "t3_1oudwoc", "content": "Move and transform data between formats and databases with a single binary. There are no dependencies and no installation headaches.\n\nhttps://reddit.com/link/1oudwoc/video/umocemg0mn0g1/player\n\nI\u2019m a developer and data systems engineer. In 2025, the data engineering landscape is full of \u201cdo-it-all\u201d platforms that are heavy, complex, and often vendor-locked. TinyETL is my attempt at a minimal ETL tool that works reliably in any pipeline.\n\n**Key features:**\n\n* **Built in Rust** for safety, speed, and low overhead.\n* Single 12.5MB binary with no dependencies, installation, or runtime overhead.\n* High performance, streaming up to 180k+ rows per second even for large datasets.\n* Zero configuration, including automatic schema detection, table creation, and type inference.\n* Flexible transformations using Lua scripts for custom data processing.\n* Universal connectivity with CSV, JSON, Parquet, Avro, MySQL, PostgreSQL, SQLite, and MSSQL (Support for DuckDB, ODBC, Snowflake, Databricks, and OneLake is coming soon).\n* Cross-platform, working on Linux, macOS, and Windows.\n\nI would love feedback from the community on how it could fit into existing pipelines and real-world workloads.\n\nSee the repo and demo here: [https://github.com/alrpal/TinyETL](https://github.com/alrpal/TinyETL)", "date_utc": 1762878380.0, "title": "TinyETL: Lightweight, Zero-Config ETL Tool for Fast, Cross-Platform Data Pipelines", "upvote_ratio": 0.94, "score": 57, "url": "https://www.reddit.com/r/dataengineering/comments/1oudwoc/tinyetl_lightweight_zeroconfig_etl_tool_for_fast/"}, {"id": "1ou0igf", "name": "t3_1ou0igf", "content": "I just wrapped up a BI project on a staff aug basis with datatobiz where I spent weeks perfecting data models, custom DAX, and a BI dashboard.  \nLooked beautiful. Ran smooth. Except\u2026the client didn\u2019t use half of it.\n\nTurns out, they only needed one view, a daily sales performance summary that their regional heads could check from mobile. I went full enterprise when a simple Power BI embedded report in Teams would\u2019ve solved it.\n\nLesson learned: not every client wants \u201cscalable,\u201d some just want usable.  \nNow, before every sprint, I ask, \u201cwhat decisions will this dashboard actually drive?\u201d It\u2019s made my workflow (and sanity) 10x better.\n\nAnyone else ever gone too deep when the client just wanted a one-page view?", "date_utc": 1762836525.0, "title": "DON\u2019T BE ME !!!!!!!", "upvote_ratio": 0.93, "score": 227, "url": "https://www.reddit.com/r/dataengineering/comments/1ou0igf/dont_be_me/"}, {"id": "1ouh53l", "name": "t3_1ouh53l", "content": "I\u2019m building a setup where an LLM interacts with a live SQL database.\n\nArchitecture:\n\nI built an MCP (Model Context Protocol) server exposing two tools:\n\nget_schema \u2192 returns table + column metadata\n\nexecute_query \u2192 runs SQL against the DB\n\nThe LLM sees only the schema, not the data.\n\nProblem: Local LLMs (LLaMA / Mistral / etc.) are still weak at accurate SQL generation, especially with joins and aggregations.\n\nIdea:\n\nUse OpenAI / Groq / Sonnet only for SQL generation (schema \u2192 SQL)\n\nUse local LLM for analysis and interpretation (results \u2192 explanation / insights)\n\nNo data leaves the environment.\nOnly the schema is sent to the cloud LLM.\n\nQuestions:\n\n1. Is this safe enough from a data protection standpoint?\n\n2. Anyone tried a similar hybrid workflow (cloud SQL generation + local analysis)?\n\n3. Anything I should watch out for? (optimizers, hallucinations, schema caching, etc.)\n\nLooking for real-world feedback, thanks!\n\n", "date_utc": 1762885464.0, "title": "Hybrid LLM + SQL architecture: Cloud model generates SQL, local model analyzes. Anyone tried this?", "upvote_ratio": 0.84, "score": 17, "url": "https://www.reddit.com/r/dataengineering/comments/1ouh53l/hybrid_llm_sql_architecture_cloud_model_generates/"}, {"id": "1ou3gb3", "name": "t3_1ou3gb3", "content": "Nobody can say what the future brings of course, but I am in the process of setting up a greenfield project and now that Fivetran bought both of these technologies, I do not know what to build on for the long term. ", "date_utc": 1762846953.0, "title": "What is the long-term open-source future for technologies like dbt and SQLMesh?", "upvote_ratio": 0.97, "score": 73, "url": "https://www.reddit.com/r/dataengineering/comments/1ou3gb3/what_is_the_longterm_opensource_future_for/"}, {"id": "1ou6ccv", "name": "t3_1ou6ccv", "content": "I've been a DE for 2.5 years and was a test engineer for 1.5 years before that. I studied biology at uni so I've been programming for around 4 years in total with no CS background. \nI'm working on the back end of a project from the bare bones upwards, creating a user interface for a company billing system. I wrote a SQL query with 5 IF ELSE statements based on 5 different parameters coming from the front end which worked as it should. My college just refactored this using a CTE and now I'm worried my brain doesn't think logically like that... He made the query super efficient and simplified it massively. I don't know how to force my brain to think of efficient solutions like that, when my first instinct is IF this ELSE this. Surely, I should be at this stage after 2 years? Am I behind in my skill set? How can I improve on this?", "date_utc": 1762858004.0, "title": "Am I still a noob?", "upvote_ratio": 0.83, "score": 19, "url": "https://www.reddit.com/r/dataengineering/comments/1ou6ccv/am_i_still_a_noob/"}, {"id": "1oudibi", "name": "t3_1oudibi", "content": "    project/\n    \u251c\u2500\u2500 airflow_setup/ # Airflow Docker setup\n    \u2502 \u251c\u2500\u2500 dags/ # \u2190 Airflow DAGs folder\n    \u2502 \u251c\u2500\u2500 config/ \n    \u2502 \u251c\u2500\u2500 logs/ \n    \u2502 \u251c\u2500\u2500 plugins/ \n    \u2502 \u251c\u2500\u2500 .env \n    \u2502 \u2514\u2500\u2500 docker-compose.yaml\n    \u2502 \n    \u2514\u2500\u2500 airflow_working/\n      \u2514\u2500\u2500 sample_ml_project/ # Your ML project\n        \u251c\u2500\u2500 .env \n        \u251c\u2500\u2500 airflow/\n        \u2502 \u251c\u2500\u2500 __init__.py\n        \u2502 \u2514\u2500\u2500 dags/\n        \u2502   \u2514\u2500\u2500 data_ingestion.py\n        \u251c\u2500\u2500 data_preprocessing/\n        \u2502 \u251c\u2500\u2500 __init__.py\n        \u2502 \u2514\u2500\u2500 load_data.py\n        \u251c\u2500\u2500 __init__.py\n        \u251c\u2500\u2500 config.py \n        \u251c\u2500\u2500 setup.py \n        \u2514\u2500\u2500 requirements.txt\n\nDo you think it\u2019s a good idea to follow this structure?\n\n\n\nIn this setup, Airflow runs separately while the entire project lives in a different directory. Then, I would import or link each project\u2019s DAGs into Airflow and schedule them as needed.\n\n\n\nI will also be adding multiple projects later.\n\n\n\nIf yes, please guide me on how to make it work. I\u2019ve been trying to set it up for the past few days, but I haven\u2019t been able to figure it out.", "date_utc": 1762877504.0, "title": "Best Way to Organize ML Projects When Airflow Runs Separately?", "upvote_ratio": 0.76, "score": 6, "url": "https://www.reddit.com/r/dataengineering/comments/1oudibi/best_way_to_organize_ml_projects_when_airflow/"}, {"id": "1oui9h6", "name": "t3_1oui9h6", "content": "I\u2019m running into schema drift while processing SEC XBRL data. The same financial concept can show up under different GAAP tags depending on the filing or year\u2014for example, us-gaap:Revenues in one period and us-gaap:SalesRevenueNet in another.\n\nFor anyone who has worked with XBRL or large-scale financial data pipelines:\nHow do you standardize or map these inconsistent concept/tag names so they roll up into a single canonical field over time?\n\nContext: I built a site that reconstructs SEC financial statements (https://www.freefinancials.com). When companies change tags across periods, it creates multiple rows for what should be the same line item (like Revenue). I\u2019m looking for approaches or patterns others have used to handle this kind of concept aliasing or normalization across filings.\n", "date_utc": 1762887864.0, "title": "XBRL tag name changing", "upvote_ratio": 0.81, "score": 3, "url": "https://www.reddit.com/r/dataengineering/comments/1oui9h6/xbrl_tag_name_changing/"}, {"id": "1ouf9je", "name": "t3_1ouf9je", "content": "I'm the author of [zsv](https://github.com/liquidaty/zsv) ([https://github.com/liquidaty/zsv](https://github.com/liquidaty/zsv))\n\n**TLDR:**\n\n\\- the fastest and most versatile bare-metal real-world-CSV parser for any platform (including wasm)\n\n\\- \\[edited\\] also includes CLI with commands including \\`sheet\\`, a grid-line viewer in the terminal (see comment below), as well as sql (ad hoc querying of one or multiple CSV files), compare, count, desc(ribe), pretty, serialize, flatten, 2json, 2tsv, stack, 2db and more\n\n\\- [install](https://github.com/liquidaty/zsv/blob/main/INSTALL.md) on any OS with brew, winget, direct download or other popular installer/package managers\n\n**Background:**\n\nzsv was built because I needed a library to integrate with my application, and other CSV parsers had one or more of a variety of limitations. I needed:\n\n\\- handles \"real-world\" CSV including edge cases such as double-quotes in the middle of values with no surrounding quotes, embedded newlines, different types of newlines, data rows that might have a different number of columns from the first row, multi-row headers etc\n\n\\- fast and memory efficient. None of the python CSV packages performed remotely close to what I needed. Certain C based ones such \\`mlr\\` were also orders of magnitude too slow. xsv was in the right ballpark\n\n\\- compiles for any target OS and for web assembly\n\n\\- compiles to library API that can be easily integrated with any programming language\n\nAt that time, SIMD was just becoming available on every chip so a friend and I tried dozens of approaches to leveraging that technology while still meeting the above goals. The result is the zsv parser which is faster than any other parser we've tested (even xsv).\n\nWith parser built, I added other parser nice-to-haves such as both a pull and a push API, and then added a CLI. Most of the CLI commands are run-of-the-mill stuff: echo, select, count, sql, pretty, 2tsv, stack.\n\nSome of the commands are harder to find in other utilities: compare (cell-level comparison with customizable numerical tolerance-- useful when, for example, comparing CSV vs data from a deconstructed XLSX, where the latter may look the same but technically differ by < 0.000001), serialize/flatten, 2json (multiple different JSON schema output choices). A few are not directly CSV-related, but dovetail with others, such as 2db, which converts 2json output to sqlite3 with indexing options, allowing you to run e.g. \\`zsv 2json my.csv --unique-index mycolumn | zsv 2db -t mytable -o my.db\\`.\n\nI've been using zsv for years now in commercial software running bare metal and also in the browser (for a simple in-browser example, see [https://liquidaty.github.io/zsv/](https://liquidaty.github.io/zsv/)), and we've just tagged our first release.\n\nHope you find some use out of it-- if so, give it a star, and feel free to post any questions / comments / suggestions to a new issue.\n\n[https://github.com/liquidaty/zsv](https://github.com/liquidaty/zsv)", "date_utc": 1762881366.0, "title": "ZSV \u2013 A fast, SIMD-based CSV parser and CLI", "upvote_ratio": 0.83, "score": 4, "url": "https://www.reddit.com/r/dataengineering/comments/1ouf9je/zsv_a_fast_simdbased_csv_parser_and_cli/"}, {"id": "1oubfow", "name": "t3_1oubfow", "content": "Hi there, Data PM here.\n\nI recently joined a mid-sized growing SaaS company that has had many \"lives\" (business model changed a couple times), which you can *see* in the data model. Browsing our warehouse layer alone (not all the source tables are hooked up to it) you find dozens of schemas and hundreds of tables. Searching for what should be a standard entity \"Order\" returns dozens of tables with confusing names and varying content. Every person who writes queries in the company (they're in every department) complains about how hard it is to find things. There's a lack of centralized reference tables that give us basic information about our clients and the services we offer them (it's technically not crucial to the architecture of the tools) and each client is configured differently so running queries on all our data is complex.\n\nThe company is still growing and made it this far despite this, **so is it urgent to address this right now?** I don't know. But I'm concerned by my lack of ability to easily answer \"how many clients would be impacted by this Product change.\" (though I'm sure with more time I'll figure it out)\n\n**I pitched to head of Product that I dedicate my next year to focusing on upgrading the data models behind our core business areas**, and to do this in tandem with new Product launches (so it's not just a \"data review\" exercise), but I was met with the reasonable question of \"how would this impact client experience and your personal KPIs?\". The only impact I can think of measuring is reduction in hours spent by eng and data on sifting through things (which is not easy to measure), but cutting costs when you're a growing business is usually not the highest priority.\n\nMy question: **what are metrics have you used to justify data model reviews?** How do you know when a confusing model might be a problem and when?\n\nWelcome all thoughts - thank you!", "date_utc": 1762872785.0, "title": "Data PM seeking Eng input - How do I convince head of Product that cleaning up the data model is important?", "upvote_ratio": 0.67, "score": 4, "url": "https://www.reddit.com/r/dataengineering/comments/1oubfow/data_pm_seeking_eng_input_how_do_i_convince_head/"}, {"id": "1ouk9x5", "name": "t3_1ouk9x5", "content": "How are you all debugging sql triggers? Aside from setting up dummy tables, running the script, editing the script and rerunning. Or is that the only way? Is there a reason for not having a great way to do this?", "date_utc": 1762892323.0, "title": "Debugging sql triggers", "upvote_ratio": 0.75, "score": 2, "url": "https://www.reddit.com/r/dataengineering/comments/1ouk9x5/debugging_sql_triggers/"}, {"id": "1oufh7c", "name": "t3_1oufh7c", "content": "SQLite: OLTP\n\nDuckDB: OLAP\n\nI want to check what are similar ones, for examples things you can use within python or so to embed as part of process for a pipeline then get rid of\n\nGraph: Kuzu?\n\nVector: LanceDB?\n\nTime: QuestDB?\n\nGeo: Duckdb? postgresgis?\n\nsearch: SQLite FTS? \n\n  \nI don't have much use for them, duckdb probably enough but asking out of curiosity.", "date_utc": 1762881824.0, "title": "DBs similar to SQLite and DuckDB", "upvote_ratio": 0.62, "score": 3, "url": "https://www.reddit.com/r/dataengineering/comments/1oufh7c/dbs_similar_to_sqlite_and_duckdb/"}, {"id": "1ougqm5", "name": "t3_1ougqm5", "content": "Hi,\n\nI started a data engineering project with the goal of stock predictions to learn about data science, engineering and about AI/ML and started on my own. What I achieved is a prefect ETL pipeline that collects data from 3 different source cleans the data and stores them into a local postgres database, the prefect also is local and to be more professional I used docker for containerization.  \n  \nTwo days ago I've got an advise to use databricks, the free edition, I started learning it. Now I need some help from more experienced people.\n\n\n\nMy question is:  \nIf we take the hypothetical case in which I deployed the prefect pipeline and I modified the load task to databricks how can I integrate the pipeline in to databricks:\n\n1. Is there a tool or an extension that glues these two components\n2. Or should I copy paste the prefect python code into \n3. Or should I create the pipeline from scratch", "date_utc": 1762884571.0, "title": "How to integrate prefect pipeline to databricks?", "upvote_ratio": 0.76, "score": 2, "url": "https://www.reddit.com/r/dataengineering/comments/1ougqm5/how_to_integrate_prefect_pipeline_to_databricks/"}, {"id": "1oucznb", "name": "t3_1oucznb", "content": "I work as a data engineer in a an organisation which ingests a lot of time series data: telemetry data (5k sensors with mostly 15 min. intervals, sometimes 1. min. intervals.), manual measurements (couple of hundred every month), batch time series (couple of hundred every month with 15 min. interval) etc. Scientific models are built on top of this data, and are published and used by other companies.\n\nThese time series often get corrected in hindsight, because they're calibrated, find out a sensor has been influenced by unexpected phenomena, or have had the wrong settings to begin with. How do I deal best with this type of data as a data engineer? Putting data into a quarantine time agreed upon with the owner of the data source, and only publishing it after? If data changes significantly, models need to be re-run, which can be very time consuming.\n\nFor data exploration, the time series + location data are displayed in a hydrological application, while a basic interface would probably suffice. We'd need a simple interface to display all of these time series (also deducted ones, in total maybe 5k), point locations and polygons, and connect them together. What applications would you recommend? With preference managed applications, and otherwise simple frameworks with little maintenance. Maybe Dash + TimescaleDB / PostGIS?\n\nWhat other theory could be valuable to me in this job and where can I find it?\n\n", "date_utc": 1762876365.0, "title": "Tips for managing time series & geospatial data", "upvote_ratio": 0.81, "score": 3, "url": "https://www.reddit.com/r/dataengineering/comments/1oucznb/tips_for_managing_time_series_geospatial_data/"}, {"id": "1oub1i5", "name": "t3_1oub1i5", "content": "Hi guys,\n\nI'm looking for recommendation for a service to stream table changes from postgres using CDC to a target database where the data is denormalized.\n\nI have \\~7 tables in postgres which I would like to denormalized  so that analytical queries perform faster.\n\nFrom my understanding an OLAP database (clickhouse, bigquery etc.) is better suited for such tasks. The fully denormalized data would be about  \\~500 million rows with about 20+ columns\n\nI've also been considering whether I could get away with a table within postgres which manually gets updated with triggers.\n\nDoes anyone have any suggestions? I see a lot of fancy marketing websites but have found the amount of choices a bit overwhelming.", "date_utc": 1762871823.0, "title": "Denormalizing a table via stream processing", "upvote_ratio": 0.81, "score": 3, "url": "https://www.reddit.com/r/dataengineering/comments/1oub1i5/denormalizing_a_table_via_stream_processing/"}, {"id": "1otqnr3", "name": "t3_1otqnr3", "content": "Just (unfortunately) bombed a technical. Was really nervous, did not brush up on basic sql enough, froze on a python section. BUT I really appreciated the company sending the explicit subject list before so the assessment. Wish I had just studied more, but appreciated this forwardness. It was a white board kind of set up and they were really nice. Fuel to the fire to not bomb the next one!", "date_utc": 1762810272.0, "title": "Good Hiring Practice Shout Out", "upvote_ratio": 0.91, "score": 46, "url": "https://www.reddit.com/r/dataengineering/comments/1otqnr3/good_hiring_practice_shout_out/"}, {"id": "1ougoxj", "name": "t3_1ougoxj", "content": "", "date_utc": 1762884467.0, "title": "ClickPipes for Postgres now supports failover replication slots", "upvote_ratio": 0.5, "score": 0, "url": "https://clickhouse.com/blog/clickpipes-postgres-failover-replication"}, {"id": "1ou55xy", "name": "t3_1ou55xy", "content": "I love fish and work with dbt everyday. I used to have completion for zsh before I switched and not having those has been a daily frustration so I decided to refactor the [bash/zsh version](https://github.com/dbt-labs/dbt-completion.bash) for fish.\n\nThis has been 50% vibe coded as a weekend project so I am still tweaking things as a I go but it does exactly what I need.\n\nThe cross section of fish users and dbt users is small but hopefully this will be useful for others too!\n\nHere is the Github link: [https://github.com/theodotdot/dbt.fish](https://github.com/theodotdot/dbt.fish)", "date_utc": 1762853641.0, "title": "dbt.fish - completion for dbt in fish", "upvote_ratio": 0.75, "score": 6, "url": "https://www.reddit.com/r/dataengineering/comments/1ou55xy/dbtfish_completion_for_dbt_in_fish/"}, {"id": "1ouc6b3", "name": "t3_1ouc6b3", "content": "Hello everyone! Recently I\u2019ve got a problem - I need to insert data from MySQL table to Clickhouse and amount of rows in this table is approximately ~900M. I need to do this via Spark and MinIO, can do partitions only by numeric columns but still Spark app goes down because of heap space error. Any best practices or advises please? Btw, I\u2019m new to Spark (just started using it couple of months ago)", "date_utc": 1762874493.0, "title": "Extract and load problems [Spark]", "upvote_ratio": 0.6, "score": 1, "url": "https://www.reddit.com/r/dataengineering/comments/1ouc6b3/extract_and_load_problems_spark/"}, {"id": "1otf0bc", "name": "t3_1otf0bc", "content": "Despite my pleas about scalability and efficiency, they still are adamant about n8n. Tomorrow I will sit with the CTO, how can I convince them Python is the way to go? This is a big regional company btw with no OLAP database \n\nEDIT: Thank you for the comments so far! I stupidly didn't elaborate on the context. There are multiple transactional databases, APIs, and salesforce. N8n is being chosen because it's \"easy\". I disagree because it isn't scaleable and I believe my solution (a modular Prefect Python script deployed on AWS, specifics to be determined) to be better as it has less clutter and it's better performance wise. We already have AWS and our own servers so the cost shouldn't be an issue. ", "date_utc": 1762784415.0, "title": "Boss wants to do data pipelines in n8n", "upvote_ratio": 0.91, "score": 78, "url": "https://www.reddit.com/r/dataengineering/comments/1otf0bc/boss_wants_to_do_data_pipelines_in_n8n/"}, {"id": "1otilmc", "name": "t3_1otilmc", "content": "What's the project you're working on or the most significant impact you're making at your company at Data engineering & AI. Share your storyline !", "date_utc": 1762792630.0, "title": "What\u2019s your achievements in Data Engineering", "upvote_ratio": 0.89, "score": 37, "url": "https://www.reddit.com/r/dataengineering/comments/1otilmc/whats_your_achievements_in_data_engineering/"}, {"id": "1otwn02", "name": "t3_1otwn02", "content": "hi all, having trouble finding information on Dataiku pricing. wanted to see if anyone here had any insight from personal experience? \n\nthanks in advance! ", "date_utc": 1762825226.0, "title": "Dataiku Pricing?", "upvote_ratio": 0.78, "score": 5, "url": "https://www.reddit.com/r/dataengineering/comments/1otwn02/dataiku_pricing/"}, {"id": "1ots464", "name": "t3_1ots464", "content": "Hey all, my organization settled on Fabric, and I was asked to stand up our data pipelines in Fabric. Nothing crazy, just ingest from a few sources, model it, and push it out to Power BI. But I'm running into errors where the results are different depending on where I run the query. \n\nIn researching what was happening, I came across [this post](https://www.linkedin.com/posts/tasetech_fabric-activity-7392923023613390848-NcB0/) and realized maybe this is more common than I thought. \n\nIs anyone else running into this with CTEs or window functions? Or have a clue what\u2019s actually happening here?", "date_utc": 1762813696.0, "title": "Are CTEs supposed to behave like this?", "upvote_ratio": 1.0, "score": 8, "url": "https://www.reddit.com/r/dataengineering/comments/1ots464/are_ctes_supposed_to_behave_like_this/"}, {"id": "1ou4b0j", "name": "t3_1ou4b0j", "content": "How do you keep schema changes in sync across multiple Kafka environments?\n\nI\u2019ve been running dev, staging, and production clusters on Aiven, and even with managed Kafka it\u2019s tricky. Push too fast and consumers break, wait too long and pipelines run with outdated schemas.\n\nSo far, I\u2019ve been exporting and versioning schemas manually, plus using Aiven\u2019s compatibility settings to prevent obvious issues. It\u2019s smoother than running Kafka yourself, but still takes some discipline.\n\nDo you use a single shared registry, or one per environment? Any strategies for avoiding subtle mismatches between dev and prod?", "date_utc": 1762850236.0, "title": "Handling schema registry changes across environments", "upvote_ratio": 0.5, "score": 0, "url": "https://www.reddit.com/r/dataengineering/comments/1ou4b0j/handling_schema_registry_changes_across/"}, {"id": "1ouelep", "name": "t3_1ouelep", "content": "Just like in a movie where one question changes the tone and flips everyone\u2019s perspective, what\u2019s that strategic data engineering question you\u2019ve asked about a technical issue, people, or process that led to a real, quantifiable impact on your team or project. \n\nI make it a point to sit down with people at every level, really listen to their pain points, and dig into why we\u2019re doing the project and, most importantly, how it\u2019s actually going to benefit them once it\u2019s live", "date_utc": 1762879907.0, "title": "What\u2019s a TOP Strategic data engineering question you\u2019ve actually asked", "upvote_ratio": 0.31, "score": 0, "url": "https://www.reddit.com/r/dataengineering/comments/1ouelep/whats_a_top_strategic_data_engineering_question/"}, {"id": "1ou9vn6", "name": "t3_1ou9vn6", "content": "Hi everyone! I just received and signed an offer to be a Data Engineering Intern at Meta over the coming summer and was wondering if anyone had advice on securing a return offer. \n\nAfter talking with my recruiter she said that a very large part of getting it is headcount on whatever team I end up joining. \n\nDoes anyone have tips on types of teams to look for in team matching? (only happening March - April) Thanks!\n\n", "date_utc": 1762868841.0, "title": "Meta Data Engineering Intern Return Offer", "upvote_ratio": 0.36, "score": 0, "url": "https://www.reddit.com/r/dataengineering/comments/1ou9vn6/meta_data_engineering_intern_return_offer/"}, {"id": "1ou02cs", "name": "t3_1ou02cs", "content": "I recently got into data, but I got confused in the middle of all the resources available for learning SQL besides python. One day I was checking on resources for data implementation, and I found this website with practical cases, that I could add to my portfolio.   \nI have taken some courses, but nothing really practical, and pay a bootcamp is way too expensive. My goal is to start from data analyst to become a ML engineer.   \nAll the advices are well taken, and in case you use another resources and could share with me your path I will listen.", "date_utc": 1762835078.0, "title": "Any experience with this website for training concepts?", "upvote_ratio": 0.4, "score": 0, "url": "https://www.interviewmaster.ai/"}, {"id": "1otehvq", "name": "t3_1otehvq", "content": "Good morning everyone. I\u2019ve been working in the data field since 2020, mostly doing data science and analytics tasks. Recently, I was hired as a mid-level data engineer at a company, where the activities promised during the interviw were to build pipelines and workflows in Databricks, perform data transformations, and manage data pipelines \u2014 nothing new. However, now in my day-to-day work, after two months on the job, I still hadn\u2019t been assigned any tasks until recently. They\u2019ve started giving me tasks related to Terraform \u2014 configuring and creating resources using Terraform with another platform. I\u2019ve never done this before in my life. Wouldn\u2019t this fall under the infrastructure team\u2019s responsibilities? What\u2019s the actual need for learning Terraform within the scope of data engineering? Thanks for your attention.", "date_utc": 1762783190.0, "title": "Help with Terraform", "upvote_ratio": 0.81, "score": 12, "url": "https://www.reddit.com/r/dataengineering/comments/1otehvq/help_with_terraform/"}, {"id": "1ota9vg", "name": "t3_1ota9vg", "content": "Hey everyone,\n\nI'm working as an AI Architect consultant for a mid-sized B2B SaaS company, and we're in the final forecasting stage for a new \"AI Co-pilot\" feature. This agent is customer-facing, designed to let their Pro-tier users run complex queries against their own data.\n\nThe projected API costs are raising serious red flags, and I'm trying to benchmark how others are handling this.\n\n**1. The Cost Projection:** The agent is complex. A single query (e.g., \"Summarize my team's activity on Project X vs. their quarterly goals\") requires a 4-5 call chain to GPT-4T (planning, tool-use 1, tool-use 2, synthesis, etc.). We're clocking this at **\\~$0.75 per query**.\n\nThe feature will roll out to **\\~5,000 users**. Even with a conservative **20% DAU** (1,000 users) asking just **5 queries/day**, the math is alarming: \\*(1,000 DAUs \\* 5 queries/day \\* 20 workdays \\* $0.75/query) = **\\~$75,000/month.**\\*\n\nThis turns a feature into a major COGS problem. How are you justifying/managing this? Are your numbers similar?\n\n**2. The Data Conflict Problem:** Honestly, this might be worse than the cost. The agent has to query multiple internal systems *about* the customer's data (e.g., their usage logs, their tenant DB, the billing system).\n\nWe're seeing conflicts. For example, the **usage logs** show a customer is using an \"Enterprise\" feature, but the **billing system** has them on a \"Pro\" plan. The agent doesn't know what to do and might give a wrong or confusing answer. This reliability issue could kill the feature.\n\n**My Questions:**\n\n* Are you all just eating these high API costs, or did you build a sophisticated middleware/proxy to aggressively cache, route to cheaper models, and reduce \"ping-pong\"?\n* How are you solving these data-conflict issues? Is there a \"pre-LLM\" validation layer?\n* Are any of the observability tools (Langfuse, Helicone, etc.) actually helping solve this, or are they just for logging?\n\nWould appreciate any architecture or strategy insights. Thanks!", "date_utc": 1762770281.0, "title": "How are you handling projected AI costs ($75k+/mo) and data conflicts for customer-facing agents?", "upvote_ratio": 0.8, "score": 18, "url": "https://www.reddit.com/r/dataengineering/comments/1ota9vg/how_are_you_handling_projected_ai_costs_75kmo_and/"}, {"id": "1otlm4w", "name": "t3_1otlm4w", "content": "Has anyone ever used Azure Data Factory to push data from Snowflake to Salesforce? \n\nMy company is looking to use ADF to bring Salesforce data to Snowflake as close to real-time as we can and then also push data that has been ingested into Snowflake from other sources (Epic, Infor) into Salesforce using ADF as well. We have a very complex Salesforce data model with a lot of custom relationships we've built and schema that is changing pretty often. Want to know how difficult it is going to be to both setup and maintain these pipelines.", "date_utc": 1762799138.0, "title": "Bidirectional Sync with Azure Data Factory - Salesforce & Snowflake", "upvote_ratio": 0.8, "score": 6, "url": "https://www.reddit.com/r/dataengineering/comments/1otlm4w/bidirectional_sync_with_azure_data_factory/"}, {"id": "1otab89", "name": "t3_1otab89", "content": "Hello! I have a set of data pipelines here tagged as \"idempotent\". They work pretty fine unless some data gets removed from the source. \n\nGiven that they use the \"upsert\" strategy, they never remove entries, requiring a manual exclusion if desired. However, every re-run generates the same output. \n\nCould I still call then idempotent or is there a stronger property that ensures information synchronization? Thank you! ", "date_utc": 1762770429.0, "title": "Is part of idempotency property also ensuring information synchronization with the source?", "upvote_ratio": 0.67, "score": 2, "url": "https://www.reddit.com/r/dataengineering/comments/1otab89/is_part_of_idempotency_property_also_ensuring/"}, {"id": "1osj0qo", "name": "t3_1osj0qo", "content": "Has anyone worked in an organization that migrated their EDW workloads from Databricks to Snowflake?\n\nI\u2019ve worked in 2 companies already that migrated from Snowflake to Databricks, but wanted to know if the opposite is true. My perception could be wrong but Databricks seems to be eating Snowflake\u2019s market share nowadays ", "date_utc": 1762693924.0, "title": "Snowflake to Databricks Migration?", "upvote_ratio": 0.81, "score": 88, "url": "https://www.reddit.com/r/dataengineering/comments/1osj0qo/snowflake_to_databricks_migration/"}, {"id": "1otc4zh", "name": "t3_1otc4zh", "content": "P99 Conf recordings & Slides are now online. Here are some that stood out to me: \n\n* [Performance Insights Beyond P99: Tales from the Long Tail - P99 CONF](https://www.p99conf.io/session/performance-insights-beyond-p99-tales-from-the-long-tail/)\n* [Timeseries Storage at Ludicrous Speed - P99 CONF](https://www.p99conf.io/session/timeseries-storage-at-ludicrous-speed/)\n* [xCapture v3: Efficient, Always-On Thread Level Observability with eBPF - P99 CONF](https://www.p99conf.io/session/xcapture-v3-efficient-always-on-thread-level-observability-with-ebpf/)\n\n* [8x Better Than Protobuf: Rethinking Serialization for Data Pipelines - P99 CONF](https://www.p99conf.io/session/8x-better-than-protobuf-rethinking-serialization-for-data-pipelines/)\n* [Parsing Protobuf as Fast as Possible - P99 CONF](https://www.p99conf.io/session/parsing-protobuf-as-fast-as-possible/)\n\n* [Apache Flink at Scale: 7x Cost Reduction in Real-Time Deduplication - P99 CONF](https://www.p99conf.io/session/apache-flink-at-scale-7x-cost-reduction-in-real-time-deduplication/)\n* [Building Planet-Scale Streaming Apps: Proven Strategies with Apache Flink - P99 CONF](https://www.p99conf.io/session/building-planet-scale-streaming-apps-proven-strategies-with-apache-flink/)\n* [Rivian's Push Notification Sub Stream with Mega Filter - P99 CONF](https://www.p99conf.io/session/rivians-push-notification-sub-stream-with-mega-filter/)", "date_utc": 1762776750.0, "title": "Some interesting talks from P99 Conf", "upvote_ratio": 0.5, "score": 0, "url": "https://www.reddit.com/r/dataengineering/comments/1otc4zh/some_interesting_talks_from_p99_conf/"}, {"id": "1osqyd9", "name": "t3_1osqyd9", "content": "I work at a non profit organization with about 4.000 employees. We offer child care, elderly care, language courses and almost every kind of social work you can think of. Since the business is so wide there are lots of different software solutions around and yet lots of special tasks can't be solved with them. Since we dont have a software development team everyone is using the tools at their disposal. Meaning: there's dubious Excel sheets with macros nobody ever understood and that more often than not break things.\n\nA colleague and I are kind of the \"data guys\". we are setting up and maintaining a small - not as professional as we'd wish - Data Warehouse and probably know most of the source systems the best. And we know the business needs. \n\nSo we started engineering little micro-apps using the tools we now: Python and SQL. The first app we wrote is a calculator for revenue. It's pulling data from a source systems, cleans it, applies some transformations and presents the output to the user for approval. Afterwards the transformed data is being written into another DB and injected to our ERP.  We're using Pandas for the database connection and transformations and streamlit as the UI. \n\nI recon if a real swe would see the code he'd probably give us a lecture about how to use orms appropriately, what oop is and so on but to be honest I find the result to be quite alright. Especially when taking into account that developing applications isnt our main task. \n\nAre you guys writing smaller or bigger apps or do you leave that to the software engineering peepz? ", "date_utc": 1762713426.0, "title": "Are u building apps?", "upvote_ratio": 0.85, "score": 16, "url": "https://www.reddit.com/r/dataengineering/comments/1osqyd9/are_u_building_apps/"}, {"id": "1otfl35", "name": "t3_1otfl35", "content": "\nI deal with tons of screenshots and scanned documents every week??\n\nI've tried basic OCR but it usually messes up the table format or merges cells weirdly.", "date_utc": 1762785827.0, "title": "How to convert image to excel (csv) ??", "upvote_ratio": 0.33, "score": 0, "url": "https://www.reddit.com/r/dataengineering/comments/1otfl35/how_to_convert_image_to_excel_csv/"}, {"id": "1osm6x8", "name": "t3_1osm6x8", "content": "I\u2019m exploring the trade-offs between **database-level isolation** and **application/middleware-level serialisation**.\n\nSuppose I already enforce **per-key serial order** outside the database (e.g., productId) via one of these:\n\n- local **per-key locks** (single JVM),\n\n- a **distributed lock** (Redis/ZooKeeper/etcd),\n\n- a **single-writer queue** (Kafka partition per key).\n\nIn these setups, only one update for a given key reaches the DB at a time. Practically, the DB doesn\u2019t see concurrent writers for that key.\n\n**Questions**\n\n1. If serial order is already enforced upstream, does it still make sense to keep the DB at SERIALIZABLE? Or can I safely relax to READ COMMITTED / REPEATABLE READ?\n\n2. Where does **contention** go after relaxing isolation\u2014does it simply move from the DB\u2019s lock manager to my app/middleware (locks/queue)?\n\n3. Any **gotchas, patterns, or references** (papers/blogs) that discuss this trade-off?\n\n**Minimal examples to illustrate context**\n\n**A) DB-enforced (serialisable transaction)**\n\n```sql\nBEGIN TRANSACTION ISOLATION LEVEL SERIALIZABLE;\n\nSELECT stock FROM products WHERE id = 42;\n-- if stock > 0:\nUPDATE products SET stock = stock - 1 WHERE id = 42;\n\nCOMMIT;\n```\n\n**B) App-enforced (single JVM, per-key lock), DB at READ COMMITTED**\n\n```java\n// map: productId -> lock object\nLock lock = locks.computeIfAbsent(productId, id -> new ReentrantLock());\n\nlock.lock();\ntry {\n  // autocommit: each statement commits on its own\n  int stock = select(\"SELECT stock FROM products WHERE id = ?\", productId);\n  if (stock > 0) {\n    exec(\"UPDATE products SET stock = stock - 1 WHERE id = ?\", productId);\n  }\n} finally {\n  lock.unlock();\n}\n```\n\n**C) App-enforced (distributed lock), DB at READ COMMITTED**\n\n```java\nRLock lock = redisson.getLock(\"lock:product:\" + productId);\nif (!lock.tryLock(200, 5_000, TimeUnit.MILLISECONDS)) {\n  // busy; caller can retry/back off\n  return;\n}\ntry {\n  int stock = select(\"SELECT stock FROM products WHERE id = ?\", productId);\n  if (stock > 0) {\n    exec(\"UPDATE products SET stock = stock - 1 WHERE id = ?\", productId);\n  }\n} finally {\n  lock.unlock();\n}\n```\n\n**D) App-enforced (single-writer queue), DB at READ COMMITTED**\n\n```java\n// Producer (HTTP handler)\nenqueue(topic=\"purchases\", key=productId, value=\"BUY\");\n\n// Consumer (single thread per key-partition)\nfor (Message m : poll(\"purchases\")) {\n  long id = m.key;\n  int stock = select(\"SELECT stock FROM products WHERE id = ?\", id);\n  if (stock > 0) {\n    exec(\"UPDATE products SET stock = stock - 1 WHERE id = ?\", id);\n  }\n}\n```\n\nI understand that each approach has different failure modes (e.g., lock TTLs, process crashes between select/update, fairness, retries). I\u2019m specifically after **when it\u2019s reasonable to relax DB isolation** because order is guaranteed elsewhere, and how teams reason about the **shift in contention** and **operational complexity**.", "date_utc": 1762702207.0, "title": "If serialisability is enforced in the app/middleware, is it safe to relax DB isolation (e.g., to READ COMMITTED)?", "upvote_ratio": 0.81, "score": 9, "url": "https://www.reddit.com/r/dataengineering/comments/1osm6x8/if_serialisability_is_enforced_in_the/"}, {"id": "1oshd0z", "name": "t3_1oshd0z", "content": "Hello Data Engineering, \n\nJust a question because I got curious. Why many of the company that not even dealing with cloud still using paid data integration platform? I mean I read a lot about them migrating their data from one on-prem database to another with a paid subscription while there's SSIS that you can even get for free and can be use to integrate data.\n\nThank you.", "date_utc": 1762688761.0, "title": "SSIS for Migration", "upvote_ratio": 0.72, "score": 10, "url": "https://www.reddit.com/r/dataengineering/comments/1oshd0z/ssis_for_migration/"}, {"id": "1osk7z8", "name": "t3_1osk7z8", "content": "I understand that ye olde worlde  DW appliances have a high CapEx hit,  whereas Snowflake & Databricks are more OpEx.\n\nObviously you make your best estimate as to what capcity you need with an appliance  and if you over-egg the pudding you pay over the odds. \n\nWith that in mind and when the dust settles after migration, is there truly a cost saving?\n\nIn my career I've been through more DW migrations than feels healthy and I'm dubious if the migrations really achieve their goals?", "date_utc": 1762697134.0, "title": "After a DW migration", "upvote_ratio": 0.7, "score": 5, "url": "https://www.reddit.com/r/dataengineering/comments/1osk7z8/after_a_dw_migration/"}, {"id": "1orvevk", "name": "t3_1orvevk", "content": "Hello everyone, hope all are doing great!\n\nI am sharing a new edition to Data Tech Stack series covering Shopify where we will explore what tech stack is used at Shopify to process 284 million peak requests per minute generating $11+ billions in sales.\n\nKey Points:\n\n* Massive Real-Time Data Throughput: Kafka handles 66 million messages/sec, supporting near-instant analytics and event-driven workloads at Shopify\u2019s global scale.\n* High-Volume Batch Processing & Orchestration: 76K Spark jobs (300 TB/day) coordinated via 10K Airflow DAGs (150K+ runs/day) reflect a mature, automated data platform optimized for both scale and reliability.\n* Robust Analytics & Transformation Layer: DBT\u2019s 100+ models and 400+ unit tests completing in under 3 minutes highlight strong data quality governance and efficient transformation pipelines.\n\n  \nI would love to hear feedback and suggestions on future companies to cover. If you want to collab to showcase your company stack, lets work together.", "date_utc": 1762623493.0, "title": "Shopify Data Tech Stack", "upvote_ratio": 0.92, "score": 98, "url": "https://www.junaideffendi.com/p/shopify-data-tech-stack"}, {"id": "1orwym9", "name": "t3_1orwym9", "content": "Hello Polars lads,\n\nLong story short , I hopped on the Polars train about 3 years ago. At some point, my company needed a data pipeline, so I built one with Polars. It\u2019s been running great ever since\u2026 but now I\u2019m starting to wonder what\u2019s next \u2014 because I need more power. \u26a1\ufe0f\n\nWe use GCP, and process hourly over 2M data points arriving in streaming to pub/sub, then saved to cloud storage.  \nHere goes the pipeline, with a proper batching  i'm able to use 4GB memory cloud run jobs to read parquet, process and export parquet.  \nUntil now everything is smooth, but at the final step this data is used by our dashboard, because polars + parquet files are super fast this used to work properly but recently some of our biggest clients started having some latency and here comes the big debate:\n\nI'm currently querying parquet files with polars and responding to the dashboard\n\n\\- Should i give more power to polars ? mode cpu, larger machine ...\n\n\\- Or it's time to add a Data Warehouse layer ...\n\nThere is one extra challenging point: the data is sort of semi structured. each rows is a session with 2 attributes and list of dynamic attributes, thanks to parquet files and pl.Struct the format is optimized in buckets:\n\n    <s_1, Web, 12, [country=US, duration=12]\n    <s_2, Mobile,13, [isNew=True,...]\n\nMost of the queries will be group\\_by that would filter on the dynamic list (and you got it not all the sessions have the same attributes)  \nThe first intuitive solution was BiGquery, but it will not be efficient when querying with filters on a list of struct (or a json dict)\n\nSo here i'm waiting for you though on this what would you recommend ?\n\nThanks in advance.", "date_utc": 1762627160.0, "title": "Polars has been crushing it for me \u2026 but is it time to go full Data Warehouse?", "upvote_ratio": 0.93, "score": 49, "url": "https://www.reddit.com/r/dataengineering/comments/1orwym9/polars_has_been_crushing_it_for_me_but_is_it_time/"}, {"id": "1osaqtl", "name": "t3_1osaqtl", "content": "Do you have experience in making a database for a team that has no clear business process? Where do you start to make one?\n\nI assume the best start is at understanding their process then making standard and guidelines on writing sales data. From there, I should conceptualize the data model then proceed to logical and physical modeling. \n\nBut is there a faster way than this?\n\n\n\nCONTEXT  \nI'm going to make one for sales team but they somewhat has no standard process. \n\nFor example, they can change order data anytime they one thus creating conflict between order data and payment data. A better design would be to relate payment data on order data that way I can create some constraint to avoid such conflict.\n\n", "date_utc": 1762664867.0, "title": "Experience in creating a proper database within a team that has a questionable data entry process", "upvote_ratio": 0.72, "score": 3, "url": "https://www.reddit.com/r/dataengineering/comments/1osaqtl/experience_in_creating_a_proper_database_within_a/"}, {"id": "1oroc7q", "name": "t3_1oroc7q", "content": "It\u2019s easy to celebrate successes, but failures are where we really learn.   \nWhat's a story that shaped you into a better engineer?", "date_utc": 1762605374.0, "title": "What failures made you the engineer you're today?", "upvote_ratio": 0.96, "score": 40, "url": "https://www.reddit.com/r/dataengineering/comments/1oroc7q/what_failures_made_you_the_engineer_youre_today/"}, {"id": "1os0qhe", "name": "t3_1os0qhe", "content": "I recently wrote about replacing traditional process historians with modern open-source tools (Part 1). [Part 2](https://h3xagn.com/designing-a-modern-industrial-data-stack-part-2/) explores something I find more interesting: automated edge analytics using InfluxDB's Python processing engine.\n\nThis post is about architectural patterns for real-time edge processing in time-series data contexts.\n\n**Use Case: Built a time-of-use (TOU) electricity tariff cost calculator for home energy monitoring**  \n\\- Aggregates grid consumption every 30 minutes  \n\\- Applies seasonal tariff rates (peak/standard/off-peak)  \n\\- Compares TOU vs fixed prepaid costs  \n\\- Writes processed results for real-time visualization\n\nBut the pattern is broadly applicable to industrial IoT, equipment monitoring, quality prediction, etc.\n\n**Results**  \n\\- Real-time cost visibility validates optimisation strategies  \n\\- Issues addressed in hours, not discovered at month-end  \n\\- Same codebase runs on edge (InfluxDB) and cloud (ADX)  \n\\- Zero additional infrastructure vs running separate processing\n\n**Challenges**  \n\\- Python dependency management (security, versions)  \n\\- Resource constraints on edge hardware  \n\\- Debugging is harder than standalone scripts  \n\\- Balance between edge and cloud processing complexity\n\n**Modern approach**  \n\\- Standard Python (vast ecosystem)  \n\\- Portable code (edge \u2192 cloud)  \n\\- Open-source, vendor-neutral  \n\\- Skills transfer across projects\n\n**Questions for the Community**\n\n1. What edge analytics patterns are you using for time-series data?\n2. How do you balance edge vs cloud processing complexity?\n3. Alternative approaches to InfluxDB's processing engine?\n\nFull post: [Designing a modern industrial data stack - Part 2](https://h3xagn.com/designing-a-modern-industrial-data-stack-part-2/)", "date_utc": 1762636488.0, "title": "Edge Analytics with InfluxDB Python Processing Engine - Moving from Reactive to Proactive Data Infrastructure", "upvote_ratio": 0.75, "score": 2, "url": "https://www.reddit.com/r/dataengineering/comments/1os0qhe/edge_analytics_with_influxdb_python_processing/"}, {"id": "1orc11c", "name": "t3_1orc11c", "content": "On my throwaway account.\n\nI\u2019m currently at a well known F50 company as a mid level DE with 3 yoe. \n\nbase: $115k usd\nbonus: 7-8%\nstack: python, sql, terraform, aws (redshift, glue, athena, etc)\n\nI love my team, great manager, incredible wlb and i generally enjoy the work. \n\nbut we do move very slowly, lot of red tape and projects constantly delayed by months. And I do want to learn data engineering frameworks beyond just Glue jobs moving and transforming data w pyspark transformations.\n\nI just got an offer at a consumer facing tech company for 175k TC. but as i was interviewing with the company, i talked to engineers who worked there on Blind who confirmed the glassdoor reviews citing bad wlb and toxic culture. \n\nAm i insane for not taking/hesitating a 50k pay bump because of bad culture and wlb? Have to decide by Monday and since i have a final round with another tech company next friday, it\u2019s either do or die with this offer. ", "date_utc": 1762564020.0, "title": "Unsure whether to take 175k DE offer", "upvote_ratio": 0.78, "score": 68, "url": "https://www.reddit.com/r/dataengineering/comments/1orc11c/unsure_whether_to_take_175k_de_offer/"}, {"id": "1or815u", "name": "t3_1or815u", "content": "", "date_utc": 1762553733.0, "title": "Trying to think of a git commit message at 4:45 pm on Friday.", "upvote_ratio": 0.95, "score": 82, "url": "https://i.redd.it/hs5zde66twzf1.jpeg"}, {"id": "1orw45f", "name": "t3_1orw45f", "content": "", "date_utc": 1762625169.0, "title": "Former TransUnion VP Reveals How Credit Bureaus Use Data Without Consent", "upvote_ratio": 0.5, "score": 0, "url": "https://youtu.be/Bt4W5QpFZr4"}, {"id": "1or4akw", "name": "t3_1or4akw", "content": "When you\u2019re stuck on a bug or need help refactoring, it\u2019s easy to just drop a code snippet into ChatGPT, Copilot, or another AI tool.\n\nBut I\u2019m curious, do you ever think twice before sharing pieces of your company or client code?  \nDo you change variable names or simplify logic first, or just paste it as is and trust it\u2019s fine?\n\nI\u2019m wondering how common it is for developers to be cautious about what kind of internal code or text they share with AI tools, especially when it\u2019s proprietary or tied to production systems.\n\nWould love to hear how you or your team handle that balance between getting AI help and protecting what shouldn\u2019t leave your repo.", "date_utc": 1762544916.0, "title": "Question for data engineers: do you ever worry about what you paste into any AI LLM", "upvote_ratio": 0.78, "score": 23, "url": "https://www.reddit.com/r/dataengineering/comments/1or4akw/question_for_data_engineers_do_you_ever_worry/"}, {"id": "1or956h", "name": "t3_1or956h", "content": "But can destinies be changed?", "date_utc": 1762556459.0, "title": "Your data model is your destiny", "upvote_ratio": 0.76, "score": 12, "url": "https://notes.mtb.xyz/p/your-data-model-is-your-destiny"}, {"id": "1oqjm4v", "name": "t3_1oqjm4v", "content": "https://preview.redd.it/1dc9xc1y5rzf1.png?width=708&format=png&auto=webp&s=6c565b68dbbd65ce9582ab7eed5204be0e4b6573\n\nhttps://preview.redd.it/2awirdgy5rzf1.png?width=772&format=png&auto=webp&s=611fcbf389d67a661385e4248bd5c62036625ddb\n\nHe literally sent an email openly violating Trustpilot policy by asking people to leave 5 star reviews to extend access to the free bootcamp. Like did he not think that through?\n\nThen he followed up with another email basically admitting guilt but turning it into a self therapy session saying \u201cI slept on it... the four 1 star reviews are right, but the 600 five stars feel good.\u201d What kind of leader says that publicly to students?\n\nAnd the tone is all over the place. Defensive one minute, apologetic the next, then guilt trippy with \u201cplease stop procrastinating and get it done though.\u201d It just feels inconsistent and manipulative.\n\nHonestly it came off so unprofessional. Did anyone else get the same messages or feel the same way?", "date_utc": 1762485581.0, "title": "Anyone else get that strange email from DataExpert.io\u2019s Zack Wilson?", "upvote_ratio": 0.92, "score": 159, "url": "https://www.reddit.com/r/dataengineering/comments/1oqjm4v/anyone_else_get_that_strange_email_from/"}, {"id": "1oqui9i", "name": "t3_1oqui9i", "content": "I am leading the implementation of a pilot project to implement an enterprise Data Lakehouse on AWS for a University. I decided to use the Medallion architecture (Bronze: raw data, Silver: clean and validated data, Gold: modeled data for BI) to ensure data quality, traceability and long-term scalability.\nWhat AWS services, based on your experience, what AWS services would you recommend using for the flow?\nIn the last part I am thinking of using AWS Glue Data Catalog for the Catalog (Central Index for S3), in Analysis Amazon Athena (SQL Queries on Gold) and finally in the Visualization Amazon QuickSight.\nFor ingestion, storage and transformation I am having problems, my database is in RDS but what would also be the best option. What courses or tutorials could help me?\nThank you", "date_utc": 1762522357.0, "title": "Piloting a Data Lakehouse", "upvote_ratio": 0.9, "score": 13, "url": "https://www.reddit.com/r/dataengineering/comments/1oqui9i/piloting_a_data_lakehouse/"}, {"id": "1or4ygj", "name": "t3_1or4ygj", "content": "My team works in Databricks and while the platform itself is great, our metadata, DevOps, and data quality validation processes are still really immature. Our goal right now is to move fast, not to build perfect data or the best quality pipelines.\n\nThe business recognizes the value of data, but it\u2019s messy in practice. I swear I could send a short survey with five data-related questions to our analysts and get ten different tables, thirty different queries, and answers that vary by ten percent either way.\n\nHow do you actually fix that?  \nWe have duplicate or near-duplicate tables, poor discoverability, and no clear standard for which source is \u201cofficial.\u201d Analysts waste a ton of time figuring out which data to trust.\n\nI\u2019ve thought about a few things:\n\n* Having subject matter experts fill in or validate table and column descriptions since they know the most context\n* Pulling all metadata and running some kind of similarity indexing to find overlapping tables and see which ones could be merged\n\nAre these decent ideas? What else could we do that\u2019s practical to start with?  \nAlso curious what a realistic timeline looks like to see real improvement? are we talking months or years for this kind of cleanup?\n\nWould love to hear what\u2019s worked (or not worked) at your company.", "date_utc": 1762546431.0, "title": "Solving data discoverability, where do you even start?", "upvote_ratio": 0.6, "score": 2, "url": "https://www.reddit.com/r/dataengineering/comments/1or4ygj/solving_data_discoverability_where_do_you_even/"}, {"id": "1oqoygu", "name": "t3_1oqoygu", "content": "I\u2019m early in my career, just starting out as a Data Engineer (primarily working with Snowflake and ETL tools).\n\nAs I grow into a strong Data Engineer, I believe domain knowledge and expertise will also give me a huge edge and play a crucial role in future job search.\n\nSo, what are the domains that really pay well and are highly valued if I gain 5+ years of experience in a particular domain?\n\nSome domains I\u2019m considering are:\nFintech / Banking / AI & ML / Healthcare / E-commerce / Tech / IoT / Insurance / Energy / SaaS / ERP\n\nPlease share your insights on these different domains \u2014 including experience, pay scale, tech stack, pros, and cons of each.\n\nThank you.", "date_utc": 1762503907.0, "title": "Best domain for data engineer ? Generalist vs domain expertise.", "upvote_ratio": 0.9, "score": 34, "url": "https://www.reddit.com/r/dataengineering/comments/1oqoygu/best_domain_for_data_engineer_generalist_vs/"}, {"id": "1or6pte", "name": "t3_1or6pte", "content": "I\u2019m working on a project and looking to see if any users have worked on preprocessing scanned documents for OCR or IDP usage.\n\nMost documents we are using for this project are in various formats of written and digital text. This includes standard and cursive fonts. The PDFs can include degraded-slightly difficult to read text, occasional lines crossing out different paragraphs, scanner artifacts.\n\nI\u2019ve research multiple solutions for preprocessing but would also like to hear if anyone who has worked on a project like this had any suggestions.\n\nTo clarify- we are looking to preprocess AFTER the scanning already happened so it can be pushed through a pipeline. We have some old documents saved on computers and already shredded.\n\nThank you in advanced!", "date_utc": 1762550564.0, "title": "(Question) Document Preprocessing", "upvote_ratio": 0.75, "score": 2, "url": "https://www.reddit.com/r/dataengineering/comments/1or6pte/question_document_preprocessing/"}, {"id": "1oqo9tk", "name": "t3_1oqo9tk", "content": "Can folks who use ClickHouse or are familiar with it help me understand the use case / traction this is gaining in real time analytics? What is ClickHouse the best replacement for? Or which net new workloads are best suited to ClickHouse?", "date_utc": 1762501263.0, "title": "ClickHouse?", "upvote_ratio": 0.93, "score": 24, "url": "https://www.reddit.com/r/dataengineering/comments/1oqo9tk/clickhouse/"}, {"id": "1oqouqg", "name": "t3_1oqouqg", "content": "Hello,\n\nLooking for some advice to learn databricks for a job i start in 2 months. I come from snowflake background with GCP.\n\nI want to learn databricks and AWS. But i need to choose my time well. I am very good at SQL but slightly out of practice with using python syntax for handling data (pandas, spark etc).\n\nI am looking for some specific resources I can follow through with, I dont want cookbooks or Reference books (O'Reilly mainly) as I can just use documentation. I need resources that are essentially project based -> which is why I love Manning and Packt books.\n\nHas anyone completed these **Packt** books?  \n**Building Modern Data Applications Using Databricks Lakehouse : Develop, optimize, and monitor data pipelines on Databricks** \\- Will Girten  \n  \n**Data Engineering with Apache Spark, Delta Lake, and Lakehouse: Create scalable pipelines that ingest, curate, and aggregate complex data in a timely and secure way** \\- Kukreja\n\nAnd whilst I am at it, has anyone completed **Data Engineering with AWS: Acquire the skills to design and build AWS-based data transformation pipelines like a pro , Second Edition - Eager** \n\n**(sorry I am not allowed to post links to these or the post gets autofiltered/blocked)**\n\nplease feel free to suggest any any material.\n\nAlso I have watched the first 2 episodes\u00a0Bryan Cafferky series which is absolutely phenomenal quality, but it has been a little theory focussed so far. So if someone has has watched these and tell me what I can expect.\n\nAs for databricks, am I just using a community edition? with snowflake the free trial is enough to complete a book.\n\nThanks again, I learn by doing so please dont just tell me to look at the documentation (I wont learn anything reading it, and I dont have time the plan out a project which can conveniently cover all bases) ! However, any pointers will go a long way.", "date_utc": 1762503492.0, "title": "Study Guide - Databricks/Apache Spark", "upvote_ratio": 0.86, "score": 16, "url": "https://www.reddit.com/r/dataengineering/comments/1oqouqg/study_guide_databricksapache_spark/"}, {"id": "1or0aht", "name": "t3_1or0aht", "content": " I\u2019m working on a Power BI data model that follows Kimball\u2019s dimensional modeling approach. The underlying database can\u2019t be changed anymore, so all modeling must happen in Power Query / Power BI.\n\nHere\u2019s the situation:\n\t\u2022\tI have a fact table with ProjectID and a measure Revenue.\n\t\u2022\tA dimension table dim_Project with descriptive project attributes.\n\t\u2022\tA separate table ProjectContribution with columns: ProjectID, Contributor, ContributionPercent\n\nEach project can have multiple contributors with different contribution percentages.\n\nI need to calculate contributor-level revenue by weighting Revenue from the fact table according to ContributionPercent.\n\nMy question:\nHow should I model this in Power BI so that it still follows Kimball\u2019s star schema principles?\nShould I create a bridge table between dim_Project and a new dim_Contributor? Is is ok?\nOr is there a better approach, given that all transformations happen in Power Query?", "date_utc": 1762535841.0, "title": "How to model a many-to-many project\u2013contributor relationship following Kimball principles (PBI)", "upvote_ratio": 0.83, "score": 4, "url": "https://www.reddit.com/r/dataengineering/comments/1or0aht/how_to_model_a_manytomany_projectcontributor/"}, {"id": "1oq6zoy", "name": "t3_1oq6zoy", "content": "I just got banned from\u00a0r/MicrosoftFabric for sharing what I thought was a useful blog on OneLake vs. ADLS costs. Seems like people can get banned there for anything that isn't positive, which isn't a good sign for the community.\n\nJust wanted to raise this for everyone's awareness.\n\n  \n", "date_utc": 1762454164.0, "title": "Banned from r/MicrosoftFabric for sharing a blog", "upvote_ratio": 0.83, "score": 166, "url": "https://www.reddit.com/r/dataengineering/comments/1oq6zoy/banned_from_rmicrosoftfabric_for_sharing_a_blog/"}, {"id": "1or3sxw", "name": "t3_1or3sxw", "content": "Lately, I feel data infrastructure is changing to serve AI use cases. There's a sort of merger between the traditional data stack and the new AI stack. I see this most in two places: 1) the semantic layer and 2) the control plane.\n\nOn the first point, if AI writes SQL and its answers aren't correct for whatever reason - different names for data elements across the data stack, different definitions for the same metric - this is where a semantic model comes in. It's basically giving the LLM the context to create the right results. \n\nOn the second point, it seems data infrastructure and AI infrastructure are collapsing into one control plane. For example, analytics are now agent-facing, not just customer-facing. This changes the requirements for data processing. Quality and lineage checks need to be available to agents. Systems need to meet latency requirements that are designed around agents doing analytic work and retrieving data effectively. \n\nHow are y'all seeing this show up? What steps are y'all taking when implementing these semantic data models? Which metrics, context, and ontology are you providing to the LLMs to make sure results are good?", "date_utc": 1762543770.0, "title": "The collapse of Data and AI Infrastructure into one", "upvote_ratio": 0.58, "score": 2, "url": "https://www.reddit.com/r/dataengineering/comments/1or3sxw/the_collapse_of_data_and_ai_infrastructure_into/"}, {"id": "1oqwgu2", "name": "t3_1oqwgu2", "content": "Fivetran recently retired Log Miner for on-prem Oracle connectors and pushed to use the Binary Log Reader instead.\n\nSince we did the change - the connector can't figure out where it left of at last synch, or at least it can't get the proper list of log files to read, so it's reading *every* log file, taking forever to go through.\n\nWe are seeing a connector going from a nice 5-10 mins per synch to now... 3 hours and 45 mins, of just reading gigs of log files to extract 10 megs of actual data.\n\nWe had tickets for almost 14 days now, no answer in sight. I remember this post: [https://www.reddit.com/r/dataengineering/comments/11xbpjy/beware\\_of\\_fivetran\\_and\\_other\\_elt\\_tools/](https://www.reddit.com/r/dataengineering/comments/11xbpjy/beware_of_fivetran_and_other_elt_tools/) and I regret bitterly not taking its advise.\n\nAnyone experiencing the same issue? Have you guys figured a way to fix it on your end?", "date_utc": 1762527190.0, "title": "is anyone experiencing long Fivetran synchs on Oracle connector?", "upvote_ratio": 0.76, "score": 2, "url": "https://www.reddit.com/r/dataengineering/comments/1oqwgu2/is_anyone_experiencing_long_fivetran_synchs_on/"}, {"id": "1opz0ve", "name": "t3_1opz0ve", "content": "Hear me out. We spend countless hours building data quality frameworks, setting up Great Expectations, and writing custom DBT tests. But 90% of the data quality issues we get paged for are because the business logic changed and no one told us.\n\nA product manager wouldn't launch a new feature in an app without defining what quality means for the user. Why do we accept this for data products?\n\nWe're treated like janitors cleaning up other people's messes instead of engineers building a product. The root cause is a lack of ownership and clear requirements\u00a0*before*\u00a0data is produced.\n\nDiscussion Points:\n\n* Am I just jaded, or is this a universal experience?\n* How have you successfully pushed data quality ownership\u00a0*upstream*\u00a0to the product teams that generate the data?\n* Should Data Engineers start refusing to build pipelines until acceptance criteria for data quality are signed off?\n\nLet's vent and share solutions.", "date_utc": 1762435990.0, "title": "Unpopular Opinion: Data Quality is a product management problem, not an engineering one.", "upvote_ratio": 0.93, "score": 211, "url": "https://www.reddit.com/r/dataengineering/comments/1opz0ve/unpopular_opinion_data_quality_is_a_product/"}, {"id": "1or153t", "name": "t3_1or153t", "content": "I work in a Databricks environment, so that\u2019s my main frame of reference. Between Databricks Apps (especially the new Node.js support), the addition of transactional databases, and the already huge set of analytical and ML tools, it really feels like Databricks is becoming a full-on data powerhouse.\n\nA lot of companies already move and transform their ERP data in Databricks, but most people I talk to complain about every ERP under the sun (SAP, Oracle, Dynamics, etc.). Even just extracting data from these systems is painful, and companies end up shaping their processes around whatever the ERP allows. Then you get all the exceptions: Access databases, spreadsheets, random 3rd-party systems, etc.\n\nI can see those exception processes gradually being rebuilt as Databricks Apps. Over time, more and more of those edge processes could move onto the Databricks platform (or something similar like Snowflake). Eventually, I wouldn\u2019t be surprised to see Databricks or partners offer 3rd-party templates or starter kits for common business processes that expand over time. These could be as custom as a business needs while still being managed in-house.\n\nThe reason I think this could actually happen is that while AI code generation isn\u2019t the miracle tool execs make it out to be, it *will* make it easier to cross skill boundaries. You might start seeing hybrid roles. For example a data scientist/data engineer/analyst combo, or a data engineer/full-stack dev hybrid. And if those hybrid roles don't happen, I still believe simpler corporate roles will probably get replaced by folks who can code a bit. Even my little brother has a programming class in fifth grade. That shift could drive demand for more technical roles that bridge data, apps, and automation. \n\nWhat do you think? Totally speculative, I know, but I\u2019m curious to hear how others see this playing out.", "date_utc": 1762537752.0, "title": "Could modern data platforms evolve into full-blown custom ERP systems?", "upvote_ratio": 0.64, "score": 3, "url": "https://www.reddit.com/r/dataengineering/comments/1or153t/could_modern_data_platforms_evolve_into_fullblown/"}, {"id": "1oqbtwj", "name": "t3_1oqbtwj", "content": "Ex digital marketing data engineer here, and I\u2019ve definitely witnessed this first hand. Wondering what other\u2019 stories are like. ", "date_utc": 1762465189.0, "title": "How true is \u201c90% of data projects fail?\u201d", "upvote_ratio": 0.83, "score": 37, "url": "https://www.reddit.com/r/dataengineering/comments/1oqbtwj/how_true_is_90_of_data_projects_fail/"}, {"id": "1oqlm32", "name": "t3_1oqlm32", "content": "As part of my job, I need to generate some as is and to to be architectures to push through to senior leadership which does not get reviewed in a lot of detail. I am not keen to painstakingly create them in a Miro. Is there any process to prompt it in detail and have a platform/tool generate a decent representation of the architecture I described in the prompt ? I tried some of the AI integrations in Miro and it sucked tbh.  Any suggestions would be great !", "date_utc": 1762491800.0, "title": "LLM for Architecture Diagrams", "upvote_ratio": 0.73, "score": 9, "url": "https://i.redd.it/e0tt7090przf1.jpeg"}, {"id": "1oqxhyx", "name": "t3_1oqxhyx", "content": "I haven't dug into how the columns are used, but this report took a bunch of aggregate data, created a unique ID out of the rows, and mushroomed the size my using it to \"join tables\". 80% of the space is used in this unique key generation.\n\nWhat is the general strategy to do this correctly? I haven't really worked on OLAP reports before but this looks like someone is misapplying OLTP join logic with OLAP data and making a huge mess.\n", "date_utc": 1762529546.0, "title": "What is the next step from this messed up PowerBI report?", "upvote_ratio": 0.5, "score": 0, "url": "https://i.redd.it/xiphkkv8tuzf1.jpeg"}, {"id": "1oqidyq", "name": "t3_1oqidyq", "content": "Similar to data lineage - is there a way to take it forward and have similar lineage for analytics reports ? Like who is the owner, what are data sources, associated KPI etc etc.\n\nAre there any tools that tracks such lineage.", "date_utc": 1762482038.0, "title": "How to track Reporting Lineage", "upvote_ratio": 0.78, "score": 9, "url": "https://www.reddit.com/r/dataengineering/comments/1oqidyq/how_to_track_reporting_lineage/"}, {"id": "1oq6id9", "name": "t3_1oq6id9", "content": "I 25M am working as a data engineer for a large financial institution in the UK with 3yoe and I feel somewhat behind at the moment.\n\nMy academic background is in applied mathematics and I first was a contractor at my firm for 2 years with a partner company before I got made permanent. It is a hybrid role with 2 days per week in the office in London.\n\nThe positives of the role are as follows:\n- Quite good WLB (Only about 10 hrs per week actual work)\n- Good non-toxic culture with friendly technical and non technical colleagues who are always happy to help\n- I have been able to upskill in the role, and now have skills in Python, SQL, Java, DevOps, machine learning, ETL pipelines, GCP, business analysis, basic architecture design and SRE for maintaining data products.\n\nThe negatives are as follows:\n- Low TC (only \u00a360k TC) in London\n- Unclear how I might get a promotion in my organisation.\n\nDue to the good WLB mentioned above, I have used time to learn new skills and learn value investing and because I live with my parents I have been able to build a fairly good portfolio for my age.\n\nI am soon going to buy a flat however so I will not be able to invest as much in the near future.\n\nWhat should I be focusing on? Because although I partially think I should look for another highest TC role, the grass isn\u2019t always greener, so I might be better off milking this good WLB role for all its worth then pursuing some kind of entrepreneurial venture alongside it, because that could have potentially unlimited upside with low downside if my corporate role provides a margin of safety, and if that takes off I could become a full time entrepreneur.\n\nWhat thoughts/advice do people have? Anything is appreciated, thanks!", "date_utc": 1762453107.0, "title": "I (25M) working as a data engineer hybrid role want advice", "upvote_ratio": 0.8, "score": 17, "url": "https://www.reddit.com/r/dataengineering/comments/1oq6id9/i_25m_working_as_a_data_engineer_hybrid_role_want/"}, {"id": "1opu9sk", "name": "t3_1opu9sk", "content": "I was asking myself that the title of Data Engineer didn't exist 10-15 (being generous) years ago, so it's possible that in 5 to 10 years it will disappear, even if we do kind of the same things that we do right now (moving data from point A to point B).\n\nI know that predicting these things is impossible, but as someone that started his career 3 years ago as a Data Engineer, I wonder what is the future for me if I stay technical and if what I do will change significantly as the market changes.   \n  \nPeople that have been many years in the industry, how it's been the road for you? How did your responsibilities and day to day job change over time? Was it difficult to stay up to date when new technologies and new jobs and titles appeared?", "date_utc": 1762420637.0, "title": "What will Data Engineers evolve into in the future?", "upvote_ratio": 0.93, "score": 71, "url": "https://www.reddit.com/r/dataengineering/comments/1opu9sk/what_will_data_engineers_evolve_into_in_the_future/"}, {"id": "1oq7z4e", "name": "t3_1oq7z4e", "content": "https://preview.redd.it/8lthviqkrozf1.png?width=1705&format=png&auto=webp&s=52621202ce6d4516016d1993325f782e36a98810\n\nHello Data Engineers\n\nI've learned a ton from this community and wanted to share a personal project I built to practice on.\n\nIt's an end-to-end data platform \"playground\" that simulates an e-commerce site. It's **not production-ready**, just a sandbox for testing and learning.\n\n**What it does:**\n\n* It has three Python data generators for a realistic mix:\n   1. **Transactional (CDC):** Simulates MySQL changes streamed via **Debezium** & **Kafka**.\n   2. **Clickstream:** Sends real-time JSON events to a cloud API.\n   3. **Ad Spend:** Creates daily batch CSVs (e.g., ad spend).\n* **Terraform** provisions the entire AWS stack (API Gateway, Kinesis Firehose, S3, Glue, Athena, and Lake Formation with pre-configured user roles).\n* **dbt** (running on Athena with Iceberg) transforms the data, and **Dagster** (running locally) orchestrates the dbt models.\n\nRight now, only the AWS stack is implemented. My main goal is to build this *same* platform in **GCP** and **Azure** to learn and compare them.\n\nI hope it's useful for anyone else who wants a full end-to-end sandbox to play with. I'd be honored if you took a look.\n\n**GitHub Repo:** [https://github.com/adavoudi/multi-cloud-data-platform](https://github.com/adavoudi/multi-cloud-data-platform)\u00a0\n\nThanks!", "date_utc": 1762456396.0, "title": "I built an open-source AWS data playground (Terraform, Kafka, dbt, Dagster) and wanted to share", "upvote_ratio": 0.77, "score": 9, "url": "https://www.reddit.com/r/dataengineering/comments/1oq7z4e/i_built_an_opensource_aws_data_playground/"}, {"id": "1oq4167", "name": "t3_1oq4167", "content": "I\u2019ve been experimenting with browser-native data tools for visualizing, exploring, and querying large datasets client-side. The idea is to treat the browser as part of the data stack using pure JavaScript to load, slice, and inspect data interactively without a backend.\n\nA couple of open-source experiments (Hyparquet for reading Parquet files and HighTable for virtualized tables) aim to test where the browser stops being a thin client and starts acting like a real data engine.\n\nCurious how others here think about browser-first architectures:\n\n* Where do you see the practical limits for client-side data processing?\n* Could browser-based tools ever replace parts of the traditional data stack, or will they stay complementary?", "date_utc": 1762447618.0, "title": "How far can we push the browser as a data engine?", "upvote_ratio": 0.61, "score": 4, "url": "https://www.reddit.com/r/dataengineering/comments/1oq4167/how_far_can_we_push_the_browser_as_a_data_engine/"}, {"id": "1opu7zh", "name": "t3_1opu7zh", "content": "This is a very open question, I know. I am going to be the fix slow queries guy and need to learn a lot. I know. But as starting point I need to get some input. Yes, I know that I need to read the query plan/look at logs to fix each problem. \n\nIn general when you have found slow queries, what is the most common reasons? I have tried to talk with some old guys at work and they said that it is very difficult to generalize. Still some of they says that slow queries if often the result of a bad data model which force the users to write complicated queries in order to get their answers.   ", "date_utc": 1762420438.0, "title": "Most common reason for slow quries?", "upvote_ratio": 0.87, "score": 16, "url": "https://www.reddit.com/r/dataengineering/comments/1opu7zh/most_common_reason_for_slow_quries/"}, {"id": "1oqbvgj", "name": "t3_1oqbvgj", "content": "For our webapp, I built a OLAP cube backend for powetong certain insights, I know typically it is powered by OLTP DB( myself, oracle) or some KV DB, but for our use case we went with a cube. I wanted to stress test the cube SLO, any techniques?", "date_utc": 1762465284.0, "title": "Do you guys perform stress testing for data cubes?", "upvote_ratio": 0.5, "score": 0, "url": "https://www.reddit.com/r/dataengineering/comments/1oqbvgj/do_you_guys_perform_stress_testing_for_data_cubes/"}, {"id": "1oqbh3e", "name": "t3_1oqbh3e", "content": "Hello data friends. Want to share a ETL and analytics data pipeline for McDonald menu price by cities & states. The most accurate data pipeline compared to other projects. We ensured SLA and DQC! \n\nWe used BigQuery for the data pipeline and analyzed the product price in states and cities. \nWe used NodeJS for the backend and Bootstrap/JS/charts for the front end. For the dashboard, we use Looker Studio.\n\nSome insights \n\nMcDonald\u2019s menu prices in key U.S. cities, and here are the wild findings this month:\n\ud83e\udd64 Medium Coke: SAME drink, yet 2\u00d7 the price depending on the city\ud83c\udf54 Big Mac Meal: quietly dropped ~10% in THE NATION \nIt\u2019s like inflation\u2026 but told through fries and Big Macs.\n\nAMA. Provide your feedbacks too \u2764\ufe0f\ud83c\udf89\n\n", "date_utc": 1762464375.0, "title": "ETL McDonald Pipeline [OC]", "upvote_ratio": 0.62, "score": 3, "url": "http://mconomics.com"}, {"id": "1op7yd0", "name": "t3_1op7yd0", "content": "I\u2019ve been a data engineer for a few years now and honestly, I\u2019m starting to think work life balance in this field just doesn\u2019t exist.\n\nEvery company I\u2019ve joined so far has been the same story. Sprints are packed with too many tickets, story points that make no sense, and tasks that are way more complex than they look on paper. You start a sprint already behind.\n\nEven if you finish your work, there\u2019s always something else. A pipeline fails, a deployment breaks, or someone suddenly needs \u201ca quick fix\u201d for production. It feels like you can never really log off because something is always running somewhere.\n\nIn my current team, the seniors are still online until midnight almost every night. Nobody officially says we have to work that late, but when that\u2019s what everyone else is doing, it\u2019s hard not to feel pressured. You feel bad for signing off at 7 PM even when you\u2019ve done everything assigned to you.\n\nI actually like data engineering itself. Building data pipelines, tuning Spark jobs, learning new tools, all of that is fun. But the constant grind and unrealistic pace make it hard to enjoy any of it. It feels like you have to keep pushing non-stop just to survive.\n\nIs this just how data engineering is everywhere, or are there actually teams out there with a healthy workload and real work life balance?\n", "date_utc": 1762361087.0, "title": "Is work life balance in data engineering is non-existent?", "upvote_ratio": 0.92, "score": 158, "url": "https://www.reddit.com/r/dataengineering/comments/1op7yd0/is_work_life_balance_in_data_engineering_is/"}, {"id": "1opw2z9", "name": "t3_1opw2z9", "content": "How are you tracking Airflow costs and how granular? I'm involved with a team that's building a personalization system in a multi-tenent context: each customer we serve has an application and each application is essentially an orchestrated series of tasks (&DAGs) to process the necessary end-user profile, which it's then being exposed for consumption via an API.\n\nIt costs us about $30k/month and, based on the revenue we're generating, we might be looking at some ever decreasing margins. We'd like to identify the non-efficient tasks/DAGs.\n\nAny suggestions/recommendations of tools we could use for surfacing costs at that granularity? Much appreciated!", "date_utc": 1762427277.0, "title": "Cost observability for Airflow?", "upvote_ratio": 0.63, "score": 2, "url": "https://www.reddit.com/r/dataengineering/comments/1opw2z9/cost_observability_for_airflow/"}, {"id": "1oq4lmn", "name": "t3_1oq4lmn", "content": "Looking to get feedback on my tech blog for cdc replication and streaming data. ", "date_utc": 1762448895.0, "title": "Change Data Capture", "upvote_ratio": 0.5, "score": 0, "url": "https://medium.com/@ChillData/modernizing-data-architectures-with-change-data-capture-702bd38e5d88"}, {"id": "1opgre4", "name": "t3_1opgre4", "content": "Has anyone here transitioned from Data Engineering leadership to Data Governance leadership (Director Level)?\n\nHas anyone made a similar move at this or senior level? How did it impact your career long term? I have a decent understanding of governance, but I\u2019m trying to gauge whether this is typically seen as a step up, a lateral move, or a step down?", "date_utc": 1762380226.0, "title": "Data Governance!", "upvote_ratio": 0.88, "score": 35, "url": "https://www.reddit.com/r/dataengineering/comments/1opgre4/data_governance/"}, {"id": "1oq8i7h", "name": "t3_1oq8i7h", "content": "Here I have one doubt the files in s3 is more than 3 lakhs and it some files are very larger like 2.4Tb like that. And file formats are like csv,txt,txt.gz, and excel . If I need to run this in AWS glue means what type I need to choose whether I need to choose AWS glue Spark or else Python shell and one thing am making my metadata as csv ", "date_utc": 1762457599.0, "title": "I need to take the metadata information from the AWS s3 using boto3", "upvote_ratio": 0.36, "score": 0, "url": "https://www.reddit.com/r/dataengineering/comments/1oq8i7h/i_need_to_take_the_metadata_information_from_the/"}, {"id": "1opxazl", "name": "t3_1opxazl", "content": "I need to transform pages from books that are separate .svg Files to text for RAG, but I didn't find a tool for it. They are also not standalone, which would be better. I am not very experienced with svg files, so I don't know what the best approach to this is.  \nI tried turning the svgs as the are to pngs and then to pdfs for OCR, but that doesn't work that well for math formulas.  \nHelp would be very much appreciated :>", "date_utc": 1762431251.0, "title": "Need help with svgs", "upvote_ratio": 0.43, "score": 0, "url": "https://www.reddit.com/r/dataengineering/comments/1opxazl/need_help_with_svgs/"}, {"id": "1opwegi", "name": "t3_1opwegi", "content": "Hello, I've been digging around the internet looking for a solution to what appears to be a niche case.\n\nSo far, we were normalizing data to a master schema, but that has proven troublesome with potentially breaking downstream components, and having to rerun all the data through the ETL pipeline whenever there are breaking master schema changes.  \nAnd we've received some new requirements which our system doesn't support, such as time travel.\n\nSo we need a system that can better manage schema, support time travel.\n\nI've looked at Apache Iceberg with Spark Dataframes, which comes really close to a perfect solution, but it seems to only work around the newest schema, unless querying snapshots which don't bring new data.  \nWe may have new data that follows an older schema come in, and we'd want to be able to query new data with an old schema.\n\nI've seen suggestions that Iceberg supports those cases, as it handles the schema with metadata, but I couldn't find a concrete implementation of the solution.  \nI can provide some code snippets for what I've tried, if it helps.\n\nSo does Iceberg already support this case, and I'm just missing something?  \nIf not, is there an already available solution to this kind of problem?\n\n**EDIT:** Forgot to mention that data matching older schemas may still be coming in after the schema evolved", "date_utc": 1762428358.0, "title": "Looking for a Schema Evolution Solution", "upvote_ratio": 0.5, "score": 0, "url": "https://www.reddit.com/r/dataengineering/comments/1opwegi/looking_for_a_schema_evolution_solution/"}, {"id": "1opw668", "name": "t3_1opw668", "content": "TL DR I live in a shithole country and so incredibly jobless so I'm looking for industrial gaps and ways to improve my skills and apparently plumbers reaaaaaaally struggle with tracking this stuff and can't really keep track of what costs there are in relation to what they're charging (and a million other issues that arise from lack of data systems n shit) so I thought I'd learn something and then charge handsomely for it but I have NOOOOO fucking idea about this field so I need to know: \n\nWHAT COULD I LEARN TO SOLVE SUCH A PROBLEM? \n\nfucking anything....skill, course, any certain program, etc. Etc. \n\nJust point in a direction and I'll go there \n\nFYI I have like fucking zero background in anything related to data and/or computers but I'm willing to learn....give me all you've got guys.\n\nThank you in advance \ud83d\ude4f", "date_utc": 1762427579.0, "title": "If I want to help plumbers track costs and invoices and job profitability what could I use?", "upvote_ratio": 0.54, "score": 1, "url": "https://www.reddit.com/r/dataengineering/comments/1opw668/if_i_want_to_help_plumbers_track_costs_and/"}, {"id": "1op2hle", "name": "t3_1op2hle", "content": "pg\\_lake has just been made open sourced and I think this will make a lot of things easier.\n\nTake a look at their Github:  \n[https://github.com/Snowflake-Labs/pg\\_lake](https://github.com/Snowflake-Labs/pg_lake)\n\nWhat do you think? I was using pg\\_parquet for archive queries from our Data Lake and I think pg\\_lake will allow us to use Iceberg and be much more flexible with our ETL.   \n  \nAlso, being backed by the Snowflake team is a huge plus.  \n  \nWhat are your thoughts?", "date_utc": 1762348389.0, "title": "pg_lake is out!", "upvote_ratio": 0.93, "score": 55, "url": "https://www.reddit.com/r/dataengineering/comments/1op2hle/pg_lake_is_out/"}, {"id": "1ophgtn", "name": "t3_1ophgtn", "content": "Hi,\n\nI\u2019m looking for a new job as my current company is becoming toxic and very stressful. I\u2019m currently getting over $100k for a remote permanent position for a relatively mid level position. But all the people that are reaching out to me are offering $40 per hour for a fully onsite role in NYC on a W2 role. When I tell them it\u2019s way too less, all I hear is that\u2019s the market rate. I do understand market is tough but these rates doesn\u2019t make any sense at all. I don\u2019t how would anyone in NYC would accept those rates. So please help me understand current market rates. ", "date_utc": 1762381838.0, "title": "Please help me understand market rates.", "upvote_ratio": 0.74, "score": 7, "url": "https://www.reddit.com/r/dataengineering/comments/1ophgtn/please_help_me_understand_market_rates/"}, {"id": "1op0rqb", "name": "t3_1op0rqb", "content": "What are your favorite conferences each year to catch up on Data Engineering topics, what in particular do you like about the conference, do you attend consistently?", "date_utc": 1762343504.0, "title": "Best Conferences for Data Engineering", "upvote_ratio": 0.99, "score": 45, "url": "https://www.reddit.com/r/dataengineering/comments/1op0rqb/best_conferences_for_data_engineering/"}, {"id": "1opt7vx", "name": "t3_1opt7vx", "content": "I'm working on architecture for multi-tenant data platforms (think: deploying similar data infrastructure for multiple clients/business units) and wanted to get the community's technical insights:\n\nHas anyone worked on \"Data as a Product\" initiatives where you're packaging/delivering data or analytics capabilities to external consumers (customers, partners, etc.)?\n\nLooking for technical insights on:\n\n1. Tooling & IaC: Have you built custom platforms or use existing tools? Any experience using IaC to deploy white-labeled versions for different consumers?\n2. Cloud-agnostic options: Tools like Databricks but more portable across clouds for delivering data products? (Using AWS Cleanrooms, etc.)\n3. Are you seeing more requests for this type of work? Feeling like data-as-a-product engineering is growing?\n4. Does the tooling/ecosystem feel mature or still emerging? Do you think there is a possible emerging market for data monetisation tools?", "date_utc": 1762416469.0, "title": "Building \"Data as a Product\" platforms - tools, deployment patterns, and market demand?", "upvote_ratio": 0.57, "score": 1, "url": "https://www.reddit.com/r/dataengineering/comments/1opt7vx/building_data_as_a_product_platforms_tools/"}, {"id": "1oos473", "name": "t3_1oos473", "content": "There\u2019s a funny moment in most companies where the thing that was supposed to be a temporary ETL job slowly turns into the backbone of everything. It starts as a single script, then a scheduled job, then a workflow, then a whole chain of dependencies, dashboards, alerts, retries, lineage, access control, and \u201cdon\u2019t ever let this break or the business stops functioning.\u201d\n\nNobody calls it out when it happens. One day the pipeline is just *the system*.\n\nAnd every change suddenly feels like defusing a bomb someone else built three years ago.", "date_utc": 1762312797.0, "title": "When the pipeline stops being \u201ca pipeline\u201d and becomes \u201cthe system\u201d", "upvote_ratio": 0.97, "score": 178, "url": "https://www.reddit.com/r/dataengineering/comments/1oos473/when_the_pipeline_stops_being_a_pipeline_and/"}, {"id": "1op9z6x", "name": "t3_1op9z6x", "content": "Hey everyone,\nI\u2019m currently doing a Data Engineering internship (been around 3 months), and I\u2019m honestly starting to question whether it\u2019s worth continuing anymore.\n\nWhen I joined, I was super excited to learn real-world stuff \u2014 build data pipelines, understand architecture, and get proper mentorship from seniors. But the reality has been quite different.\n\nMost of my seniors mainly work with Spark and SQL, while I\u2019ve been assigned tasks involving Airflow and Airbyte. The issue is \u2014 no one really knows these tools well enough to guide me.\n\nFor example, yesterday I faced an Airflow 209 error. Due to some changes, I ended up installing and uninstalling Airflow multiple times, which eventually caused my GitHub repo limit to exceed. After a lot of debugging, I finally figured out the issue myself \u2014 but my manager and team had no idea what was going on.\n\nSame with Airbyte 505 errors \u2014 and everyone\u2019s just as confused as I am. Even my manager wasn\u2019t sure why they happen. I end up spending hours debugging and searching online, with zero feedback or learning support.\n\n\nI totally get that self-learning is a big part of this field, but lately it feels like I\u2019m not really learning, just surviving through errors. There\u2019s no code review, no structured explanation, and no one to discuss better approaches with.\n\nNow I\u2019m wondering:\nShould I stay another month and try to make the best of it,\nor resign and look for an opportunity where I can actually grow under proper guidance?\n\nWould leaving after 3 months look bad if I can still talk about the things I\u2019ve learned \u2014 like building small workflows, debugging orchestrations, and understanding data flow?\n\nHas anyone else gone through a similar \u201cno mentorship, just errors\u201d internship?\nI\u2019d really appreciate advice from senior data engineers, because I genuinely want to become a strong data engineer and learn the right way.\n\n\nEdit\n\nAfter going through everyone\u2019s advice here, I\u2019ve decided not to quit the internship for now. Instead, I\u2019ll focus more on self-learning and building consistency until I find a better opportunity.\nHonestly, this experience has been a rollercoaster \u2014 frustrating at times, but it\u2019s also pushing me to think like a real data engineer. I\u2019ve started enjoying those moments when, after hours of debugging and trial-and-error, I finally fix an issue without any senior\u2019s help. That satisfaction is on another level\n\nThanks", "date_utc": 1762365368.0, "title": "Is it worth staying in an internship where I\u2019m not really learning anything?", "upvote_ratio": 0.73, "score": 10, "url": "https://www.reddit.com/r/dataengineering/comments/1op9z6x/is_it_worth_staying_in_an_internship_where_im_not/"}, {"id": "1opu7ud", "name": "t3_1opu7ud", "content": "I am working in palantir foundry from almost 6 years and have personal projects experience on azure , databricks. In total I have 9 years of experience.  \nWhen 6 years back I was looking for DS roles , I did not get any since I thought i did my PG diploma in Data Science and with entry level experience, I may get and then learn.  \nI did not get any\n\nI switched on understanding DE skills - Spark , DWH , Modelling , CI/CD , Azure\n\nI started looking out\n\nI wanted to get into some organization where Azure , ML projects are there\n\nHowever , Palantir Foundry is so much in demand since most companies are starting with it. They need experienced one there\n\nPersonally - I want to maximize my skills - Ml, stats, azure , databricks\n\nPlantir foundry is strength for now.\n\nBut I feel it becomes little specific. May be I am wrong\n\nI have few offers with similar compensation\n\nPWC - Palantir Manager  \nOptum Insignts - Data Scientist  \nSwiss Re - Palantir Data Engineer (VP)  \nEPAM - Palantir Data Engineer  \nATnT - Palantir Data Engineer  \nOne more remote work - Palantir Data Engineer(More on Architect)- Algoleap\n\nHow should I think , what should I opt for , why and how to approach this situation", "date_utc": 1762420421.0, "title": "Career Advice - which way to opt", "upvote_ratio": 0.43, "score": 0, "url": "https://www.reddit.com/r/dataengineering/comments/1opu7ud/career_advice_which_way_to_opt/"}, {"id": "1opmk25", "name": "t3_1opmk25", "content": "How many of you work on Platform, Systems, or Real-Time data work?  Would you mind telling me a bit more about what you do?  \n\n\n\nI'm currently an analytics engineer but want to move more towards the technical side of DE and looking for motivation! ", "date_utc": 1762394960.0, "title": "Platform, Systems, Real-Time work", "upvote_ratio": 0.6, "score": 1, "url": "https://www.reddit.com/r/dataengineering/comments/1opmk25/platform_systems_realtime_work/"}, {"id": "1opfr0a", "name": "t3_1opfr0a", "content": "I was applying for jobs as usual and this junior data engineer position is triggering me? They mentioned entire full stack's tech requirements along with data engineering role requirements. That too for 4-5 years of experience and still call it Junior role -\\_-\n\n  \n  \n**Jr. Data Engineer**\n\n  \nDescription\n\n**Title: Jr. Data Engineer \u2013 Business Automation & Data Transformation**  \n**Location: Remote**\u00a0\u00a0\n\n**Ekman Associates, Inc. is a Southern California based company focused on the following services: Management Consulting, Professional Staffing Solutions, Executive Recruiting and Managed Services.**\u00a0\u00a0\n\n**Summary:**\u00a0\u00a0As the Automation & Jr. Data Engineer, you will play a critical role in enhancing data infrastructure and driving automation initiatives. This role will be responsible for building and maintaining API connectors, managing data platforms, and developing automation solutions that streamline processes and improve efficiency. This role requires a hands-on engineer with a strong technical background and the ability to work independently while collaborating with cross-functional teams.\n\n**Key Skill Set:**\u00a0\u00a0\n\n* **Ability to build and maintain API connectors - Mandatory**\n* Experience in cloud platforms like AWS, Azure, or Google Cloud.\u00a0\n* Familiarity with data visualization tools like Tableau or Power BI.\u00a0\n* Experience with CI/CD pipelines and DevOps practices.\u00a0\n* Knowledge of data security and privacy best practices, particularly in a media or entertainment context.\n\nRequirements\n\n* Bachelor's degree in Computer Science, Information Technology, Engineering, or a related field, or equivalent experience.\u00a0\n* 4 - 5 years of experience in data engineering, software development, or related roles, with a focus on API development, data platforms, and automation.\u00a0\n* Proficiency in programming languages such as Python, Java, or similar, and experience with API frameworks and tools (e.g., REST, GraphQL).\u00a0\n* Strong understanding of data platforms, databases (SQL, NoSQL), and data warehousing solutions.\u00a0\n* Experience in cloud platforms like AWS, Azure, or Google Cloud.\u00a0\n* Familiarity with data visualization tools like Tableau or Power BI.\u00a0\n* Experience with CI/CD pipelines and DevOps practices.\u00a0\n* Knowledge of data security and privacy best practices, particularly in a media or entertainment context.\u00a0\n* Experience with automation tools and frameworks, such as Ansible, Jenkins, or similar.\u00a0\n* Excellent problem-solving skills and the ability to troubleshoot complex technical issues.\u00a0\n* Strong communication and collaboration skills, with the ability to work effectively with cross-functional teams.\u00a0\n* \u00a0Ability to work in a fast-paced environment and manage multiple projects simultaneously.\u00a0\n* Results-oriented, high energy, self-motivated. \u00a0\n\n", "date_utc": 1762377914.0, "title": "Is this job for real?", "upvote_ratio": 0.54, "score": 1, "url": "https://www.reddit.com/r/dataengineering/comments/1opfr0a/is_this_job_for_real/"}, {"id": "1op8ox8", "name": "t3_1op8ox8", "content": "only python is showing when registering for exam pls some one confirm?\n\n  \n", "date_utc": 1762362664.0, "title": "Databricks removed scala in apache spark certification?", "upvote_ratio": 0.81, "score": 3, "url": "https://www.reddit.com/r/dataengineering/comments/1op8ox8/databricks_removed_scala_in_apache_spark/"}, {"id": "1ophvbf", "name": "t3_1ophvbf", "content": "I have a wordpress website on prem. \n\nHave basically ingested the entire website into Azure AI Search during ingestion. Currently stroing all the metadata in blob storage which is then picked up by the indexer.\n\nCurrently working on a sceduler which regularly updates the data stored in azure.\n\nUpdates and new data is fairly easy as I can fetch based on dates, but for deletions it is different.\n\nCurrently thinking of tranversing through all the records in multiple blob containes and check if that record exits in wordpress mysql on prem table or not.\n\nPlease let me know of better solutions.\n\n", "date_utc": 1762382785.0, "title": "Deletions in ETL pipeline (wordpress based system)", "upvote_ratio": 0.5, "score": 0, "url": "https://www.reddit.com/r/dataengineering/comments/1ophvbf/deletions_in_etl_pipeline_wordpress_based_system/"}, {"id": "1opuocv", "name": "t3_1opuocv", "content": "I just ran real ETL benchmarks (filter, groupby+sort) on 11M+ rows (NYC Taxi data) using both Pandas and Polars on a Databricks cluster (16GB RAM, 4 cores, Standard\\_D4ads\\_v4):\n\n\\- Pandas: Read+concat 5.5s, Filter 0.24s, Groupby+Sort 0.11s  \n\\- Polars: Read+concat 10.9s, Filter 0.42s, Groupby+Sort 0.27s\n\n**Result:** Pandas was faster for all steps. Polars was competitive, but didn\u2019t beat Pandas in this environment. Performance depends on your setup library hype doesn\u2019t always match reality.\n\n**Specs:** Databricks, 16GB RAM, 4 vCPUs, single node, Standard\\_D4ads\\_v4.\n\n**Question for the community:** Has anyone seen Polars win in similar cloud environments? What configs, threading, or setup makes the biggest difference for you?\n\n**Specs matter**. Test before you believe the hype.\n\nhttps://preview.redd.it/xx2m09mkxlzf1.png?width=601&format=png&auto=webp&s=e7abffe16bd2ed1becb361da5c9f2a9ae486db86\n\n", "date_utc": 1762422164.0, "title": "Polars is NOT always faster than Pandas: Real Databricks Benchmarks with NYC Taxi Data", "upvote_ratio": 0.25, "score": 0, "url": "https://www.reddit.com/r/dataengineering/comments/1opuocv/polars_is_not_always_faster_than_pandas_real/"}, {"id": "1ooyobk", "name": "t3_1ooyobk", "content": "# Samara\nI've been working on Samara, a framework that lets you build complete ETL pipelines using just YAML or JSON configuration files. No boilerplate, no repetitive code\u2014just define what you want and let the framework handle the execution with telemetry, error handling and alerting.\n\nThe idea hit me after writing the same data pipeline patterns over and over. Why are we writing hundreds of lines of code to read a CSV, join it with another dataset, filter some rows, and write the output? Engineering is about solving problems, the problem here is repetiviely doing the same over and over.\n\n## What My Project Does\nYou write a config file that describes your pipeline:\n- Where your data lives (files, databases, APIs)\n- What transformations to apply (joins, filters, aggregations, type casting)\n- Where the results should go\n- What to do when things succeed or fail\n\nSamara reads that config and executes the entire pipeline. Same configuration should work whether you're running on Spark or Polars (TODO) or ... Switch engines by changing a single parameter.\n\n## Target Audience\n\n**For engineers**: Stop writing the same extract-transform-load code. Focus on the complex stuff that actually needs custom logic.\n**For teams**: Everyone uses the same patterns. Pipeline definitions are readable by analysts who don't code. Changes are visible in version control as clean configuration diffs.\n**For maintainability**: When requirements change, you update YAML or JSON instead of refactoring code across multiple files.\n\n## Current State\n- 100% test coverage (unit + e2e)\n- Full type safety throughout\n- Comprehensive alerts (email, webhooks, files)\n- Event hooks for custom actions at pipeline stages\n- Solid documentation with architecture diagrams\n- Spark implementation mostly done, Polars implementation in progress\n\n## Looking for Contributors\nThe foundation is solid, but there's exciting work ahead:\n- Extend Polars engine support\n- Build out transformation library\n- Add more data source connectors like Kafka and Databases\n\nCheck out the repo: [github.com/KrijnvanderBurg/Samara](https://github.com/KrijnvanderBurg/Samara)\n\nStar it if the approach resonates with you. Open an issue if you want to contribute or have ideas.\n\n---\nExample: Here's what a pipeline looks like\u2014read two CSVs, join them, select columns, write output:\n\n```yaml\nworkflow:\n  id: product-cleanup-pipeline\n  description: ETL pipeline for cleaning and standardizing product catalog data\n  enabled: true\n  \n  jobs:\n    - id: clean-products\n      description: Remove duplicates, cast types, and select relevant columns from product data\n      enabled: true\n      engine_type: spark\n      \n      # Extract product data from CSV file\n      extracts:\n        - id: extract-products\n          extract_type: file\n          data_format: csv\n          location: examples/yaml_products_cleanup/products/\n          method: batch\n          options:\n            delimiter: \",\"\n            header: true\n            inferSchema: false\n          schema: examples/yaml_products_cleanup/products_schema.json\n      \n      # Transform the data: remove duplicates, cast types, and select columns\n      transforms:\n        - id: transform-clean-products\n          upstream_id: extract-products\n          options: {}\n          functions:\n            # Step 1: Remove duplicate rows based on all columns\n            - function_type: dropDuplicates\n              arguments:\n                columns: []  # Empty array means check all columns for duplicates\n            \n            # Step 2: Cast columns to appropriate data types\n            - function_type: cast\n              arguments:\n                columns:\n                  - column_name: price\n                    cast_type: double\n                  - column_name: stock_quantity\n                    cast_type: integer\n                  - column_name: is_available\n                    cast_type: boolean\n                  - column_name: last_updated\n                    cast_type: date\n            \n            # Step 3: Select only the columns we need for the output\n            - function_type: select\n              arguments:\n                columns:\n                  - product_id\n                  - product_name\n                  - category\n                  - price\n                  - stock_quantity\n                  - is_available\n      \n      # Load the cleaned data to output\n      loads:\n        - id: load-clean-products\n          upstream_id: transform-clean-products\n          load_type: file\n          data_format: csv\n          location: examples/yaml_products_cleanup/output\n          method: batch\n          mode: overwrite\n          options:\n            header: true\n          schema_export: \"\"\n      \n      # Event hooks for pipeline lifecycle\n      hooks:\n        onStart: []\n        onFailure: []\n        onSuccess: []\n        onFinally: []\n```", "date_utc": 1762336008.0, "title": "Samara: A 100% Config-Driven ETL Framework [FOSS]", "upvote_ratio": 0.76, "score": 11, "url": "https://www.reddit.com/r/dataengineering/comments/1ooyobk/samara_a_100_configdriven_etl_framework_foss/"}, {"id": "1oojlwa", "name": "t3_1oojlwa", "content": "I work as an analytics engineer at a Fortune 500 team and I feel honestly stressed out everyday especially over the last few months.\n\nI develop datasets for the end user in mind. The end datasets combine data from different sources we normalize in our database. The issue I\u2019m facing is that stuff that seems to have been ok-ed a few months ago is suddenly not ok - I get grilled for requirements I was told to put, if something is inconsistent I have a colleague who gets on my case and acts like I don\u2019t take accountability for mistakes, even though the end result follows the requirements I was literally told are the correct processes to evaluate whatever the end user wants. I\u2019ve improved all channels of communication and document things extensively now, so thankfully that helps point to why I did things the way I did months ago but it\u2019s frustrating the way colleagues react and behave to unexpected failures while im finishing time sensitive current tasks. \n\nOur pipelines upstream of me have some new failure or the other everyday that\u2019s not in my purview. When data goes missing in my datasets because of that, I have to dig and investigate what happened that can take forever, sometimes it\u2019s a failure because of the vendor sending an unexpectedly changed format or some failure in the pipeline that software engineering team takes care of. When things fail, I have to manually do the steps in the pipeline to temporarily fix the issue which is a series of download, upload, download and \u201ceyeball validate\u201d and upload to the folder that eventually feeds our database for multiple datasets. This eats up my entire day that I have to dedicate for other time sensitive tasks and I feel there are serious unrealistic expectations. I log into work first day out of a day off with a bulk of messages about a failed data issue and have back to back meetings in the AM. I was asked just 1.5 hours of logging in with meetings if I looked into and resolved a data issue that realistically takes a few hours\u2026.um no I was in meetings lol. There was a time in the past at 10PM or so I was asked to manually load data because it failed in our pipeline and I was tired and uploaded the wrong dataset. My manager freaked out the next day,they couldn\u2019t reverse the effects of the new dataset till the next day, so they found me incapable of the task but while yes, it was my mistake of not checking it was 10PM, I don\u2019t get paid for after hours work and I was checked out. I get bombarded with messages after hours & on the weekend. \n\nEverything here is CONSTANTLY changing without warning. I\u2019ve been added to two new different teams and I can\u2019t keep up with why I am there. I\u2019ve tried to ask but everything is unclear and murky. \n\nIs this normal part of DE work or am I in the wrong place? My job is such that I feel even after hours or on weekends im thinking of all the things I have to do. When I log into work these days I feel so groggy. ", "date_utc": 1762291138.0, "title": "Tired of my job. Feels like a new issue comes out of nowhere", "upvote_ratio": 0.83, "score": 22, "url": "https://www.reddit.com/r/dataengineering/comments/1oojlwa/tired_of_my_job_feels_like_a_new_issue_comes_out/"}, {"id": "1oonnto", "name": "t3_1oonnto", "content": "What the best standardized unique identifier to use for American cities? And the best way to map city names people enter to them? \n\nTrying to avoid issues relating to the same city being spelled differently in different places (\u201cSt Alban\u201d and \u201cSaint Alban\u201d), the fact some states have cities with matching names (Springfield), the fact a city might have multiple zip codes, and the various electoral identifiers can span multiple cities and/or only parts of them.\n\nFeels like the answer to this should be more straightforward than it is (or at least than my research has shown). Reminds me of dates and times.", "date_utc": 1762300699.0, "title": "Best unique identifier for cities?", "upvote_ratio": 0.81, "score": 12, "url": "https://www.reddit.com/r/dataengineering/comments/1oonnto/best_unique_identifier_for_cities/"}, {"id": "1ooxica", "name": "t3_1ooxica", "content": "\nHow do you handle schema changes on a cdc tracked table? \nI tested some scenarios with CDC enabled and I\u2019m a bit confused what is going to work or not. \nTo give you an overview, I want to enable CDC on my tables and consume all the data from a third party(Microsoft Fabric). Because of that, I don\u2019t want to lose any data tracked by the _CT tables and I discovered that changing the schema structure of the tables, may potentially end up with some data loss if no proper solution is found. \n\nI\u2019ll give you an example to follow.\nI have the User table with Id, Name,Age, RowVersion. The CDC is enabled at db and at this table level, and I set it to track every row of this table. \nNow some changes may appear in this operational table\n\n1. I add a new column, let\u2019s say Salary as DECIMAL. I want to track this column as well. But I don\u2019t want to disable and enable again the CDC for this table, because I will lose the data in the old capture instance\n2. After a while, I want to ALTER the column Salary from DECIMAL to INT (this is just for the sake of the example).  Here, what I observed, is that after the ALTER state is run, the Salary column in CT table is automatically changed to INT which is weird that may lead to potentially some data loss from the previous data\n3. I will Delete the Salary column. The statement will not break but I need to update somehow the tracking for this table without the column. \n4. I will rename the Name column to FirstName. The rename statement will break because it will see that the column is linked to CDC\n5. I will rename the table from User to Users. This statement is not failing but I still need to update the cdc tracking to not let misleading naming conventions that may be confusing \n\nDid you encounter similar issues in your development? How did you tackle it?\n\nAlso, if you have any advices that you want to share related to your experience with CDC, it will be more than welcomed. \n\nThanks, and sorry for the long post\n\nNote: I use Sql Server", "date_utc": 1762331415.0, "title": "CDC and schema changes", "upvote_ratio": 0.67, "score": 2, "url": "https://www.reddit.com/r/dataengineering/comments/1ooxica/cdc_and_schema_changes/"}, {"id": "1oo5rpu", "name": "t3_1oo5rpu", "content": "These platforms aren't even cheap and the vendor lock-in is real. Cloud computing is great because you can just set up containers in a few seconds independent from the provider. The platforms I'm talking about are the opposite of that.  \n  \nSometimes I think it's because engineers are becoming \"platform engineers\". I just think it's odd because pretty much all the tools that matter are free and open source. All you need is the computing power.", "date_utc": 1762258611.0, "title": "Why everyone is migrating to cloud platforms?", "upvote_ratio": 0.8, "score": 82, "url": "https://www.reddit.com/r/dataengineering/comments/1oo5rpu/why_everyone_is_migrating_to_cloud_platforms/"}, {"id": "1oolnsc", "name": "t3_1oolnsc", "content": "", "date_utc": 1762295785.0, "title": "Cluster Fatigue. Polars and PyArrow to Postgres and Apache Iceberg (streaming mode)", "upvote_ratio": 0.81, "score": 6, "url": "https://www.confessionsofadataguy.com/cluster-fatigue-polars-and-pyarrow-to-postgres-and-apache-iceberg-streaming-mode/"}, {"id": "1ooipcf", "name": "t3_1ooipcf", "content": "Hello, I was wondering if anyone here is a consultant/ runs their own firm? Just curious what the market looks like for getting clients and having continuous work in the pipelines.\n\nThanks", "date_utc": 1762288937.0, "title": "Consulting", "upvote_ratio": 0.92, "score": 10, "url": "https://www.reddit.com/r/dataengineering/comments/1ooipcf/consulting/"}, {"id": "1oot74h", "name": "t3_1oot74h", "content": "Hi everyone\n\nI\u2019m currently building a Data Engineering end-to-end portfolio project using the Microsoft ecosystem, and I started from scratch by creating a simple CRUD app.\n\nThe dataset I\u2019m using is from Kaggle, around 13 million rows (\\~1.5 GB).\n\nMy CRUD app with SQL Server (OLTP) works fine, and API tests are successful, but I\u2019m stuck on the data seeding process.\n\nBecause this is a personal project, I\u2019m running everything on a low-spec VirtualBox VM, and the data loading process is extremely slow.\n\nDo you have any tips or best practices to load or seed large datasets into SQL Server efficiently, especially with limited resources (RAM/CPU)?\n\nThanks a lot in advance", "date_utc": 1762316004.0, "title": "How to efficiently seed large dataset (~13M rows) into SQL Server on low-spec VM?", "upvote_ratio": 0.67, "score": 2, "url": "https://www.reddit.com/r/dataengineering/comments/1oot74h/how_to_efficiently_seed_large_dataset_13m_rows/"}, {"id": "1ooq4td", "name": "t3_1ooq4td", "content": "I got sick of doing the same old data cleaning steps for the start of each new project, so I made a nice, user-friendly interface to make data cleaning more palatable.   \nIt's a simple, yet comprehensive tool aimed at simplifying the initial cleaning of messy or lossy datasets.\n\nIt's built entirely in Python and uses pandas, scikit-learn, and Streamlit modules.\n\nSome of the key features include:  \n\\- Organising columns with mixed data types  \n\\- Multiple imputation methods (mean / median / KNN / MICE, etc) for missing data  \n\\- Outlier detection and remediation  \n\\- Text and column name normalisation/ standardisation  \n\\- Memory optimisation, etc\n\nIt's completely free to use, no login required:  \n[https://datacleaningtool.streamlit.app/](https://datacleaningtool.streamlit.app/)\n\nThe tool is open source and hosted on GitHub (if you\u2019d like to fork it or suggest improvements).  \n  \nI'd love some feedback if you try it out  \n  \nCheers :)", "date_utc": 1762307260.0, "title": "I made a user-friendly and comprehensive data cleaning tool in Streamlit", "upvote_ratio": 0.81, "score": 3, "url": "https://www.reddit.com/r/dataengineering/comments/1ooq4td/i_made_a_userfriendly_and_comprehensive_data/"}, {"id": "1oowf39", "name": "t3_1oowf39", "content": "", "date_utc": 1762327116.0, "title": "Cumulative Statistics in PostgreSQL 18", "upvote_ratio": 0.5, "score": 0, "url": "https://www.data-bene.io/en/blog/cumulative-statistics-in-postgresql-18/"}, {"id": "1oosu69", "name": "t3_1oosu69", "content": "Hi everyone,\n\nI'm trying to modernize the stack in my company. I want to move the data transformation layer from qlik to snowflake. Have to convince my boss. If anyone had this battle before, please advice.\n\nFor context, my team is me (frustrated DE), team manager (really supportive but with no technical background), 2 internal analyst focusing on gathering technical requirements and 2 external bi developer focusing on qlik.\n\nI use Snowflake + dbt but the models built in here are just a handful, because I was not allowed to connect to the ERP system (I am internal by the way) but only to other sources. It looks like soon I will have access to ERP data though.\n\nCurrently the external consultants connects with Qlik directly to our ERP system, downloads a bunch of data from there + snowflake + a few random excels and create a massive transformation layer in Qlik.\n\nThere is no version control, and the internal analysts do not even know how to use qlik - so they just ask the consultants to develop dashboards and have no idea of the data modelling built. Development is slow, dashboards look imho basic and as a DE I want to have at least proper development and governance standards for the data modelling.\n\nMy idea: \n\nStep 1 - have ERP data in snowflake. Build facts and dim in there.\n\nStep 2 - let the analysts use SQL and learn DBT to have the \"reporting\" models in snowflake as well. Upskill the analyst so they can use github to communicate bugs, enhancements etc. Use qlik for visualization only\n\nMy manager is sold on step 1, not yet 2. The external consultants are saying that qlik workd best with facts and dims, instead of one normalized table. So that they can handle the downloads faster and do transformations in qlik.\n\nMy points to go for step 2:\n- qlik has no version control (yet - noy sure if it is an optiom)\n- internally no visibility on the code, it is just a black box the consultants manage. Move would mean better knowledge sharing and data governance\n- the aim is not to create huge tables/views for the dashboards but rather optimal models with just the fields needed\n- possibility of internal upskill (analysts using sql/dbt + git)\n- better visbility on costs, both on the computation layer as well as storage costs decreased \n\nAnything else I can say to convince my manager to make this move?", "date_utc": 1762314928.0, "title": "Transformation layer from Qlik to Snowflake", "upvote_ratio": 0.6, "score": 1, "url": "https://www.reddit.com/r/dataengineering/comments/1oosu69/transformation_layer_from_qlik_to_snowflake/"}, {"id": "1ool0ha", "name": "t3_1ool0ha", "content": "\nHey all,\n\nLooking for some advice from teams that combine dbt with other schema management tools.\n\nI am new to dbt and I exploring using it with Snowflake. We have a pretty robust architecture in place, but looking to possibly simplify things a bit especially for new engineers.\n\nWe are currently using SnowDDL + some custom tools to handle or Snowflake Schema Change Management. This gives us a hybrid approach of imperative and declarative migrations. This works really well for our team, and give us very fined grain control over our database objects.\n\nI\u2019m trying to figure out the right separation of responsibilities between dbt and an external DDL tool:\n- Is it recommended or safe to let something like SnowDDL/Atlas manage Snowflake objects, and only use dbt as the transformation tool to update and insert records?\n- How do you prevent dbt from dropping or replacing tables it didn\u2019t create (so you don\u2019t lose grants, sequences, metadata, etc\u2026)?\n\nWould love to hear how other teams draw the line between:\n- DDL / schema versioning (SnowDDL, Atlas, Terraform, etc.)\n- Transformation logic / data lineage (dbt)\n", "date_utc": 1762294314.0, "title": "Can (or should) I handle snowflake schema mgmt outside dbt?", "upvote_ratio": 1.0, "score": 2, "url": "https://www.reddit.com/r/dataengineering/comments/1ool0ha/can_or_should_i_handle_snowflake_schema_mgmt/"}, {"id": "1ookgz2", "name": "t3_1ookgz2", "content": "Hi everyone,\n\nI'm building a real-time data pipeline using Docker Compose and I've hit a wall with the Hive Metastore. I'm hoping someone can point me in the right direction or suggest a better architecture.\n\nMy Goal:\nI want a containerized setup where:\n\n1. A PySpark container processes data (in real-time/streaming) and writes it as a table to a Delta Lake format.\n2. The data is stored in a MinIO bucket (S3-compatible).\n3. Trino can read these Delta tables from MinIO.\n4. Grafana connects to Trino to visualize the data.\n\nMy Current Architecture & Problem:\n\nI have the following containers working mostly independently:\n\n\u00b7 pyspark-app: Writes Delta tables successfully to s3a://my-bucket/ (pointing to MinIO).\n\u00b7 minio: Storage is working. I can see the _delta_log and data files from Spark.\n\u00b7 trino: Running and can connect to MinIO.\n\u00b7 grafana: Connected to Trino.\n\nThe missing link is schema discovery. For Trino to understand the schema of the Delta tables created by Spark, I know it needs a metastore. My approach was to add a hive-metastore container (with a PostgreSQL backend for the metastore DB).\n\nThis is the step that's failing. I'm having a hard time configuring the Hive Metastore to correctly talk to both the Spark-generated Delta tables on MinIO and then making Trino use that same metastore. The configurations are becoming a tangled mess.\n\nWhat I've Tried/Researched:\n\n\u00b7 Used jupyter/pyspark-notebook as a base for Spark.\n\u00b7 Set Spark configs like spark.hadoop.fs.s3a.path.style.access=true, spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog, and the necessary S3A settings for MinIO.\n\u00b7 For Trino, I've looked at the hive and delta-lake connectors.\n\u00b7 My Hive Metastore setup involves setting S3A endpoints and access keys in hive-site.xml, but I suspect the issue is with the service discovery and the thrift URI.\n\nMy Specific Question:\n\nIs the \"Hive Metastore in a container\" approach the best and most modern way to solve this? It feels brittle.\n\n1. Is there a better, more container-native alternative to the Hive Metastore for this use case? I've heard of things like AWS Glue Data Catalog, but I'm on-prem with MinIO.\n2. If Hive Metastore is the right way, what's the critical configuration I'm likely missing to glue it all together? Specifically, how do I ensure Spark registers tables there and Trino reads from it?\n3. Should I be using the Trino Delta Lake connector instead of the Hive connector? Does it still require a metastore?\n\nAny advice, a working docker-compose.yml snippet, or a pointer to a reference architecture would be immensely helpful!\n\nThanks in advance.", "date_utc": 1762293069.0, "title": "Stuck integrating Hive Metastore for PySpark + Trino + MinIO setup", "upvote_ratio": 1.0, "score": 2, "url": "https://www.reddit.com/r/dataengineering/comments/1ookgz2/stuck_integrating_hive_metastore_for_pyspark/"}, {"id": "1onwcfn", "name": "t3_1onwcfn", "content": "I started in data entry for a small startup 12 years ago, and through several acquisitions, I\u2019ve evolved alongside the company. About a year ago, I shifted from Excel and SQL into Python and OpenAI embeddings to solve name-matching problems. That step opened the door to building full data tools and pipelines\u2014now powered by AI agents\u2014connected through PostgreSQL (locally and in production) and developed entirely within Cursor.\n\nIt\u2019s been rewarding to see this grow from simple scripts into a structured, intelligent system. Still, after seven years without a raise and earning $65k, I\u2019m starting to think it might be time to move on, even though I value the remote flexibility, autonomy, and good benefits.\n\nWhere do I go from here?", "date_utc": 1762225027.0, "title": "From data entry to building AI pipelines \u2014 12 years later and still at $65k. Time to move on?", "upvote_ratio": 0.83, "score": 62, "url": "https://www.reddit.com/r/dataengineering/comments/1onwcfn/from_data_entry_to_building_ai_pipelines_12_years/"}, {"id": "1onxcfo", "name": "t3_1onxcfo", "content": "What concept you think matters most and why?", "date_utc": 1762227966.0, "title": "Data Modeling: What is the most important concept in data modeling to you?", "upvote_ratio": 0.91, "score": 52, "url": "https://www.reddit.com/r/dataengineering/comments/1onxcfo/data_modeling_what_is_the_most_important_concept/"}, {"id": "1ooo2ub", "name": "t3_1ooo2ub", "content": "Hi,\nCurrently working on migrating managed tables in Azure Databricks, to a new workspace in GCP. I read a blog suggesting using storage transfer service, while I know the storage paths of these managed tables in Azure, I don't think copying the delta files will allow recreating them, I tested in my workspace doing that and you can't create an external table on top of a managed table location, even when I copied the table folder. Don't know why though, I'd love to understand (especially when I duplicated that folder). PS, both workspaces are under unity catalog. Ps2: I'm not Databricks expert, so any help is welcome. We need to migrate years of historical data, and also might need to remigrate when new data is added. So incremental unloading is needed as well... I don't know if delta sharing is an option or would be too expensive, since we need just to copy all that history, I read there's cloning too but don't know if that's cross metastore/cloud possible...too much info, if someone migrated or you have ideas, thank you!", "date_utc": 1762301752.0, "title": "Databricks migration cross cloud", "upvote_ratio": 0.6, "score": 1, "url": "https://www.reddit.com/r/dataengineering/comments/1ooo2ub/databricks_migration_cross_cloud/"}, {"id": "1oo9ixm", "name": "t3_1oo9ixm", "content": "My team is central in the organisation; we are about to ingest data from S3 to Snowflake using Snowpipes. With between 50 & 70 data pipelines, how do we approach CI/CD? Do we create repos for division/team/source or just 1 repo? Our tech stack includes GitHub with Actions, Python and Terraform.", "date_utc": 1762268715.0, "title": "Data Engineering DevOps", "upvote_ratio": 0.88, "score": 6, "url": "https://www.reddit.com/r/dataengineering/comments/1oo9ixm/data_engineering_devops/"}, {"id": "1oog2fr", "name": "t3_1oog2fr", "content": "I have bunch of test cases that I need to schedule. Where do you usually schedule test cases and alerting if test fails? Github action? Directly only pipeline?", "date_utc": 1762283020.0, "title": "How do you schedule your test cases ?", "upvote_ratio": 0.67, "score": 2, "url": "https://www.reddit.com/r/dataengineering/comments/1oog2fr/how_do_you_schedule_your_test_cases/"}, {"id": "1onn50a", "name": "t3_1onn50a", "content": "Taking inspiration from Cal Newport's book, \"So Good They Can't Ignore You\", in which he describes the (work related) benefits of building up \"career capital\", that is, skillsets and/or expertise relevant to your industry that prove valuable to either employers or your own entreprenurial endeavours - what would you consider the most important career capital for data engineers right now?\n\nThe obvious area is AI and perhaps being ready to build AI-native platforms, optimizing infrastructure to facilitate AI projects and associated costs and data volume challenges etc.\n\nIf you're a leader, building out or have built out teams in the past, what is going to propel someone to the top of your wanted list?", "date_utc": 1762202010.0, "title": "What Data Engineering \"Career Capital\" is most valuable right now?", "upvote_ratio": 0.96, "score": 123, "url": "https://www.reddit.com/r/dataengineering/comments/1onn50a/what_data_engineering_career_capital_is_most/"}, {"id": "1ooih6o", "name": "t3_1ooih6o", "content": "To start, I\u2019m not a data engineer. I work in operations for the railroad in our control center, and I have IT leanings. But I recently noticed that one of our standard processes for monitoring crew assignments during shifts is wildly inefficient, and I want to build a proof of concept dashboard so that management can OK the project to our IT dept. \n\nRight now, when a train is delayed, dispatchers have to manually piece together information from multiple systems to judge if a crew will still make their next run. They look at real-time train delay data in one feed, crew assignments somewhere else, and scheduled arrival and departure times in a third place, cross-referencing train numbers and crew IDs by hand. Then they compile it all into a list and relay that list to our crew assignment office by phone. It\u2019s wildly inefficient and time consuming, and it\u2019s baffling to me that no one has ever linked them before, given how straightforward the logic should be.\n\nI guess my question is- is this as simple as I\u2019m assuming it should be? I worked up a dashboard prototype using Chat GPT that I\u2019d love to get some feedback on, if I get any interest on this post. I\u2019d love to hear thoughts from people who work in this field! Thanks everyone ", "date_utc": 1762288401.0, "title": "railroad ops project help/critique", "upvote_ratio": 0.67, "score": 1, "url": "https://www.reddit.com/r/dataengineering/comments/1ooih6o/railroad_ops_project_helpcritique/"}, {"id": "1ooiekb", "name": "t3_1ooiekb", "content": "Hey folks,\n\nI am working at a large insurance company where we are building a new data platform (dwh) in Azure, and I have been asked to figure out a way to move a subset of production data (around 10%) into pre prod, while making sure referential integrity is preserved across our new Data Vault model. \nThere is dev and test with synthetic data (for development) but pre prod has to have a subset of prod data. So 4 different env.\n\nHere\u2019s the rough idea I have been working on, and I would really appreciate feedback, challenges, or even \u201cdon\u2019t do it\u201d warnings.\n\nThe process would start with an input manifest \u2013 basically just a list of thousand of business UUIDs (like contract_uuid = 1234, etc.) that serve as entry points. From there, the idea is to treat the Vault like a graph and traverse it:\nI would use metadatacatalog (link tables, key columns, etc.) to figure out which link tables to scan, and each time I find a new key (e.g. a customer_uuid in a link table), that key gets added to the traversal. The engine keeps running as long as new keys are discovered. Every Iteration would start from the first entry point again (e.g contact_uuid) but with new keys discovered from the previous iteration added. Duplicates key in the iterations will be ignored.\n\nI would build this in PySpark to keep it scalable and flexible. The goal is not to pull raw tables, but rather end up with a list of UUIDs per Hub or Sat that I can use to extract just the data I need from prod into pre prod via a \u201edata exchange layer\u201c. If someone later triggers an new extract for a different business domain, we would only grab new keys no redundant data, no duplicates.\n\nI tried to challenge this approach internally but i felt like it did not lead to a discussion or even \u201ewhat could go wrong\u201c scenario.\n\nIn theory, this all makes sense. But I am aware that theory and practice do notalways match , especially when there are thousand of keys, hundreds of tables, and performance becomes an issue.\n\nSo here what I am wondering:\n\nHas anyone built something similar?\nDoes this approach scale?\nAre there proven practice for this that I might be missing?\n\nSo yeah\u2026am i on the right path or run away from this?", "date_utc": 1762288227.0, "title": "Data Vault - Subset from Prod to Pre Prod", "upvote_ratio": 0.6, "score": 1, "url": "https://www.reddit.com/r/dataengineering/comments/1ooiekb/data_vault_subset_from_prod_to_pre_prod/"}, {"id": "1oofvqb", "name": "t3_1oofvqb", "content": "Hi everyone! I don't post much, but I've been really struggling with this task for the past couple months, so turning here for some ideas. I'm trying to obtain search volume data by state (in the US) so I can generate charts kind of like what Google Trends displays for specific keywords. I've tried a couple different services including DataForSEO, a bunch of random RapidAPI endpoints, as well as SerpAPI to try to obtain this data, but all of them have flaws. DataForSEO's data is a bit questionable from my testing, SerpAPI takes forever to run and has downtime randomly, and all the other unofficial sources I've tried just don't work entirely. Does anyone have any advice on how to obtain this kind of data?", "date_utc": 1762282627.0, "title": "Looking for trends data", "upvote_ratio": 0.5, "score": 0, "url": "https://www.reddit.com/r/dataengineering/comments/1oofvqb/looking_for_trends_data/"}, {"id": "1oof1kt", "name": "t3_1oof1kt", "content": "We actively use pgvector in a production setting for maintaining and querying HNSW vector indexes used to power our recommendation algorithms. A couple of weeks ago, however, as we were adding many more candidates into our database, we suddenly noticed our query times increasing linearly with the number of profiles, which turned out to be a result of incorrectly structured and overly complicated SQL queries. \n\nTurns out that I hadn't fully internalized how filtering vector queries really worked. I knew vector indexes were fundamentally different from B-trees, hash maps, GIN indexes, etc., but I had not understood that they were essentially incompatible with more standard filtering approaches in the way that they are typically executed. \n\nI searched through google until page 10 and beyond with various different searches, but struggled to find thorough examples addressing the issues I was facing in real production scenarios that I could use to ground my expectations and guide my implementation. \n\nNow, I wrote a blog post about some of the best practices I learned for filtering vector queries using pgvector with PostgreSQL based on all the information I could find, thoroughly tried and tested, and currently in deployed in production use. In it I try to provide:\n\n\\- Reference points to target when optimizing vector queries' performance  \n\\- Clarity about your options for different approaches, such as pre-filtering, post-filtering and integrated filtering with pgvector  \n\\- Examples of optimized query structures using both Python + SQLAlchemy and raw SQL, as well as approaches to dynamically building more complex queries using SQLAlchemy  \n\\- Tips and tricks for constructing both indexes and queries as well as for understanding them  \n\\- Directions for even further optimizations and learning\n\nHopefully it helps, whether you're building standard RAG systems, fully agentic AI applications or good old semantic search! \n\n[https://www.clarvo.ai/blog/optimizing-filtered-vector-queries-from-tens-of-seconds-to-single-digit-milliseconds-in-postgresql](https://www.clarvo.ai/blog/optimizing-filtered-vector-queries-from-tens-of-seconds-to-single-digit-milliseconds-in-postgresql)\n\nLet me know if there is anything I missed or if you have come up with better strategies!", "date_utc": 1762280817.0, "title": "Optimizing filtered vector queries from tens of seconds to single-digit milliseconds in PostgreSQL", "upvote_ratio": 0.67, "score": 1, "url": "https://www.clarvo.ai/blog/optimizing-filtered-vector-queries-from-tens-of-seconds-to-single-digit-milliseconds-in-postgresql"}, {"id": "1oocwm2", "name": "t3_1oocwm2", "content": "We are onboarding a critical application that cannot tolerate any data-loss and are forced to turn to kubernetes due to server provisioning (we don't need all of the server resources for this workload). We have always hosted databases on bare-metal or VMs or turned to Cloud solutions like RDS with backups, etc.\n\nStack:\n\n* Servers (dense CPU and memory)\n* Raw HDDs and SSDs\n* Kubernetes\n\nGoal is to have production grade setup in a short timeline:\n\n* Easy to setup and maintain\n* Easy to scale/up down\n* Backups\n* True persistence\n* Read replicas\n* Ability to do monitoring via dashboards.\n\nIn 2025 (and 2026), what would you recommend to run PG18? Is Kubernetes still too much of a vodoo topic in the world of databases given its pains around managing stateful workloads?\n\n[](https://www.reddit.com/submit/?source_id=t3_1oocrpm)", "date_utc": 1762276191.0, "title": "In 2025, which Postgres solution would you pick to run production workloads?", "upvote_ratio": 0.4, "score": 0, "url": "https://www.reddit.com/r/dataengineering/comments/1oocwm2/in_2025_which_postgres_solution_would_you_pick_to/"}, {"id": "1onjmd6", "name": "t3_1onjmd6", "content": "Apache Spark 4.0 was officially released in May 2025 and is already available in Databricks Runtime 17.3 LTS.", "date_utc": 1762194277.0, "title": "What Developers Need to Know About Apache Spark 4.0", "upvote_ratio": 0.87, "score": 37, "url": "https://medium.com/@cralle/what-developers-need-to-know-about-apache-spark-4-0-508d0e4a5370?sk=2a635c3e28a7aa90c655d0a2da421725"}, {"id": "1onl9as", "name": "t3_1onl9as", "content": "Some personal background: I have worked with data for 9 years, had a nice position as an Analytics Engineer and got pressured into taking a job I knew was destined to fail. \n\nThe previous Data Engineering Manager became a specialist and left the company. It's a bad position, infrastructure has always been an afterthought for everybody here and upper management has the absolute conviction that I don't need to be technical to manage the team. It's been +/- 5 months and, obviously, I am convinced that's just BS. \n\nThe market in my country is hard right now, so looking for something in my field might be a little difficult. I decided to accept this as a challenge and try to be optimistic. \n\nSo I'm looking for advice and resources I can consult and maybe even become a full on Data Engineer myself. \n\nThis company is a Google Partner, so we mostly use GCP. Most used services include BigQuery, Cloud Run, Cloud Build, Cloud Composer, DataForm and Lookerstudio for dashboards.\n\nI'm already looking into the Skills Boost data engineer path, but I'm thinking it's all over the place and so generalist.\n\nAny help?", "date_utc": 1762197854.0, "title": "I became a Data Engineering Manager and I'm not a data engineer: help?", "upvote_ratio": 0.79, "score": 25, "url": "https://www.reddit.com/r/dataengineering/comments/1onl9as/i_became_a_data_engineering_manager_and_im_not_a/"}, {"id": "1oo5xr9", "name": "t3_1oo5xr9", "content": "Visit my newly built tool to generate research from the 200M+ research paper out there : [https://sci-database.com/](https://sci-database.com/) ", "date_utc": 1762259570.0, "title": "Build a Scientific Database from Research Papers, Instantly : https://sci-database.com/\nAutomatically extract data from thousands of research papers to build a structured database for your ML project or or to identify trends across large datasets.", "upvote_ratio": 0.5, "score": 0, "url": "https://www.reddit.com/r/dataengineering/comments/1oo5xr9/build_a_scientific_database_from_research_papers/"}, {"id": "1onbz5a", "name": "t3_1onbz5a", "content": "Right now, basically my entire workload is maintaining and adding new features to pipelines that support a few dozen dashboards. Like all dashboards....no one uses them.\n\nThe only views in the past 6 months have been from our PO and they have only been viewing dashboards in order to QA tickets. \n\nMy entire job is making sure dashboards say what one person thinks they should say....so I have started just running one off update statements to make problems go away. \n\nUPDATE some.table\n\nSET value = what\\_po\\_says\n\nWHERE id = some\\_customer", "date_utc": 1762177091.0, "title": "Dumbest thing you have ever worked on?", "upvote_ratio": 0.96, "score": 68, "url": "https://www.reddit.com/r/dataengineering/comments/1onbz5a/dumbest_thing_you_have_ever_worked_on/"}, {"id": "1oo5u6o", "name": "t3_1oo5u6o", "content": "Hey everyone,\n\nI wanted to get some community perspective on something I\u2019ve been exploring lately.\n\nI\u2019m currently pursuing my master\u2019s in Information Systems, with a focus on data-related fields \u2014 things like data engineering, data visualization, data mining, processing and AI, ML as well. Initially, I was quite interested in Data Governance, especially given how important compliance and data quality are becoming across the EU with GDPR, AI Act, and other regulations.\n\nI thought this could be a great niche \u2014 combining governance, compliance, and maybe even AI/ML-based policy automation in the future.\n\nHowever, after talking to a few professionals in the data engineering field (each with 10+ years of experience), I got a bit of a reality check. They said:\n\nIt\u2019s not easy to break into data governance early in your career.\n\nSmaller companies often don\u2019t take governance seriously or have formal frameworks.\n\nLarger companies do care, but the field is considered too fragile or risky to hand over to someone without deep experience.\n\n\nTheir suggestion was to gain strong hands-on experience in core data roles first \u2014 like data engineering or data management \u2014 and then transition into data governance once I\u2019ve built a solid foundation and credibility.\n\nThat makes sense logically, but I\u2019m curious what others think.\n\nHas anyone here transitioned into Data Governance later in their career?\n\nHow did you position yourself for it?\n\nAre there any specific skills, certifications, or experiences that helped you make that move?\n\nAnd lastly, do you think the EU\u2019s regulatory environment might create more entry-level or mid-level governance roles in the near future?\n\n\nWould love to hear your experiences or advice.\n\nThanks in advance!", "date_utc": 1762258996.0, "title": "Is it really that hard to enter into Data Governance as a career path in the EU?", "upvote_ratio": 0.57, "score": 1, "url": "https://www.reddit.com/r/dataengineering/comments/1oo5u6o/is_it_really_that_hard_to_enter_into_data/"}, {"id": "1oo0ido", "name": "t3_1oo0ido", "content": "Hello, I'm not a tech person, so please pardon me if my ignorance is showing here \u2014 but I\u2019ve been tasked with a project at work by a boss who\u2019s even less tech-savvy than I am. lol\n\nThe assignment is to comb through various websites to gather publicly available information and compile it into a spreadsheet for analysis. I know I can use ChatGPT to help with this, but I\u2019d still need to fact-check the results.\n\nAre there other (better or more efficient) ways to approach this task \u2014 maybe through tools, scripts, or workflows that make web data collection and organization easier?\n\nNot only would this help with my current project, but I\u2019m also thinking about going back to school or getting some additional training in tech to sharpen my skills. Any guidance or learning resources you\u2019d recommend would be greatly appreciated.\n\nThanks in advance! ", "date_utc": 1762238573.0, "title": "Seeking advice: best tools for compiling web data into a spreadsheet", "upvote_ratio": 0.67, "score": 1, "url": "https://www.reddit.com/r/dataengineering/comments/1oo0ido/seeking_advice_best_tools_for_compiling_web_data/"}, {"id": "1oobgxr", "name": "t3_1oobgxr", "content": "Most modern apps and systems rely on Apache Kafka somewhere in the stack, but using it as a real-time backbone across teams and applications remains unnecessarily hard.\n\nWhen we started Aklivity, our goal was to change that. We wanted to make working with real-time data as natural and familiar as working with REST. That led us to build Zilla, a streaming-native gateway that abstracts Kafka behind user-defined, stateless, application-centric APIs, letting developers connect and interact with Kafka clusters securely and efficiently, without dealing with partitions, offsets, or protocol mismatches.\n\nNow we\u2019re taking the next step with the Zilla Data Platform \u2014 a full-lifecycle management layer for real-time data. It lets teams explore, design, and deploy streaming APIs with built-in governance and observability, turning raw Kafka topics into reusable, self-serve data products.\n\nIn short, we\u2019re bringing the reliability and discipline of traditional API management to the world of streaming so data streaming can finally sit at the center of modern architectures, not on the sidelines.\n\n1. Read the full announcement here: [https://www.aklivity.io/post/introducing-the-zilla-data-platform](https://www.aklivity.io/post/introducing-the-zilla-data-platform)\n2. Request early access (limited slots) here: [https://www.aklivity.io/request-access](https://www.aklivity.io/request-access)", "date_utc": 1762273060.0, "title": "Announcing Zilla Data Platform", "upvote_ratio": 0.29, "score": 0, "url": "https://www.reddit.com/r/dataengineering/comments/1oobgxr/announcing_zilla_data_platform/"}, {"id": "1onlilw", "name": "t3_1onlilw", "content": "Role: Data engineer\nMNC company\nTeam size 5 people\nCompany: decent mnc but unfortunately my team is not\n\nMy manager said this is opportunity to improve the gaps. But if im being realistic, this is their way of telling the guy \"you are not suitable or good enough, here is some time for you to leave\"\n\nAlso, i have tried my best being a good employee. The way that i see is that this company's workload is ridiculously demanding.\n\n20 story points per sprints to begin with. And some of the tickets are just too many subtasks for 3 story points. For example setup an etl pipeline complete with cicd deployment for all envs will just cost you a 3 story point.. Besides usually the tickets just have the title, no description whatsoever. Assignee is responsible to find out information about the tickets. And i also got comments on things like i will need to have more accountability on the projects, I mean its just been 6 months.\n\n\nAnd there are 2 other seniors, both of them are workaholic and they basically set the bar here. they spent time working exactly 12 hours average on daily basis. Additionally, why im saying my team is weird is because i have been doing research and been talking to otber teams. Lets just say only my team have ridiculous story pointings. They shout worklife balance and no need to work extra hours, but how can one finish their task without extras hours if workloads are just too much. \n\nHonestly, although i can push myself to be like them, i choose not to. Im already senior level and looking for a place to settle and work as long as i could.\n\n\nQuestion, will things get better? Should I stay or leave? Manager said stuffs like will support during remaining probation but so far, everything that I suggested just thrown back at me.\n", "date_utc": 1762198425.0, "title": "Just got extended probation from a 6 months probation period", "upvote_ratio": 0.74, "score": 9, "url": "https://www.reddit.com/r/dataengineering/comments/1onlilw/just_got_extended_probation_from_a_6_months/"}, {"id": "1onc2lb", "name": "t3_1onc2lb", "content": "Besides data integrity issues, would multiple VARCHAR(256) columns differ from VARCHAR(65535) performance-wise in Redshift?   \nThank you!", "date_utc": 1762177345.0, "title": "Does VARCHAR(256) vs VARCHAR(65535) impact performance in Redshift?", "upvote_ratio": 0.82, "score": 16, "url": "https://www.reddit.com/r/dataengineering/comments/1onc2lb/does_varchar256_vs_varchar65535_impact/"}, {"id": "1onqq62", "name": "t3_1onqq62", "content": "Sorry for bit of venting, but if this helps other to make steer away from Rudderstack, self-hosting it or very unlikely, makes them get their act together, then something good came out of it.\n\nSo, we had a meeting some time back, being presented with options for dynamic configuration of destinations so that we could easily route events to our 40 +/- data sets on FB, [G.ads](http://G.ads) accounts etc. Also, we could of course have an EU data location. All on the starter subscription.\n\nThen, we sign up and pay, but who would know, EU support is now removed from the entry monthly plan. So EU data residency is now a paid extra feature.\n\nWe are told that EU data residency is for annual plans only, bit annoyed, but fair enough, so i head over to their pricing page to see the entry subscription in an annual plan. I contact them to proceed with this, and guess what, it is gone, just like that! And it is gone, despite (at this point) still being listed on their pricing page!\n\nOk, so after much back & forth, we are allowed to get the entry plan in annual (for an extra premium of course, gotta pay up). So now we finally have EU data residency, but now, all of a sudden the one important feature we were presented by their sales team is gone.\n\nWe already signed up now to the annual plan to get EU, so bit in the shit you can say, but I contact them, and 20 emails later we can get the dynamic configuration of destinations, if we upgrade to a new and more expensive plan.\n\nAnd to put it into context, starter annual is 11'800 USD for 7m events a month, so it is not like it is cheap in any way. God knows what we will end up paying in a few weeks or months from now, after having to constantly pay up for included features being moved to more expensive plans.\n\nIs segment, fivetran and the other ones equally as shit and eager with their enshittification? Is the only viable option self-hosting OSS or creating something yourself at this point?\n\nAnd what are you guys using? I have a few clients who need some good data infrastructure, and rest assured, I will surely never recommend any of them Rudderstack.", "date_utc": 1762210204.0, "title": "Rudderstack - King of enshittification. Alternatives?", "upvote_ratio": 0.86, "score": 5, "url": "https://www.reddit.com/r/dataengineering/comments/1onqq62/rudderstack_king_of_enshittification_alternatives/"}, {"id": "1on1ben", "name": "t3_1on1ben", "content": "Hi! Pam from the Microsoft Team.  Quick note to let you all know that\u00a0**Fabric Data Days starts November 4th.**\n\nWe've got live sessions on data engineering, exam vouchers and more.\n\nWe'll have sessions on cert prep, study groups, skills challenges and so much more!\n\nWe'll be offering 100% vouchers for exams DP-600 (Fabric Analytics Engineer) and DP-700 (Fabric Data Engineer) for people who are ready to take and pass the exam before December 31st!\n\nYou can register to get updates when everything starts -->\u00a0[https://aka.ms/fabricdatadays](https://aka.ms/fabricdatadays)\n\nYou can also check out the live schedule of sessions here -->\u00a0[https://aka.ms/fabricdatadays/schedule](https://aka.ms/fabricdatadays/schedule)\n\nYou can request exam vouchers starting on Nov 4 at 9am Pacific.\n\n\n\n", "date_utc": 1762140284.0, "title": "Fabric Data Days -- With Free Exam Vouchers for Microsoft Fabric Data Engineering Exam", "upvote_ratio": 0.87, "score": 37, "url": "https://www.reddit.com/r/dataengineering/comments/1on1ben/fabric_data_days_with_free_exam_vouchers_for/"}, {"id": "1onpfgb", "name": "t3_1onpfgb", "content": "Hello,\n\nI manage a fully on-prem data warehouse. We are using Datastage for our ETL and Oracle for our data warehouse. Our sources are a mix of APIs (some coded in python, others directly in datastage sequence jobs), databases and flat files. \n\nWe have a ton of transformation logic and also push out data to other systems (including SaaS platforms). \n\nWe are exploring migrating this environment in to GCP and am feeling a bit lost in terms of the variety of options it seems: Dataproc, Dataflow, Data fusion, cloud composer, etc\n\nSome of our projects are highly dependant and need to be scheduled accordingly, so I feel like a product like Composer would be helpful. But then I hear cases of people using Composer to execute Dataflow jobs. What\u2019s the benefit of this vs having composer run the python code directly? \n\nHas anyone gone through similar migrations, what worked well, any lessons learned?\n\nThanks in advance!  ", "date_utc": 1762207152.0, "title": "Datastage and Oracle to GCP", "upvote_ratio": 0.5, "score": 0, "url": "https://www.reddit.com/r/dataengineering/comments/1onpfgb/datastage_and_oracle_to_gcp/"}, {"id": "1onoavq", "name": "t3_1onoavq", "content": "", "date_utc": 1762204579.0, "title": "Creating a PostgreSQL Extension: Walk through how to do it from start to finish", "upvote_ratio": 0.57, "score": 1, "url": "https://www.pgedge.com/blog/returning-multiple-rows-with-postgres-extensions"}, {"id": "1ondntd", "name": "t3_1ondntd", "content": "Event streams are amazing for real-time pipelines, but changing schemas in production is always tricky. Adding or removing fields, or changing field types, can quietly break downstream consumers\u2014or force a painful reprocessing run.\n\nI\u2019m curious how others handle this in production: Do you version events, enforce strict validation, or rely on downstream flexibility? Any patterns, tools, or processes that actually prevented headaches?\n\nIf you can, share real examples: number of events, types of schema changes, impact on consumers, or little tricks that saved your pipeline. Even small automation or monitoring tips that made schema evolution smoother are super helpful.", "date_utc": 1762181227.0, "title": "Handling Schema Changes in Event Streams: What\u2019s Really Effective", "upvote_ratio": 0.63, "score": 2, "url": "https://www.reddit.com/r/dataengineering/comments/1ondntd/handling_schema_changes_in_event_streams_whats/"}, {"id": "1on98jz", "name": "t3_1on98jz", "content": "First episode of Upstream - a new series of 1:1 conversations about the Data Streaming industry.  \n  \nIn this episode I'm hosting Yaroslav Tkachenko, an independent Consultant, Advisor and Author.   \n  \nWe're talking about recent innovations in the Flink ecosystem:   \n\\- VERA-X   \n\\- Fluss  \n\\- Polymorphic Table Functions   \nand much more.", "date_utc": 1762169209.0, "title": "Yaroslav Tkachenko on Upstream: Recent innovations in the Flink ecosystem", "upvote_ratio": 0.67, "score": 2, "url": "https://youtu.be/X6Ukpi2p4y4"}, {"id": "1omy7dm", "name": "t3_1omy7dm", "content": "Basically title. I've been reviewing a lot of code at my new job that makes use of BigQuery's array types with patterns like\n\n    with cte as (\n    select\n        customer_id,\n        array_agg(sale_date) as purchase_dates\n    from sales\n    where foo = 'bar'\n    )\n    select\n        customer_id,\n        min(purchase_date) as first_purchase\n    from cte,\n    unnest(purchase_dates) as purchase_date\n\nMy initial instinct is that we shouldn't be doing this and should keep things purely tabular. But I'm wondering if I'm just being a boomer here. \n\n  \nHave you use array-types in your data model? How did it go? Did it help? did it make things more complicated? was it good or bad for performance?\n\n  \nI'm curious to hear your experiences", "date_utc": 1762131325.0, "title": "How do you feel about using array types in your data model?", "upvote_ratio": 0.88, "score": 25, "url": "https://www.reddit.com/r/dataengineering/comments/1omy7dm/how_do_you_feel_about_using_array_types_in_your/"}, {"id": "1on8j79", "name": "t3_1on8j79", "content": "Hi everyone,\n\nI\u2019m currently doing an academic\u2013industry internship where I\u2019m researching polyglot persistence, the idea that instead of forcing all data into one system, you use multiple specialized databases, each for what it does best.\n\nFor example, in my setup:\n\nPostgreSQL \u2192 structured, relational geospatial data\n\nMongoDB \u2192 unstructured, media-rich documents (images, JSON metadata, etc.)\n\nDuckDB \u2192 local analytics and fast querying on combined or exported datasets\n\nFrom what I\u2019ve read in literature reviews and technical articles, polyglot persistence is seen as a best practice for scalable and specialized architectures. Many papers argue that hybrid systems allow you to leverage the strengths of each database without constantly migrating or overloading one system.\n\nHowever, when I read Reddit threads, GitHub discussions, and YouTube comments, most developers and data engineers seem to say the opposite, they prefer sticking to one single database (usually PostgreSQL or MongoDB) instead of maintaining several.\n\nSo my question is:\n\n**Why is there such a big gap between the theoretical or architectural support for polyglot persistence and the real-world preference for a single database system?**\n\nIs it mostly about:\n\nMaintenance and operational overhead (backups, replication, updates, etc.)?, Developer team size and skill sets?, Tooling and integration complexity?, Query performance or data consistency concerns?, Or simply because \u201cgood enough\u201d is more practical than \u201cperfectly optimized\u201d?\n\n**Would love to hear from those who\u2019ve tried polyglot setups or decided against them, especially in projects that mix structured, unstructured, and analytical data. Big thanks! Ale**", "date_utc": 1762166716.0, "title": "Polyglot Persistence or not Polyglot Persintence?", "upvote_ratio": 0.6, "score": 2, "url": "https://www.reddit.com/r/dataengineering/comments/1on8j79/polyglot_persistence_or_not_polyglot_persintence/"}, {"id": "1onfy6g", "name": "t3_1onfy6g", "content": "Anyone moved away from Databricks clusters and hosting jobs mainly on Spark and Kubernetes? Any POC's or guidance is much appreciated.. ", "date_utc": 1762186398.0, "title": "Execution on Spark and Kubernetes", "upvote_ratio": 0.5, "score": 0, "url": "https://www.reddit.com/r/dataengineering/comments/1onfy6g/execution_on_spark_and_kubernetes/"}, {"id": "1onbxlw", "name": "t3_1onbxlw", "content": "First, I've searched for this topic in other posts, but the ones which would be of more help are years old, and since it involves a fair amount of money, I'd like an up to date point of view.\n\nContext:\n\n* I need to spend a budget the company I work for separated for training within a month, at most.\n* I'm currently working on a project that involves DE (I'm working with an experienced Data Engineer), and it would be good to get more knowledge on the field. Also, we're working on AWS.\n* I'm a Data Analyst with a couple years of experience: this is just to say I have a good base in programming and a general knowledge in the data field.\n* I already enrollled in Coursera Plus and Udemy Premium for a year using this budget, but I still have some money left to spend.\n\nThat said, I'm looking for good places on which to spend this money. The cost of Udacity's \"Data Engineering with AWS\" (the 1 year individual course) is virtually the same amount of money I have left to spend. But the thing is, even though it's not my money, I want to make it worth it. Like, I personally think it's very expensive, so I don't want to spend it on something that won't add value to my career. I've read several comments on other posts here saying this nanodegree is sometimes outdated, the mentor's knowledge being very limited to the course's subject etc.\n\nSo, in case there's someone there who did this course recently, I wish you could share some opinions on it. Other suggestions are also welcome, on the condition they fit the budget of $ 600 - $ 700, but having in mind I speak from Brazil, so in situ suggestions are harder to actually consider. Also, though I'm aiming at DE training because of the immediate context I explained above, suggestions of courses in  related fields (like, if you think I should purchase a Machine Learning course) are also welcome. Thanks in advance!", "date_utc": 1762176981.0, "title": "Looking for updated help on Udacity's \"Data Engineering with AWS\"", "upvote_ratio": 0.5, "score": 0, "url": "https://www.reddit.com/r/dataengineering/comments/1onbxlw/looking_for_updated_help_on_udacitys_data/"}, {"id": "1omgr2d", "name": "t3_1omgr2d", "content": "This is the ultimate exercise in pedantry because both are probably wrong (short for variable character so it should be Vare-Care?) yet everyone will have excruciatingly HOT TAKES on which one is correct.\n\nI say var-CAR with emphasis on the car. Fight me", "date_utc": 1762088682.0, "title": "Var-Car or Var-Char?", "upvote_ratio": 0.71, "score": 39, "url": "https://www.reddit.com/r/dataengineering/comments/1omgr2d/varcar_or_varchar/"}, {"id": "1omi90y", "name": "t3_1omi90y", "content": "I\u2019m doing my Master\u2019s in AI and Business Analytics here in the US, with about 16 months left before I graduate. I\u2019ve done an AI-focused internship for a year, and I consider myself intermediate in Python, SQL, and ML.\n\nI\u2019m stuck deciding between two paths -\n\n- AI/ML sounds exciting but honestly, It feels like I\u2019d constantly have to innovate and keep up with new research, and Idk if I can keep that pace long term.\n\n- Data engineering seems more stable and routine because it\u2019s mainly building and maintaining pipelines. I like that it feels more structured day-to-day, but I\u2019d basically be starting from scratch learning it.\n\nWith just 16 months left and visa rules changing, I\u2019m nervous about making the wrong choice.\nIf you\u2019ve worked in either field, what\u2019s your honest take on this?\n\nBased on my profile, i might struggle to land an entry-level ML job cos I only have one year of internship experience. I\u2019d really appreciate your recommendations. I get that ML jobs are limited, so any guidance to navigate this would mean a lot.\n\nI\u2019m confident I can put in the work necessary but the thought of my AI/ML internship experience going to waste if I switch to data engineering is scary. I\u2019m not afraid to start fresh, but I want to be smart about it", "date_utc": 1762092666.0, "title": "AI/ML vs Data Engineering - Need Career Advice", "upvote_ratio": 0.87, "score": 25, "url": "https://www.reddit.com/r/dataengineering/comments/1omi90y/aiml_vs_data_engineering_need_career_advice/"}, {"id": "1omf6qa", "name": "t3_1omf6qa", "content": "Hi all. I\u2019ve been studying DE for the past 6 months. Had to start from zero with Python and move slowly to sqlite and pandas. I have a family and a day job that keeps me pretty busy so I can only afford to spend just a bit of time on my learning project. But I\u2019ve got pretty deep into it now. Was wondering if you guys clould tell me what a typical day at the \u201coffice\u201d looks like for a DE? What tech stack is usually used. How much data transformation work is there to be done vs analysis. Thank you in advance for taking the time to answer. Appreciate you!", "date_utc": 1762083885.0, "title": "Aspiring Data Engineer looking for a Day in the Life", "upvote_ratio": 0.91, "score": 37, "url": "https://www.reddit.com/r/dataengineering/comments/1omf6qa/aspiring_data_engineer_looking_for_a_day_in_the/"}, {"id": "1omoi7n", "name": "t3_1omoi7n", "content": "Hello,\n\nI\u2019m a data engineer with over three years of experience, and I\u2019m wondering if it\u2019s difficult to find a job in Vienna knowing only English. I\u2019m currently learning German and willing to improve, but I\u2019m worried that it might still be hard to get a job there since many positions require German more than b2.\n\nHas anyone had a similar experience?\n\n", "date_utc": 1762107288.0, "title": "Finding a job in Vienna", "upvote_ratio": 0.65, "score": 7, "url": "https://www.reddit.com/r/dataengineering/comments/1omoi7n/finding_a_job_in_vienna/"}, {"id": "1omv5sn", "name": "t3_1omv5sn", "content": "Are there opportunities for Europeans with more than three years of experience? Is it difficult to secure a job from abroad with a working holiday visa and potential future common-law sponsorship? I\u2019ve been genuinely curious about moving to Toronto / Montreal / Vancouver someday next year.", "date_utc": 1762123172.0, "title": "DE from Canada, what's it like there in 2025?", "upvote_ratio": 0.55, "score": 1, "url": "https://www.reddit.com/r/dataengineering/comments/1omv5sn/de_from_canada_whats_it_like_there_in_2025/"}, {"id": "1omsuf4", "name": "t3_1omsuf4", "content": "I\u2019m trying to figure out which version of the Professional Data Engineer cert is easier to pass - Standard or Renewal. Since my last exam, I\u2019ve mostly been working in another cloud, so I don\u2019t have hands-on experience with the latest GCP services. That said, I\u2019ve been studying the docs and sample questions (Dataplex, Lakehouse, Data Mesh, BigLake, Analytics Hub, BigQuery Editions, etc.).\n\nI\u2019m wondering if it would be better to take the 2-hour Standard exam with my solid knowledge of the other services, or if it might make more sense to try the Renewal. I understand the newer services conceptually, but I haven\u2019t worked with them directly, so I might be missing some details.\n\nHas anyone taken the Renewal version and can share their experience?", "date_utc": 1762117441.0, "title": "GCP Cert: Standard vs Renewal - which one\u2019s easier?", "upvote_ratio": 0.72, "score": 3, "url": "https://www.reddit.com/r/dataengineering/comments/1omsuf4/gcp_cert_standard_vs_renewal_which_ones_easier/"}, {"id": "1omivbx", "name": "t3_1omivbx", "content": "I am considering a W11 on ARM laptop, but want to check if there are any gotchas with typical DE style work. If you have one please let me know how its gone.\n\nI use all the normal DE stuff for a Microsoft aligned person, so Fabric, Databricks, all the Azure goodness and CLIs, VS Code and VS. Plus the normal Microsoft stack for office apps and such (Outlook, Teams, etc).\n\nMy wife has a Surface Laptop Copilot+ PC. Ignoring the Copilot nonsense, its a great laptop. And I am very very bored of \\~2h battery life with small x86 laptops, and envious that she doesn't even take a charger with her for a full days work with her ARM laptop.\n\nConsidering almost all I do is cloud or office app based I think I'm fine. VS Code has native ARM versions, as does most of the rest of what I use. I also use devcontainers with Docker a lot, and that seems to be mostly fine from what I read. The only catches maybe some legacy tools like SSMS, but I think there's little/nothing I can't do with VS Code these days anyway.\n\ntl;dr is a W11 on ARM laptop a problem for anything DE related?", "date_utc": 1762094177.0, "title": "Are you a DE using a Windows 11 on ARM laptop? If so, tell me about any woes!", "upvote_ratio": 0.76, "score": 8, "url": "https://www.reddit.com/r/dataengineering/comments/1omivbx/are_you_a_de_using_a_windows_11_on_arm_laptop_if/"}, {"id": "1omb2va", "name": "t3_1omb2va", "content": "Been somewhat in the data field for about 4 years now, not necessarily in the pure engineering field. Using SQL (mysql, postgres for hobby projects), GCP (bigquery, cloud functions, gcs time to time), some python, package and their likes. \nI was thinking if I should keep learning the fundamentals : Linux, SQL (deepen my knowledge), python. \nBut lately I have been wondering if I should also put my energy elsewhere. \nLike dbt, pyspark, CI/CD, airflow... I mean the list go on and on. I often think I don't have the infrastructure or the type or data needed to play with pyspark, but maybe I am just finding an excuse. What would you recommend learning, something that will pay dividends in the long run ? ", "date_utc": 1762068018.0, "title": "Learning new skills", "upvote_ratio": 0.89, "score": 23, "url": "https://www.reddit.com/r/dataengineering/comments/1omb2va/learning_new_skills/"}, {"id": "1ombfgf", "name": "t3_1ombfgf", "content": "Dev team set up AWS Glue for all our Redshift pipelines. It works but our analysts are not happy with this setup because they are dependent on devs for all data points.\n\nGlue doesn't work for anyone who isnt good at PySpark. Our analysts know SQL but they can't do things themselves and are bottlenecked by the dev team. \n\nWe are looking for Redshit ETL tool setup that's like Glue but is low code enough for our BI team to not be blocked frequently. We also don't want to manage servers. And again writing Spark code just to manage new data source would also be pointless.\n\nHow do you suggest we address this? Not a pro at this.", "date_utc": 1762069432.0, "title": "Need help with Redshift ETL tools", "upvote_ratio": 0.93, "score": 22, "url": "https://www.reddit.com/r/dataengineering/comments/1ombfgf/need_help_with_redshift_etl_tools/"}, {"id": "1omok7d", "name": "t3_1omok7d", "content": "TLDR; why bother storing & managing my \\*structured\\* data in a data lake if I can store it in RMS with the same cost?\n\n\\---\n\nHi all, new data engineer here. As titled, am I Missing Hidden Costs/Tradeoffs?\n\nWe're a small shop, 5 data people, <10TB of production data.\n\nWe used to run our analytics in the production's read replica, but nowadays it always timed out / failed bcz of transaction conflicts.\n\nWe're storing a snapshot of historical data every day for audit/regulatory purposes (as pg dump and restoring it when we need to do an audit.\n\nWe're moving our data to a dedicated place. We're considering ingesting our production data to a simple iceberg/s3 tables and using Athena for analytics.\n\nBut we're also considering Redshift serverless + Redshift's managed storage, which apparently, the pricing for RMS ($0.024/GB) is now closly matches S3 Standard ($0.023/GB in our region). Our data is purely structured (Parquet/CSV) with predictable access patterns.\u00a0\u00a0\n\nFor the compute cost, we estimated that the RSS will cost us <500$/mo. I haven't estimated the Athena cost query because I don't know how to translate our workload into equivalent Athena scan cost.\n\nWith either of these new setup, we will take a full snapshot of our postgres everyday and dump it to our datalake/redshift\n\nWhy I'm Considering Redshift:\u00a0\u00a0\n\n\\- We're pretty much an all-in AWS shop now, not going to move anywhere in the quite long term.  \n\\- Avoid complex data lake/iceberg maintenance  \n\\- I can still archive snapshot data older than a certain period to the cheaper S3 tier when I need to.\n\nI'm coming here from the idea that I can have an exabyte of data on my storage, but that won't affect the performance of my DB if I don't query it.  \nOn Redshift, I'm thinking to achieve this by either 1. storing older snapshots on a different table or 2. using \"snapshot\\_date\" as a sort key so that unrelated data will be filtered when doing a query\n\nQuestion:\n\n1. Does this storage model make sense?\n2. Are there hidden compute costs? (from vacuuming/optimization)", "date_utc": 1762107415.0, "title": "Redshift Managed Storage vs S3 for Structured Data", "upvote_ratio": 0.72, "score": 3, "url": "https://www.reddit.com/r/dataengineering/comments/1omok7d/redshift_managed_storage_vs_s3_for_structured_data/"}, {"id": "1om350m", "name": "t3_1om350m", "content": "Hey everyone, I\u2019m currently a credit risk intern at a big bank in Latin America. Most of what I do is generating and running databases on SQL or Python building ETL/ELT pipelines (mostly low/no code) on AWS with services like Athena, S3, Glue, SageMaker, and QuickSight.\n\nIf I get promoted, I\u2019ll become a credit analyst, which is fine. There\u2019s also a path here for data analysts to move into data science, which pays better and involves predictive analytics and advanced modeling.\n\nThat used to be my plan, but lately I\u2019ve realized I\u2019m not sure it\u2019s for me. I\u2019m autistic and I struggle with the constant presentations, storytelling, and meetings that come with more \u201cbusiness-facing\u201d roles. I\u2019m very introverted and prefer structured, predictable work and I\u2019ve noticed the parts I enjoy most are the engineering ones: building pipelines, automating processes, making data flow efficiently.\n\nI don\u2019t have a software engineering background (I\u2019m a physicist), but I\u2019ve always done well with computational work and Python. I haven\u2019t worked with Spark, IaC, or devops yet, but I like that kind of technical challenge.\n\nI\u2019m not looking for advice per se, just wanted to share my thoughts and see if anyone here has had a similar experience, moving from a data or analytics-heavy background into something more engineering-focused.", "date_utc": 1762041929.0, "title": "For what reasons did/would you migrate from data analytics/science to data engineering?", "upvote_ratio": 0.91, "score": 57, "url": "https://www.reddit.com/r/dataengineering/comments/1om350m/for_what_reasons_didwould_you_migrate_from_data/"}, {"id": "1om6fnz", "name": "t3_1om6fnz", "content": "Hi, I've been having some kind of existential crisis because of my career. I feel like my job right now isn't very meaningful because it's not benefiting people in a notable way, it's just working to make some people richer and richer and I feel like I'm not being challenged enough.\n\nBeen through so much projects, having fun creating data pipelines but at the end of the day, late at night at wonder how could i put my technical skills towards something more meaningful than becoming a pixie?\n\nAre there any NGO or do you have some ideas worth working for?", "date_utc": 1762051691.0, "title": "How can we use data engineering for good?", "upvote_ratio": 0.75, "score": 13, "url": "https://www.reddit.com/r/dataengineering/comments/1om6fnz/how_can_we_use_data_engineering_for_good/"}, {"id": "1olr53b", "name": "t3_1olr53b", "content": "I feel quite disappointed with my career. I always have projects that are sales this sales that, discount this customer that. I feel like all exciting data engineering projects are taken by an ellite somewhere in the US, but here in Europe it\u2019s very hard!\n\n", "date_utc": 1762012047.0, "title": "Why is it so difficult to find data engineering jobs that are non-sales, non-finance, non-app engagement -related?", "upvote_ratio": 0.87, "score": 93, "url": "https://www.reddit.com/r/dataengineering/comments/1olr53b/why_is_it_so_difficult_to_find_data_engineering/"}, {"id": "1omc0ch", "name": "t3_1omc0ch", "content": "Hi all!\nI am a data engineer by trade and I am currently working on a project involving streaming data in from an s3 parquet table into an ML model hosted in ec2 (specifically a Keras model). I am using data generators to Lazy load the data with pandas wrangler and turn it into a tensor. I have already parallelized my lazy loads, but I\u2019m running into a couple of roadblocks that I was hoping the community might have answers to. \n1. What is the most efficient/standard way to lazy load data from an s3 parquet table? I\u2019ve been iterating by partition (utc date + Rand partition key) but it\u2019s a pretty slow response time (roughly 15 second round trip per partition). \n2. My features and targets are in separate s3 tables right now. Is there an efficient way to join them at load or should I set up an upstream spark job to join the feature and target set to a single bucket and work from there? My intuition is that the load and x-process of handling that join for a disjoint set will be completely inefficient, but it would be a large data duplication if I have to maintain an entire separate table just to have features and targets combined in one parquet file. Any insight here would be appreciated!\nThank you!", "date_utc": 1762071747.0, "title": "Parquet lazy loading", "upvote_ratio": 0.61, "score": 2, "url": "https://www.reddit.com/r/dataengineering/comments/1omc0ch/parquet_lazy_loading/"}, {"id": "1om9hz8", "name": "t3_1om9hz8", "content": "I\u2019ve been experimenting w/ generating grammar QA data recently and trying to keep 5-gram duplication under ~2% via a simple sliding-window check.\n\nCurious how folks here measure/monitor duplication or near-duplicates in data pipelines, especially when data is partly synthetic or augmented.\n\nDo you rely on:\n\u2013 n-grams\n\u2013 embedding similarity\n\u2013 MinHash / locality-sensitive hashing\n\u2013 or something else?\n\nBonus Q: for education-focused datasets, is ~2% dup considered \u201cgood enough\u201d in practice?\n\nNot trying to market anything \u2014 just trying to see what quality bars look like in real-world pipelines.\n\nContext: local pipeline + Colab mix for iteration.", "date_utc": 1762061990.0, "title": "What\u2019s an acceptable duplication rate for synthetic or augmented datasets in production pipelines?", "upvote_ratio": 0.67, "score": 2, "url": "https://www.reddit.com/r/dataengineering/comments/1om9hz8/whats_an_acceptable_duplication_rate_for/"}, {"id": "1olrnji", "name": "t3_1olrnji", "content": "Anybody put in place platform matching and mastering, golden records etc? What did it look like in practice? What were biggest insights and the small wins? ", "date_utc": 1762013284.0, "title": "What does Master Data Management look like in real world?", "upvote_ratio": 0.82, "score": 14, "url": "https://www.reddit.com/r/dataengineering/comments/1olrnji/what_does_master_data_management_look_like_in/"}, {"id": "1om10td", "name": "t3_1om10td", "content": "I have been appointed to perform a record linkage of some databases of a company which I am doing a intership. So I studied a bit and found thought of using a library called splink in python to do the linkage.\n\n  \nAs I introduced my plan a datascientist from my team suggested me to do everything in BigQuery and do not use colab and python as there is a chance of malware being embbed in the library (or its dependencies) -- he does not know anything about the library, just warned me.\n\nAs I have basically no xp whatsoever I got a bit afraid to move on with my idea, however I feel that yet I'm not capable to work on a script on SQL that does the job (I have basic SQL). The Databases are very untidy, with loads of missing values, no universal id and lots of errors and misspelling.\n\nI wanted to know experiences about these kind of problems and maybe to understand what should and could do.", "date_utc": 1762036220.0, "title": "Is there a chance of data leakage when doing record linkage using splink?", "upvote_ratio": 0.72, "score": 3, "url": "https://www.reddit.com/r/dataengineering/comments/1om10td/is_there_a_chance_of_data_leakage_when_doing/"}, {"id": "1olsxt2", "name": "t3_1olsxt2", "content": "Hi   \nIs there anyone who is working and has experience in Databricks + AWS (s3,Redshift)  \nI'm a data engineer who is over 1 yr exp. Now I am about getting into learning and start using Databricks for my next projects.   \nand I'm getting trouble \n\n  \ncurrently I mounted s3 bucket for databricks storage and whenever need some data I try to export from AWS Redshift to s3 so that I can use in Databricks and now some unity catalog and tracking and notebook result or ML flow are extremly rising on s3 storage. I am try to clean up and reduce this mass. I was confused to impact if I delete some folders and files, I'm afraid go to break current ML flow or pipeline or tables on Databricks. \n\nand I'm thinking what if I connect and use data from Redshift to Databricks via direct connect for what i want data same as like Redshift on Databricks.\n\nwhich method are more suitable and any other expert advice can I get from you all\n\nI do really appreciate.", "date_utc": 1762016371.0, "title": "Jump into Databricks", "upvote_ratio": 0.67, "score": 2, "url": "https://www.reddit.com/r/dataengineering/comments/1olsxt2/jump_into_databricks/"}, {"id": "1oli3us", "name": "t3_1oli3us", "content": "I need help setting up the cluster configuration for an AWS Glue job.\n\nI have around 20+ table snapshots stored in Amazon S3 ranging from 200mb to 12gb. Each snapshot contains small files.\n\nEventually, I join all these snapshots, apply several transformations, and produce one consolidated table.\n\nThe total input data size is approximately 200 GB.\n\nWhat would be the optimal worker type and number of workers for this setup?\n\nMy current setup is g4x with 30 workers and it takes about 1 hour aprox. Can i do better?", "date_utc": 1761983482.0, "title": "Need advice on AWS glue job sizing", "upvote_ratio": 0.77, "score": 7, "url": "https://www.reddit.com/r/dataengineering/comments/1oli3us/need_advice_on_aws_glue_job_sizing/"}, {"id": "1oletye", "name": "t3_1oletye", "content": "My goal is to network & learn, I'm willing to pay the conference price as well if required...\n\nWhat are the most popular that are worth attending? USA!", "date_utc": 1761970531.0, "title": "What are some of the best conferences worth attending?", "upvote_ratio": 0.78, "score": 13, "url": "https://www.reddit.com/r/dataengineering/comments/1oletye/what_are_some_of_the_best_conferences_worth/"}, {"id": "1ol22zz", "name": "t3_1ol22zz", "content": "While I think every generally has the same idea when it comes to medallion architecture, I'll see slight variations depending on who you ask. How would you define:\n\n\\- The lines between what transformations occur in Silver or Gold layers  \n\\- Whether you'd add any sub-layers or add a 4th platinum layer and why  \n\\- Do you have a preferred naming for the three layer cake approach", "date_utc": 1761934619.0, "title": "How do you define, Raw - Silver - Gold", "upvote_ratio": 0.94, "score": 64, "url": "https://www.reddit.com/r/dataengineering/comments/1ol22zz/how_do_you_define_raw_silver_gold/"}, {"id": "1olis7w", "name": "t3_1olis7w", "content": "Long question but this i the case. Working in a large company which uses Oracle (local install, computers in the basement) for warehouse. I know that that the goal is to go for cloud in the future (even if I think it is not wise) but no date and time frame is given.\n\nI have gotten the opportunity to take a deep dive into how Oracle work and how to optimize queries.  But is this knowledge that can be used in the cloud database we probably is going to use in 4-5 years? Or will this knowledge be worth anything when migrating to Google Big Query/Snowflake/WhatIsHotDatabaseToday.\n\nSome of my job is vendor independent like planning warehouse structure and making ETL and I can just go on with that if I do no want to take this role.  \n\n", "date_utc": 1761986266.0, "title": "Specialize in  Oracle query optimizationwhen team will move to another vendor in the long term?", "upvote_ratio": 0.78, "score": 5, "url": "https://www.reddit.com/r/dataengineering/comments/1olis7w/specialize_in_oracle_query_optimizationwhen_team/"}, {"id": "1okxs7r", "name": "t3_1okxs7r", "content": "With nary 8.5 hours to spare (GMT) before the end of the month, herewith a whole lotta links about things in the data engineering world that I found interesting this month. \n\n\ud83d\udc49 https://rmoff.net/2025/10/31/interesting-links-october-2025/", "date_utc": 1761924831.0, "title": "Interesting Links in Data Engineering - October 2025", "upvote_ratio": 0.91, "score": 65, "url": "https://www.reddit.com/r/dataengineering/comments/1okxs7r/interesting_links_in_data_engineering_october_2025/"}, {"id": "1okq7tm", "name": "t3_1okq7tm", "content": "Genuine question from someone who's been cleaning up after data scientists for three years now.\n\nThey'll spend months perfecting a model, then hand us a jupyter notebook with hardcoded paths and say \"can you deploy this?\" No documentation. No reproducible environment. Half the dependencies aren't even pinned to versions.\n\nLast week someone tried to push a model to production that only worked on their specific laptop because they'd manually installed some library months ago and forgot about it. Took us four days to figure out what was even needed to run the thing.\n\nI get that they're not infrastructure people. But at what point does this become their problem too? Or is this just what working with ml teams is always going to be like?", "date_utc": 1761903977.0, "title": "Why do ml teams keep treating infrastructure like an afterthought?", "upvote_ratio": 0.97, "score": 181, "url": "https://www.reddit.com/r/dataengineering/comments/1okq7tm/why_do_ml_teams_keep_treating_infrastructure_like/"}, {"id": "1ol60vw", "name": "t3_1ol60vw", "content": "I just published a short video explaining the core idea behind Dagster \u2014 *assets*.\n\nNo marketing language, no hand-waving \u2014 just the conceptual model, explained in 4 minutes.\n\nLooking forward to thoughts / critique from others using Dagster in production.", "date_utc": 1761944135.0, "title": "Dagster 101 \u2014 The Core Concepts Explained (In 4 Minutes)", "upvote_ratio": 0.83, "score": 22, "url": "https://youtube.com/watch?v=k-iQNrtKhjo&si=6NoEcoDnNP-oh6NM"}, {"id": "1ol5wr5", "name": "t3_1ol5wr5", "content": "I\u2019m building a dbt project with multiple source systems that all eventually feed into a single modeled (mart) table (e.g., accounts). Each source requires quite a bit of unique, source-specific transformation such as de-duping, pivoting, cleaning, enrichment, *before* I can union them into a common intermediate model.\n\nRight now I\u2019m wondering where that heavy, source-specific work should live. Should it go in the staging layer? Should it be done in the intermediate layer?  What\u2019s the dbt recommended pattern for handling complex per-source transformations before combining everything into unified intermediate or mart models?", "date_utc": 1761943849.0, "title": "DBT - How to handle complex source transformations before union?", "upvote_ratio": 0.96, "score": 20, "url": "https://www.reddit.com/r/dataengineering/comments/1ol5wr5/dbt_how_to_handle_complex_source_transformations/"}, {"id": "1okuvgs", "name": "t3_1okuvgs", "content": "**Context**: Work for a big consultant firm. We have a hardware/onprem biz unit as well as a digital/cloud-platform team (snow/bricks/fabric)\n\n**Recently**: Our leaders of the onprem/hdwr side were approached by a major hardware vendor re; their new AI/Data in-a-box. I've seen similar from a major storage vendor.. Basically hardware + Starburst + Spark/OSS + Storage + Airflow + GenAI/RAG/Agent kit. \n\n**Questions**: Not here to debate the functional merits of the onprem stack. They work, I'm sure. but...\n\n1) Who's building on a modern data stack, \\*\\*on prem\\*\\*? Can you characterize your company anonymously? E.g. Industry/size? \n\n2) Overall impressions of the DE experience? \n\n  \nThanks. Trying to get a sense of the market pull and if should be enthusiastic about their future. ", "date_utc": 1761917952.0, "title": "Onprem data lakes: Who's engineering on them?", "upvote_ratio": 0.9, "score": 25, "url": "https://www.reddit.com/r/dataengineering/comments/1okuvgs/onprem_data_lakes_whos_engineering_on_them/"}, {"id": "1oksihc", "name": "t3_1oksihc", "content": "Hello everyone, I'm making a follow up question to my post here in this sub too.\n\ntl;dr: I made up my mind to migrate to SQLite and using dbeaver to view my data, potentially in the future making simple interfaces myself to easily insert new data/updating some stuff.\n\nNow here's the new issue, as a background the data I'm working it is actually similar to the basic data presented on my dbms course, class/student management. Essentially, I will have the following entity:\n\n* student\n* class\n* teacher\n* payment\n\nAnd while designing this new database, aside from migration, I'm currently planning ahead on implementing design choices that will help me with my work, some of them are currently this:\n\n* track payments (installment/renewal, if installment, how much left, etc)\n* attendance (to track whether or not the student skipped the class, more on that below)\n\nBasically, my company's course model is session based, so students paid some amount of sessions, and they will attend the class based on this sessions balance, so to speak. I came up with a two ideas for this attendance tracking:\n\n* since they are on fixed schedule, only lists out when they took a leave (so it wouldn't be counted on the number of sessions they used)\n* make an explicit attendance entity.\n\nI get quite overwhelmed with the rabbit hole of trying to make the db perfect from the start. Is it easy to just change my schema on the run? Or is what I'm doing (i.e. putting more efforts at the start) is better? How should I know is my design is already fine?\n\nThanks for the help!", "date_utc": 1761911703.0, "title": "Database Design for Beginners: How not to overthink?", "upvote_ratio": 0.89, "score": 19, "url": "https://www.reddit.com/r/dataengineering/comments/1oksihc/database_design_for_beginners_how_not_to_overthink/"}, {"id": "1okvhyc", "name": "t3_1okvhyc", "content": "NOTE: Im new in this.  \nI'm interested if there are any current opensource solutions that have both of these in one?  \nI saw that UC has, but doesn't work with iceberg tables, and that DataHub has Iceberg Catalog, but i feel like i am missing something.\n\nIf im not asking something smart, feel free to roast me. Thanks", "date_utc": 1761919475.0, "title": "Data catalog that also acts as metadata catalog", "upvote_ratio": 0.92, "score": 10, "url": "https://www.reddit.com/r/dataengineering/comments/1okvhyc/data_catalog_that_also_acts_as_metadata_catalog/"}, {"id": "1ol4qz0", "name": "t3_1ol4qz0", "content": "Can Kafka keep pace with modern AI workloads? Let\u2019s find out.\n\nStreamfest 2025 (Nov 5\u20136) brings together\u00a0[Alexander Gallego](https://www.linkedin.com/in/alexandergallego/)\u00a0\u00a0with\u00a0[Stanislav Kozlovski](https://www.linkedin.com/in/stanislavkozlovski/),\u00a0[Filip Yonov](https://www.linkedin.com/in/filipyonov/),\u00a0[Kir Titievsky \ud83c\uddfa\ud83c\udde6](https://www.linkedin.com/in/kir-titievsky-%F0%9F%87%BA%F0%9F%87%A6-7775052/), and\u00a0[Tyler Akidau](https://www.linkedin.com/in/takidau/)\u00a0\u2014 a rare panel spanning\u00a0[Redpanda Data](https://www.linkedin.com/company/redpanda-data/),\u00a0[Google](https://www.linkedin.com/company/google/), and\u00a0[Aiven](https://www.linkedin.com/company/aiven/).\n\nExpect takeaways on: scaling AI pipelines with Kafka, ecosystem upgrades to watch, and what enterprises should plan for next.\n\nRegister now: [https://www.redpanda.com/streamfest](https://www.redpanda.com/streamfest)\n\n\\[Disclosure: I work for Redpanda Data.\\]", "date_utc": 1761940968.0, "title": "The Future of Kafka [Free Online Event / Panel Talk]", "upvote_ratio": 0.73, "score": 5, "url": "https://i.redd.it/fy4rbmzz6iyf1.jpeg"}, {"id": "1okq6mu", "name": "t3_1okq6mu", "content": "Many data engineering pipelines now deal with semi-structured data like JSON, Avro, or Parquet. Storing and querying this kind of data efficiently in production can be tricky. I\u2019m curious what strategies data engineers have used to handle semi-structured datasets at scale.\n\n* Did you rely on native JSON/JSONB in PostgreSQL, document stores like MongoDB, or columnar formats like Parquet in data lakes?\n* How did you handle query performance, indexing, and schema evolution?\n* Any batching, compression, or storage format tricks that helped speed up ETL or analytics?\n\nIf possible, share concrete numbers: dataset size, query throughput, storage footprint, and any noticeable impact on downstream pipelines or maintenance overhead. Also, did you face trade-offs like flexibility versus performance, storage cost versus query speed, or schema enforcement versus adaptability?\n\nI\u2019m hoping to gather real-world insights that go beyond theory and show what truly scales when working with semi-structured data.", "date_utc": 1761903858.0, "title": "Handling Semi-Structured Data at Scale: What\u2019s Worked for You?", "upvote_ratio": 0.95, "score": 20, "url": "https://www.reddit.com/r/dataengineering/comments/1okq6mu/handling_semistructured_data_at_scale_whats/"}, {"id": "1ol3zgs", "name": "t3_1ol3zgs", "content": "TL;DR: Assuming quantum computing reaches industry viability, what core assumptions about data change with this technology?\n\nI've been paying attention to quantum computing lately and its advancements towards industry applications over the past few years. Now, there is a huge question mark on whether this technology will even become viable within the next decade for industry application beyond research labs-- but regardless, it's fun to do these thought exercises.\n\n  \nTwo areas where I see key assumptions changing for data engineering are...\n\n1. Security Compliance and Governance\n2. Managing State\n\nThe security component is actually already top of mind for governments and major enterprises who are concerned with \"harvest now, decrypt later\" attacks ([NIST.gov Report](https://www.nist.gov/cybersecurity/what-post-quantum-cryptography), [Reuters article](https://www.reuters.com/investigates/special-report/us-china-tech-quantum/)). Essentially the core assumption is that encryption is \"obsolete\" if quantum becomes viable at scale so various actors are scooping up encrypted data today hoping the secrets will be useful in a future state.\n\n  \nThe managing state component is interesting to me as an entity can either be 0, 1, or simultaneously both (i.e. superposition) until measured. This is what opens up strong computing capabilities, but how would you model data with these properties?\n\n  \nIs anyone else thinking about this stuff?\n\n", "date_utc": 1761939119.0, "title": "Quantum Computing and Data Engineering?", "upvote_ratio": 0.63, "score": 2, "url": "https://www.reddit.com/r/dataengineering/comments/1ol3zgs/quantum_computing_and_data_engineering/"}, {"id": "1okshm9", "name": "t3_1okshm9", "content": "Thought this would be a fun one. I have a few good ones but I dont want to skew anyone\u2019s perception. Excited to hear what you all think!", "date_utc": 1761911631.0, "title": "What is your best metaphor for DE?", "upvote_ratio": 0.87, "score": 11, "url": "https://www.reddit.com/r/dataengineering/comments/1okshm9/what_is_your_best_metaphor_for_de/"}, {"id": "1ol7lta", "name": "t3_1ol7lta", "content": "Hello everyone,\n\nI\u2019m looking for feedback from this community and other data engineers on a small personal project I just built.\n\nAt this stage, it\u2019s a lightweight, local-first tool to validate and transform CSV/Parquet datasets using a simple registry-driven approach (YAML). You define file patterns, validation rules, and transformations in the registries, and the tool:\n\n* Matches input files to patterns defined in the registry\n* Runs validators (e.g., required columns, null checks, value ranges, hierarchy checks)\n* Applies ordered transformations (e.g., strip whitespace, case conversions)\n* Writes reports only when validations fail or transforms error out\n* Saves compliant or transformed files to the output directory\n* Generate report with failed validations\n* Give the user maximum freedom to manage  and configure his own validators and trasformer\n\nThe process is run by the [main.py](http://main.py) where the users can define any number of steps of Validation and trasformation at his preference. \n\nThe main idea is not only validate but provide something similar to a well structured template where is more difficult for the users to create a a data cleaning process with a messy code (i have seen tons of them). \n\nThe tool should be of interest to anyone who receives data from third parties on a recurring basis and needs a quick way to pinpoint where files are non-compliant with the expected process.\n\nI am not the best of programmers but with your feedback i can probably get better.\n\nWhat do you think about the overall architecture? is it well structured? probably i should manage in a better way the settings.\n\nWhat do you think of this idea? Any suggestion?\n\n", "date_utc": 1761948247.0, "title": "Personal Project feedback: Lightweight local tool for data validation and transformation", "upvote_ratio": 0.5, "score": 0, "url": "https://github.com/lorygaFLO/DataIngestionValidation"}, {"id": "1ok9dj2", "name": "t3_1ok9dj2", "content": "Basically the title!", "date_utc": 1761852883.0, "title": "Anyone using uv for package management instead of pip in their prod environment?", "upvote_ratio": 0.93, "score": 90, "url": "https://www.reddit.com/r/dataengineering/comments/1ok9dj2/anyone_using_uv_for_package_management_instead_of/"}, {"id": "1ok4yug", "name": "t3_1ok4yug", "content": "Every other submission is an ad disguised as a blog post or a self promotion post disguised as a question. \n\nI\u2019ll also add \u201cproduct research\u201d type posts from folks trying to build something. That\u2019s a cool endeavor but it has the same effect and just outsources their work. \n\nAny posts with outbound links should be auto-removed and we can have a dedicated self promotion thread once a week. \n\nIt\u2019s clear that data and data adjacent companies have honed in on this sub and it\u2019s clearly resulting in lower quality posts and interactions. \n\nEDIT: not even 5min after I posted this: https://www.reddit.com/r/dataengineering/s/R1kXLU6120\n\n- https://www.reddit.com/r/dataengineering/s/jIiud4fmLr\n\n- https://www.reddit.com/r/dataengineering/s/Q6SyVQ2mvl\n\n- https://www.reddit.com/r/dataengineering/s/vzOaqLw3Ue", "date_utc": 1761842987.0, "title": "Can we ban corporate \u201cblog\u201d posts and self promotion links", "upvote_ratio": 0.97, "score": 141, "url": "https://www.reddit.com/r/dataengineering/comments/1ok4yug/can_we_ban_corporate_blog_posts_and_self/"}, {"id": "1okyd7x", "name": "t3_1okyd7x", "content": "Rephrasing orig question\u2026does industry perception matter for future job prospects or is it purely the tech stack and the level of sophistication of the data engineering problems you\u2019re solving? E.g. currently only solving easy DE problems in a well respected industry - batch processing small data volumes vs potential job opp working with petabytes of streaming data for an industry that has a negative stigma?", "date_utc": 1761926166.0, "title": "Industry perception vs tech stack?", "upvote_ratio": 0.6, "score": 1, "url": "https://www.reddit.com/r/dataengineering/comments/1okyd7x/industry_perception_vs_tech_stack/"}, {"id": "1okvhpf", "name": "t3_1okvhpf", "content": "[https://github.com/telophasehq/tangent/](https://github.com/telophasehq/tangent/)\n\nHey y'all \u2013 There has been a lot of talk about stream processing with WebAssembly. Vector ditched it in 2021 because of performance and maintenance burden, but the wasmtime team has recently made major performance improvements since (with more exciting things to come like async!) and it felt like a good time to experiment to try it again.\n\nWe benchmarked a go WASM transform against a pure go pipeline + transform and saw WASM throughput within 10%.\n\nThe big win for us was not passing logs directly into wasm and instead giving it access to the host memory. More about that [here](https://docs.telophasehq.com/runtime)\n\nLet me know what you think!", "date_utc": 1761919459.0, "title": "Stream processing with WASM", "upvote_ratio": 0.67, "score": 1, "url": "https://www.reddit.com/r/dataengineering/comments/1okvhpf/stream_processing_with_wasm/"}, {"id": "1ojs19i", "name": "t3_1ojs19i", "content": "6 years of experience managing mainly spark streaming pipelines, more recently transitioned to Azure + Databricks. \n\nWhat\u2019s the temperature on the industry at the moment? Any resources you guys would recommend for preparing for my search?", "date_utc": 1761804319.0, "title": "Welp, just got laid off.", "upvote_ratio": 0.96, "score": 193, "url": "https://www.reddit.com/r/dataengineering/comments/1ojs19i/welp_just_got_laid_off/"}, {"id": "1ol4c9i", "name": "t3_1ol4c9i", "content": "Hola everyone,\n\nJust wondering how safe it is to paste table and column names from SQL code snippets into ChatGPT? \nIs that classed as sensitive data? I never share any raw data in chat or any company data, just parts of the code I'm not sure about or need explanation of. Quite new to the data world so just wondering if this is allowed.\nWe are allowed to use Copilot from Teams but I just don't find it as helpful as ChatGPT.\n\nThanks!", "date_utc": 1761939978.0, "title": "Pasting SQL code into Chat GPT", "upvote_ratio": 0.32, "score": 0, "url": "https://www.reddit.com/r/dataengineering/comments/1ol4c9i/pasting_sql_code_into_chat_gpt/"}, {"id": "1okdu9q", "name": "t3_1okdu9q", "content": "Hi everyone,\n\n\n\nI'm currently running a cluster with two servers for ClickHouse and two servers for ClickHouse Keeper. Given my setup (64 GB RAM, 32 vCPU cores per ClickHouse server \u2014 1 shard, 2 replicas), I'm able to process terabytes of data in a reasonable amount of time. However, I\u2019d like to reduce query times, and I\u2019m considering adding two more servers with the same specs to have 2 shards and 2 replicas.\n\n\n\nWould this significantly decrease query times? For context, I have terabytes of Parquet files stored on a NAS, which I\u2019ve connected to the ClickHouse cluster via NFS. I\u2019m fairly new to data engineering, so I\u2019m not entirely sure if this architecture is optimal, given that the data storage is decoupled from the query engine \\[any comments about how I'm handling the data and query engine will be more than welcome :) \\].", "date_utc": 1761863665.0, "title": "Adding shards to increase (speed up) query performance | Clickhouse.", "upvote_ratio": 0.73, "score": 5, "url": "https://www.reddit.com/r/dataengineering/comments/1okdu9q/adding_shards_to_increase_speed_up_query/"}, {"id": "1oknspw", "name": "t3_1oknspw", "content": "Hi everyone, \n\nI'm a new data engineer, and I'm looking for some feedback on an idea. I want to know if this is a real problem for others or if I'm just missing an existing tool.\n\n**My Questions:**\n\n1. When your data pipelines fail, are you happy with the error logs you get?\n2. Do you find yourself manually digging for the \"real\" root cause, even when logs tell you the *location* of the error?\n3. Does a good **open-source** tool for this already exist that I'm missing?\n\n\n\n**The Problem I'm Facing:**\n\n*When my pipelines fail (e.g., schema change), the error logs tell me where the error is (line 50) but not the context or the \"why.\" Manually finding the true root cause takes a lot of time and energy.*\n\n\n\n**The Idea:**\n\nI'm thinking of building an open-source tool that connects to your logs and, instead of just gibberish, gives you a **human-readable summary of the problem.**\n\n* **Instead of:** `KeyError: 'user_id' on line 50 of transform_script.py`\n* **It would say:** \"Root Cause: The pipeline failed because the 'user\\_id' column is missing from the 'source\\_table' input. This column was present in the last successful run.\"\n\nI'm building this for myself, but I was wondering if this is a common problem.\n\nIs this something you'd find useful and potentially contribute to?\n\nThanks!\n\n", "date_utc": 1761894086.0, "title": "Would you use an open-source tool that gave \"human-readable RCA\" for pipeline failures?", "upvote_ratio": 0.44, "score": 0, "url": "https://www.reddit.com/r/dataengineering/comments/1oknspw/would_you_use_an_opensource_tool_that_gave/"}, {"id": "1oksdno", "name": "t3_1oksdno", "content": "As data engineers, we sometimes work in big teams and other times handle everything ourselves. No matter the setup, it\u2019s important to understand the tools we use.\n\nWe rely on certain settings, libraries, and databases when building data pipelines with tools like Airflow or dbt. Making sure everything works the same on different computers can be hard.\n\nThat\u2019s where Docker helps.\n\nDocker lets us build clean, repeatable environments so our code works the same everywhere. With Docker, we can:\n\n* Avoid setup problems on different machines\n* Share the same setup with teammates\n* Run tools like dbt, Airflow, and Postgres easily\n* Test and debug without surprises\n\n**In this post, we cover:**\n\n* The difference between virtual machines and containers\n* What Docker is and how it works\n* Key parts like Dockerfile, images, and volumes\n* How Docker fits into our daily work\n* A quick look at Kubernetes\n* A hands-on project using\u00a0**dbt**\u00a0and\u00a0**PostgreSQL**\u00a0in Docker", "date_utc": 1761911296.0, "title": "Docker for Data Engineers", "upvote_ratio": 0.46, "score": 0, "url": "https://pipeline2insights.substack.com/p/docker-for-data-engineers"}, {"id": "1ojxzi9", "name": "t3_1ojxzi9", "content": "I built Cloudfloe, its an open-source query interface for Apache Iceberg tables using DuckDB. It's available both as a hosted service and for self-hosting.\n\n## What it does\n- Query Iceberg tables directly from S3/MinIO/R2 via web UI\n- Per-query Docker isolation with resource limits\n- Multi-user authentication (GitHub OAuth)\n- Works with REST catalogs only for now.\n\n## Why I built it\nAthena can be expensive for ad-hoc queries, setting up Trino or Flink is overkill for small teams, and I wanted something you could spin up in minutes. DuckDB + Iceberg is a great combo for analytical queries on data lakes.\n\n## Tech Stack\n- **Backend**: FastAPI + DuckDB (in ephemeral containers)\n- **Frontend**: Vanilla JS\n- **Caching**: Snapshot hash-based cache invalidation\n\n## Links\n- **Live Demo**: https://www.cloudfloe.com (GitHub login)\n- **GitHub**: https://github.com/gordonmurray/cloudfloe\n\n## Current Status\nWorking MVP with:\n- Multi-user query execution\n- CSV export of results\n- Query history and stats\n\nI'd love feedback on\n1. Would you use this vs something else?\n2. Any features that would make this more useful for you or your team?\n\nHappy to answer any questions", "date_utc": 1761826352.0, "title": "Built an open source query engine for Iceberg tables on S3. Feedback welcome", "upvote_ratio": 0.84, "score": 17, "url": "https://i.redd.it/0leznwvrp8yf1.png"}, {"id": "1okj885", "name": "t3_1okj885", "content": "Hello,\nI have been stuck in this project and definitely need help on how to do this. For reference, I am the only data guy in my whole company and there is nobody to help me. So, I work for a small company and it is non-profit. I have been given this task to build a dynamic dashboard. The dynamic dashboard must be able to track grants, and also provide demographic information. For instance, say we have a grant called \u2018grantX\u2019 worth of 50,000$. Using this 50,000 the company promised to provide medical screening for 10 houseless people. Of these, 50,000 the company used 10,000 to pay salaries and 5000 for gas, and other miscellaneous things, and the rest 35,000 to screen the houseless individuals. The dynamic dashboard should show this information. Mind you, there are a lot of grants and the data they collect for each grant is different. For example they collect name, age of the person served for one grant but they only get initials for the second grant. The company does not have a database and only uses office 365 environment. And most of the data is in sharepoint lists or excel spreadsheets. And the grant files are located in a dropbox. I am not sure how to work on this. I would like to use database and things as it would strengthen my portfolio. Please let me know how to work on this project. Thanks in advance!!\n", "date_utc": 1761878355.0, "title": "Need suggestions", "upvote_ratio": 0.5, "score": 0, "url": "https://www.reddit.com/r/dataengineering/comments/1okj885/need_suggestions/"}, {"id": "1ok4d97", "name": "t3_1ok4d97", "content": "I'm trying to build a standalone CRM app that retrieves JSON data (subscribers,  emails, DMs, chats, products, sales, events, etc.) from multiple REST API endpoints, normalizes the data, and loads it into a DuckDB database file on the user's computer.  Then, the user could ask natural language questions about the CRM data using the Claude AI desktop app or a similar tool, via a connection to the DuckDB MCP server. \n\nThese REST APIs require the user to be connected (using a session cookie or, in some cases, an API token) to the service and make potentially 1,000 to 100,000 API calls to retrieve all the necessary details.  To keep the data current, an automated scheduler is necessary. \n\n* I've built a Go program that performs the complete ETL and tested it, packaging it as a macOS application; however, maintaining database changes manually is complicated. I've reviewed various Go ORM packages that could add significant complexity to this project. \n* I've built a Python DLT library-based ETL script that does a better job normalizing the JSON objects into database tables, but I haven't found a way to package it yet into a standalone macOS app.   \n* I've built several Chrome extensions that can extract data and save it as CSV or JSON files, but I haven't figured out how to write DuckDB files directly from Chrome.   \n\nIdeally, the standalone app would be just a \"drag to Applications folder, click to open, and leave running,\"  but there are so many onboarding steps to ensure correct configuration,  MCP server setup, Claude MCP config setup, etc., that non-technical users will get confused after step #5. \n\nHas anybody here built a similar ETL product that can be distributed as a standalone app to non-technical users?  Is there like a \"Docker for consumers\" type of solution? ", "date_utc": 1761841653.0, "title": "How to build a standalone ETL app for non-technical users?", "upvote_ratio": 0.78, "score": 5, "url": "https://www.reddit.com/r/dataengineering/comments/1ok4d97/how_to_build_a_standalone_etl_app_for/"}, {"id": "1oj9q76", "name": "t3_1oj9q76", "content": "With over 15 years of experience leading large-scale data modernization and cloud migration initiatives, I\u2019ve noticed that despite handling major merger integrations and on-prem to cloud transformations, I\u2019m not getting calls for Data Engineering Manager roles at FAANG or $250K+ positions. What concrete steps should I take over the next year to strategically position myself and break into these top-tier opportunities. Any tools which can do ATS,AutoApply,rewrite,any reference cover letter or resum\\*.", "date_utc": 1761756500.0, "title": "What exactly does a Data Engineering Manager at a FAANG company or in a $250k+ role do day-to-day", "upvote_ratio": 0.93, "score": 236, "url": "https://www.reddit.com/r/dataengineering/comments/1oj9q76/what_exactly_does_a_data_engineering_manager_at_a/"}, {"id": "1okarpo", "name": "t3_1okarpo", "content": "(mods, if this comes through twice I apologize - my browser froze)\n\n  \nI'm looking at updating our data architecture with Coalesce, however I'm not sure if the cost will be viable long term.\n\nHas anyone successfully transitioned their work from Coalesce to DBT?  If so, what was involved in the process?", "date_utc": 1761856131.0, "title": "Transitioning from Coalesce.io to DBT", "upvote_ratio": 0.6, "score": 1, "url": "https://www.reddit.com/r/dataengineering/comments/1okarpo/transitioning_from_coalesceio_to_dbt/"}, {"id": "1okamkz", "name": "t3_1okamkz", "content": "My team uses Sql Server Management Studio, 2014 version. I am wondering if there's anyway to set an API connection between SSMS and say, HunSpot or Broadly? The alternatives are all manual and not scalable. I work remote using a VPN, so it has to be able to get past the firewall, it has to be able to run at night without my computer being on (I can use a Remote Desktop Connection,) and I'd like some sort of log or way to track errors. \n\nI just have no idea where to even start. Ideally, I'd rather build a solution, but if there's a proven tool, I am open to using that too!\n\nThank you so so much!!\n\n", "date_utc": 1761855791.0, "title": "Noob question", "upvote_ratio": 0.6, "score": 1, "url": "https://www.reddit.com/r/dataengineering/comments/1okamkz/noob_question/"}, {"id": "1ok2ygi", "name": "t3_1ok2ygi", "content": "[https://www.kaggle.com/datasets/adrianjuliusaluoch/global-food-prices](https://www.kaggle.com/datasets/adrianjuliusaluoch/global-food-prices)\n\nfor a portfolio project, i am building an end to end ETL script on AWS using this data. In the unit section,there are like 6 lakh types of units (kg,gm,L, 10 L , 10gm, random units ). I decided to drop all the units which are not related to L or KG and decided to standardise the remaining units. Could do the L columns as there were only like 10 types ( 1L, 10L, 10 ml,100ml etc.) usiing case when statements.\n\nBut the fields related to Kg and g have like 85 units. Should I pick the top 10 ones or just hardcode them all ( just one prompt in GPT after uploading the CSV)?\n\nHow are these scenarios handled in production? \n\nP.S: Doing this cus I need to create a price/ L , price/ KG column\nhttps://preview.redd.it/3e47xpugq9yf1.png?width=2176&format=png&auto=webp&s=bdc6b860c3afc67fd159921168c2f34495e6da06\n\n ", "date_utc": 1761838522.0, "title": "How would you handle this in production scenario?", "upvote_ratio": 0.7, "score": 4, "url": "https://www.reddit.com/r/dataengineering/comments/1ok2ygi/how_would_you_handle_this_in_production_scenario/"}, {"id": "1ok7thb", "name": "t3_1ok7thb", "content": "What is the feasibility of data preprocessing programs like [these](https://dbclean.dev/). My theory is that they only work for basic basic raw data from like user inputs, and I'm not sure how feasibility they would be in real-life.", "date_utc": 1761849346.0, "title": "Automated data cleaning programs feasibility?", "upvote_ratio": 0.4, "score": 0, "url": "https://www.reddit.com/r/dataengineering/comments/1ok7thb/automated_data_cleaning_programs_feasibility/"}, {"id": "1ojemmf", "name": "t3_1ojemmf", "content": "", "date_utc": 1761767356.0, "title": "Sail 0.4 Adds Native Apache Iceberg Support", "upvote_ratio": 0.93, "score": 54, "url": "https://github.com/lakehq/sail"}, {"id": "1ojqmre", "name": "t3_1ojqmre", "content": "I have been working as a data engineer at a large healthcare organization. Entire Data Engineering and Analytics team is remote. We had a new VP join in march and we are in the midst of modernizing our data stack. Moving from existing sql server on-prem to databricks and dbt. Everyone on my team has been handed work on learning and working on the new tech stack and doing migrations. During my 1:1 with my manager she promises that I will start on it soon but I am still stuck doing legacy work on the old systems. Pretty much everyone else on my team were referrals and have worked with either the VP or the manager and director(both from same old company) except me. My performance feedback has always been good and I have had exceeds expectations for the last 2 years. \n\nAt this point I want to move to another job and company but without experience in the new tech stack I cannot find jobs or clear interviews most of who want experience in the new data engineering tech stack. What do I do? ", "date_utc": 1761799215.0, "title": "Manager promises me new projects on tech stack but doesn\u2019t assign them to me. What should I do?", "upvote_ratio": 0.78, "score": 7, "url": "https://www.reddit.com/r/dataengineering/comments/1ojqmre/manager_promises_me_new_projects_on_tech_stack/"}, {"id": "1ojwfdk", "name": "t3_1ojwfdk", "content": "Hi all thanks in advance for the help.\n\nI have a flow that generates lots of data in a batched style h5 files where each batch contains the same datasets. So for example, I have for job A 100 batch files, each containing x datasets, are ordered which means the first batch has the first datapoints and the last contains the last - the order has important factor. Each batch contains y rows of data in every dataset where each dataset can have a different shape. The last file in the batch might contain less than y rows. Another job, job B can have less or more batch files, will still have x datasets but the split of rows per batch (the amount of data per batch) might be different than y. \n\n  \nI've tried a combo of kerchunk, zarr, and dask but keep on having issues with the different shapes, I've lost data between batches - only the first batch data is found or many shapes issues.\n\nWhat solution do you recommend for efficiently doing data analysis? I liked the idea of having the pre-process the data and then being able to query it, and use it efficiently.", "date_utc": 1761821474.0, "title": "Efficient data processing for batched h5 files", "upvote_ratio": 0.76, "score": 2, "url": "https://www.reddit.com/r/dataengineering/comments/1ojwfdk/efficient_data_processing_for_batched_h5_files/"}, {"id": "1ok0655", "name": "t3_1ok0655", "content": "Howdy y\u2019all.\n\nI am curious what other folks are doing to develop durable, reusable context across for AI agents their organizations. I\u2019m especially curious how folks are keeping agents/claude/cursor files up to date, what length is appropriate for such files, and what practices have helped with Dbt and Airflow models. If anyone has stories of what doesn\u2019t work, that would be super helpful too.\n\nContext: I am working with my org on AI best practices. I\u2019m currently focused on using 4 channels of context (eg https://open.substack.com/pub/evanvolgas/p/building-your-four-channel-context) and building a shared context library (eg https://open.substack.com/pub/evanvolgas/p/building-your-context-library). I have thoughts on how to maintain the library and some observations about the length of context files (despite internet \u201cbest practices\u201d of never more than 150-250 lines, I\u2019m finding some 500 line files to be worthwhile). I also have some observations about pain points of working with Dbt models, but may simply be doing it wrong. I\u2019m interested in understanding how folks are doing data engineering with agents, and what I can reuse/avoid. ", "date_utc": 1761832019.0, "title": "Developing durable context for coding agents", "upvote_ratio": 0.4, "score": 0, "url": "https://www.reddit.com/r/dataengineering/comments/1ok0655/developing_durable_context_for_coding_agents/"}, {"id": "1ojz8gn", "name": "t3_1ojz8gn", "content": "Hi everyone, I have a question regarding integration of Azure DevOps and VS Code for data engineering in Fabric.\n\nSay, I created notebook in the Fabric workspace and then synced to git (Azure DevOps). In Azure DevOps I go to Clone -> Open VS Code to develop notebook locally in VS Code. Now, all notebooks in Fabric and repo are stored as .py files. Normally, developers often prefer working interactively in\u00a0`.ipynb`\u00a0(Jupyter/VS Code), not in .py.\n\nAnd now I don't really know how to handle this scenario. In VS Code in Explorer pane I see all the Fabric items, including notebooks. I would like to develop this notebook which i see in the repo. However, I don't know I how to convert .py to .ipynb to locally develop my notebook. And after that how to convert .ipynb back to .py to push it to repo. I don't want to keep .ipynb and .py in remote repo. I just need the update, final .py version in repo. I can't right-click on .py file in repo and switch to .ipynb somehow. I can't do anyhting.\n\nSo the\u00a0**best-practice workflow**\u00a0for me (and I guess for other data engineers) is:\n\nWork interactively in\u00a0`.ipynb`\u00a0\u2192 convert/sync to\u00a0`.py`\u00a0\u2192 commit\u00a0`.py`\u00a0to Git.\n\nI read that some use jupytext library:\n\n`jupytext --set-formats ipynb,py:light notebooks/my_notebook.py`\n\nbut don't know if it's the common practice. What's the best approach? Could you share your experience?", "date_utc": 1761829705.0, "title": "How to develop Fabric notebooks interactively in local repo (Azure DevOPs + VS Code)?", "upvote_ratio": 0.6, "score": 1, "url": "https://www.reddit.com/r/dataengineering/comments/1ojz8gn/how_to_develop_fabric_notebooks_interactively_in/"}, {"id": "1oj8uaq", "name": "t3_1oj8uaq", "content": "We\u2019re currently evaluating modern data warehouse platforms and would love to get input from the data engineering community. Our team is primarily considering Microsoft Fabric and Snowflake, but we\u2019re open to insights based on real-world experiences.\n\nI\u2019ve come across mixed feedback about Microsoft Fabric, so if you\u2019ve used it and later transitioned to Snowflake (or vice versa), I\u2019d really appreciate hearing why and what you learned through that process.\n\nCurrent Context:\nWe don\u2019t yet have a mature data engineering team. Most analytics work is currently done by analysts using Excel and Power BI. Our goal is to move to a centralized, user-friendly platform that reduces data silos and empowers non-technical users who are comfortable with basic SQL.\n\nKey Platform Criteria:\n\t1.\tLow-code/no-code data ingestion\n\t2.\tSQL and low-code data transformation capabilities\n\t3.\tIntuitive, easy-to-use interface for analysts\n\t4.\tAbility to connect and ingest data from CRM, ERP, EAM, and API sources (preferably through low-code options)\n\t5.\tCentralized catalog, pipeline management, and data observability\n\t6.\tSeamless integration with Power BI, which is already our primary reporting tool\n\t7.\tScalable architecture \u2014 while most datasets are modest in size, some use cases may involve larger data volumes best handled through a data lake or exploratory environment\n", "date_utc": 1761754525.0, "title": "Snowflake vs MS fabric", "upvote_ratio": 0.98, "score": 46, "url": "https://www.reddit.com/r/dataengineering/comments/1oj8uaq/snowflake_vs_ms_fabric/"}, {"id": "1ojwuv4", "name": "t3_1ojwuv4", "content": "As we are considering to move to Microsoft Fabric I wanted to know which Microsoft Fabric partner provides comprehensive migration services. ", "date_utc": 1761822891.0, "title": "Best Microsoft fabric solution migration partners for enterprise companies", "upvote_ratio": 0.67, "score": 1, "url": "https://www.reddit.com/r/dataengineering/comments/1ojwuv4/best_microsoft_fabric_solution_migration_partners/"}, {"id": "1oj1itv", "name": "t3_1oj1itv", "content": "Hi everyone,\n\nTLDR: The team prefers SSIS over Airflow, I want to convince them to accept the switch as a long term goal.\n\nI am a Senior Data Engineer and I started at an SME earlier this year.\n\nPreviously I used a lot of Cloud Services, like AWS BatchJob for the ETL of an Kubernetes application, EC2 with airflow in docker-compose, developed API endpoints for a frontend Application using sqlalchemy at a big company, worked TDD in Scrum etc.\n\nHere, I found the current setup of the ETL pipeline to be a massive library of SSIS Packages basically getting data from an on prem ERP to a Reporting Model.\n\nThere are no tests, there are many small-small hacky ways inside SSIS to get what you want out of the data. The is no style guide or Review Process. In general it's lacking the usual oversight you would have in a \\*\\*searchable\\*\\* code project as well as the capability to run tests on the system and databases. git is not really used at all. Documentation is hardly maintained\n\nEverything is being worked on in the Visual Studio UI, which is buggy at best and simply crashing at worst (around twice per day).\n\nI work in a 2-person team and our Job it is to manage the SSIS ETL, Tabular Model and all PowerBI Reports throughout the company. The two of us are the entire reporting team.\n\nI replaced a long-time employee that has been in the company for around 15 years and didn't know any code and left minimal documentation.  \n\nGenerally my colleague (data scientist) does documentation only in his personal notebook which he shares sporadically on request.\n\n\n\nSince my start I introduced JIRA for our processes with a clear task board (it was a mess before) and bi-weekly sprints. Also a Wiki which I filled with hundreds of pages by now. I am currently introducing another tool, so at least we don't have to use buggy VS to manage the tabular model and can use git there as well.\n\nI am transforming all our PBI reports into .pbip files, so we can work with git there, too (We have like 100 reports).\n\nAlso, I built an entire prod Airflow Environment on an on-prem Windows server to be able to query APIs (not possible in SSIS) and run some basic statistical analysis (\"AI-capabilities\"). The Airflow repo is fully tested, has Exception Handling, feature and hotfix branches, dev, prod etc. and can be used locally as well as on remote.\n\nBut I am the only one currently maintaining it. My colleague does not want to change to Airflow, because \"the other one is working\". \n\nFact is, I am losing a lot of time managing SSIS in VS while getting a lower quality system.\n\nPlus, if we ever want to hire an additional colleague, he will probably face the same issues as I do (no docs, massive monolith, no search function, etc.) and will probably not get a good hire.\n\nMy boss is non-technical, so he is not of much help. We are also not in IT, so every time the SQL Server bugs, we need to run to the IT department to fix our ETL Job, which can take days.\n\nSo, how can I convince my colleague to eventually switch to Airflow?\n\nIt doesn't need to be today, but I want this to be a committed long term goal.\n\nWriting this, I feel I have committed so much to this company already and would really like to give them a chance (preference of industry and location)\n\nThank you all for reading, maybe you have some insight how to handle this. I would rather not quit on this, but might be my only option.", "date_utc": 1761735862.0, "title": "How to convince a switch from SSIS to python Airflow?", "upvote_ratio": 0.89, "score": 46, "url": "https://www.reddit.com/r/dataengineering/comments/1oj1itv/how_to_convince_a_switch_from_ssis_to_python/"}, {"id": "1oj2r42", "name": "t3_1oj2r42", "content": "Hi everyone, I searched the sub for some answers but couldn't find. My client has multiple CRMs and data sources with different key structures. Some rely on GUIDs and others use email or phone as primary key. We're in a pickle trying to reconcile records across systems.\n\nHow are you doing cross-system key management?\n\nLet me know if you need extra info, I'll try and source from my client.", "date_utc": 1761739709.0, "title": "How do you handle complex key matching between multiple systems?", "upvote_ratio": 0.94, "score": 27, "url": "https://www.reddit.com/r/dataengineering/comments/1oj2r42/how_do_you_handle_complex_key_matching_between/"}, {"id": "1ojci6y", "name": "t3_1ojci6y", "content": "Hello! I'm a new user here so I apologize if I'm doing anything incorrectly. I'm curious if anyone has any experience using Google Cloud's managed Airflow, which is called Composer V3. I'm a newer Airflow administrator at a small company, and I can't get this product to work for me whatsoever outside of running DAGs one by one. [I'm experiencing this same issue that's documented here](https://issuetracker.google.com/issues/447556586), but I can't seem to avoid it even when using other images. Additionally it seems that my jobs are constantly stuck in a queued state even though my settings should allow for them to run. What's odd is I have no problem running my DAGs on local containers.\n\nI guess what I'm trying to ask is: **Do you use Composer V3? Does it work for you?** Thank you!\n\nAgain thank you for going easy on my first post if I'm doing something wrong here :)", "date_utc": 1761762586.0, "title": "Airflow - GCP Composer V3", "upvote_ratio": 1.0, "score": 6, "url": "https://www.reddit.com/r/dataengineering/comments/1ojci6y/airflow_gcp_composer_v3/"}, {"id": "1oj5rw1", "name": "t3_1oj5rw1", "content": "Hi everyone;\n\nI need help from the community to classify my current position.\n\nI used to work for a small company for several years that was acquired recently by a large company, and the problem is that this large company does not know how to classify my position in their job profile grid. As a result, I find myself in a generic \u201cdata engineer\u201d category, and my package is assessed accordingly, even though data engineering is only a part of my job and my profile is much broader than that.\n\nBefore, when I was at my small company, my package evolved comfortably each year as I expanded my skills and we relied less and less on external subcontractors to manage the data aspects that I did not master well. Now, even though I continue to improve my skills and expertise, I find myself stuck with a fixed package because my new company is unaware of the breadth of my expertise...\n\nSpecifically, on my local industrial site, I do the following:\n\n* Manage all the data ingestion pipeline (cleaning, transformation, uploading to the database, management of feedback loops, automatic alerts, etc.)\n* Manage a very large Postgresql database (maintenance, backup, upgrades, performance optimization, etc.) with multiple schema and broad variaty of data embedded\n* Create new database structures (new schemas, tables, functions, etc.)\n* Build custom data exploitation platforms and implement various business visualisations\n* Use data for modelling/prediction with machine learning techniques\n* Manage our cloud services (access, upgrades, costs, etc.) and the cloud architectures required for data pipelines, database, BI,\u2026 (on AWS: EC2, lambda, SQS, RDS, dynamoDB, Sagemaker, Quicksight,\u2026)\n\nI added these functions over the years. I was originally hired to do just \"data analysis\" and industrial statistics (I'm basically a statistician and I have 25 years of experience in the industry), but I'm quite good at teaching myself new things. For example, I am able to read documentation and several books on a subject, practice, correct my errors and then apply this new knowledge in my work. I have always progressed like this: ir is my main professional strength and what my small company valued most.\n\nI do not claim to be as skilled an expert as a specialist in these various fields, but I am sufficiently proficient to have been able to handle everything fully autonomously for several years.\n\n\u00a0**What job profile do you think would cover all these skills?** \n\n**=>** *I would like to propose a job profile that would allow my new large company to benchmark my profile* and realize that my package can still evolve and that I am saving them a lot of money (external consultants or new hires, I also do a lot of custom development, which saves us from having to purchase professional software solutions).\n\nPersonally, I don't want to change companies because I know it will be difficult to find another position that is as broad and intellectually so interesting, especially since I don't claim to know EVERY aspect of these different professions (for example, I now know AWS very well because I work on this platform on a day to day basis, but I know very little about Azure or Google Cloud; I know machine learning fairly well, but I know very little about deep learning, which I have hardly ever practised, etc.). But it's really frustrating to feel like you're working really hard, tackling successfully technical challenges where our external consultants have proven to be less effective, spending hundreds of hours (often on my own time) to strengthen my skills without any recognition and package increase perspective...\n\nThanks for your help!\n\n\u00a0", "date_utc": 1761747498.0, "title": "What job profile do you think would cover all these skills?", "upvote_ratio": 0.62, "score": 3, "url": "https://www.reddit.com/r/dataengineering/comments/1oj5rw1/what_job_profile_do_you_think_would_cover_all/"}, {"id": "1oj2jm2", "name": "t3_1oj2jm2", "content": "Most data competitions today focus heavily on model accuracy or predictive analytics, but those challenges only capture a small part of what data engineers actually do. In real-world scenarios, the toughest problems are often about architecture, orchestration, data quality, and scalability rather than model performance.\n\nIf a competition were designed specifically for data engineers, what should it include?\n\n* Building an end-to-end ETL or ELT pipeline with real, messy, and changing data\n* Managing schema drift and handling incomplete or corrupted inputs\n* Optimizing transformations for cost, latency, and throughput\n* Implementing observability, alerting, and fault tolerance\n* Tracking lineage and ensuring reproducibility under changing requirements\n\nIt would be interesting to see how such challenges could be scored - perhaps balancing pipeline reliability, efficiency, and maintainability instead of prediction accuracy.\n\nHow would you design or evaluate a competition like this to make it both challenging and reflective of real data engineering work?", "date_utc": 1761739105.0, "title": "What would a realistic data engineering competition look like?", "upvote_ratio": 0.67, "score": 5, "url": "https://www.reddit.com/r/dataengineering/comments/1oj2jm2/what_would_a_realistic_data_engineering/"}, {"id": "1oibg8j", "name": "t3_1oibg8j", "content": "Delayed post and many won't care, but I love it and have been using it for a while. Would recommend trying", "date_utc": 1761663037.0, "title": "DataGrip Is Now Free for Non-Commercial Use", "upvote_ratio": 0.97, "score": 241, "url": "https://blog.jetbrains.com/datagrip/2025/10/01/datagrip-is-now-free-for-non-commercial-use/"}, {"id": "1ojjmug", "name": "t3_1ojjmug", "content": "Hello there,\n\nI've been working in the space for 3 years now, doing a lot of data modeling and pipeline building both on-prem and cloud. I really love data engineering and I was thinking of researching deeper into a topic in the field for my masters thesis.\n\nI'd love to hear some suggestions, anything that has came up in your mind where you did not find a clear answer or just gaps in the data engineering knowledge base that could be researched. \n\nI was thinking in the realms of optimization techniques, maybe comparing different data models, file formats or processing engines and benchmarking them but it doesn't feel novel enough just yet.\n\nIf you have any pointers or ideas I'd really appreciate it!", "date_utc": 1761779240.0, "title": "Master thesis topic suggestions", "upvote_ratio": 0.38, "score": 0, "url": "https://www.reddit.com/r/dataengineering/comments/1ojjmug/master_thesis_topic_suggestions/"}, {"id": "1oj7pv5", "name": "t3_1oj7pv5", "content": "I'm a trainee in IT at an NBFC, and my reporting manager( not my teams chief manager) is exploiting me big time. \nI'm doing overtime every day, sometimes till midnight. He dumps his work on me and then takes all the credit \u2013 classic toxic boss moves. But it's killing my mental peace as I am sacrificing all my time for his work. I talked to the IT head about switching teams, but he wants me to stick it out for 6 months. He doesn't get it\u2019s the manager, not the team, that\u2019s the issue.\nI am thinking of pushing again for a team change and tell him the truth or just leave the company . I need some serious advice! Please help!", "date_utc": 1761752028.0, "title": "Drowning in toxicity: Need advice ASAP!", "upvote_ratio": 0.56, "score": 1, "url": "https://www.reddit.com/r/dataengineering/comments/1oj7pv5/drowning_in_toxicity_need_advice_asap/"}, {"id": "1oj3leh", "name": "t3_1oj3leh", "content": "# \n\nHey everyone,\n\nI'm working solo on the data infrastructure at our manufacturing facility, and I'm hitting some roadblocks I'd like to get your thoughts on.\n\n**The Setup**\n\nWe use an Oracle-based ERP system that's pretty restrictive. I've filtered their fact tables down to show only active jobs on our floor, and most of our reporting centers around that data. I built a Go ETL program that pulls data from Oracle and pushes it to Postgres every hour (currently moving about 1k rows per pull). My next step was to use dbt to build out proper dimensions and new fact tables.\n\n**Why the Migration?**\n\nThe company moved their on-premise Oracle database to Azure, which has tanked our Power BI and Excel report performance. On top of that, the database account they gave us for reporting doesn't have access to materialized views, can't create indexes, or schedule anything. We're basically locked into querying views-on-top-of-views with no optimization options.\n\n**Where I'm Stuck**\n\nI've hit a few walls that are keeping me from moving forward:\n\n1. **Development environment**: The dbt plugin is deprecated in IntelliJ, and the VS Code version is pretty rough. SqlMesh doesn't really solve this either. What tools do you all use for writing this kind of code?\n2. **Historical tracking**: The ERP uses object versions and business keys built by concatenating two fields with a `^` separator. This makes incremental syncing really difficult. I'm not sure how to handle this cleanly.\n3. **Dimension table design**: Since I'm filtering to only active jobs to keep row volume down, my dimension tables grow and shrink. That means I have to truncate them on each run instead of maintaining a proper slowly changing dimension. I know it's not ideal, but I'm not sure what the better approach would be here.\n\nYour advice would be appreicated. I dont have anyone in my company to talk to about this and I want to make good decisions to help my company move from the stoneage into something modern.\n\nThanks! ", "date_utc": 1761742066.0, "title": "Workaround Architecture: Postgres ETL for Oracle ERP with Limited Access(What is acceptable)", "upvote_ratio": 0.81, "score": 3, "url": "https://www.reddit.com/r/dataengineering/comments/1oj3leh/workaround_architecture_postgres_etl_for_oracle/"}, {"id": "1ojcrji", "name": "t3_1ojcrji", "content": "Principal Data Architect - this is the title my director and I originally threw out there, but I'd like some opinions from any of you. I've heard architect is a dying title and don't want to back myself into a corner for future opportunities. We also floated Principal BI Engineer or Principal Data Engineer, but I hardly feel that implementing Stitch and Fivetran for ELT justifies a data engineer title and don't feel my background would line up with that for future opportunities. It may be a moot point if I ever try going for a Director of Analytics role in the future, but not sure if that will ever happen as I've never had direct reports and don't like office politics. I do enjoy being an individual contributor, data governance, and working directly with stakeholders to solve their unique needs on data and reporting. Just trying to better understand what I should call myself, what I should focus on, and where I should try to go to next.\n\nBackground and context below.\n\nI have 14 years experience behind me, with previous roles as Reporting Analyst, Senior Pricing Analyst, Business Analytics Manager, and currently Senior Data Analytics Manager. With leadership and personnel changes in my current company and team, after 3 years of being here my responsibilities have shifted and leadership is open to changing my title, but I'm not sure what direction I should take it.\n\nBack in college I set out to be a Mechanical Engineer; I loved physics, but was failing Calc 2 and panicked and regrettably changed my major to their Business program. When I started my career, I took to Excel and VBA macros naturally because my physics brain just likes to build things. Then someone taught me the first 3 lines of SQL and everything took off from there.\n\nIn my former role as Business Analytics Manager I was an analytics team of 1 for 4 years where I rebuilt everything from the ground. Implemented Stitch for ELT, built standardized data models with materialized views in Redshift, and built dashboards in Periscope (R.I.P.).\n\nI got burnt out as a team of 1 and moved to my current company so I can be a part of a larger team, at first I was hired into the Marketing Department just focusing on standardizing data models and reporting under Marketing, but soon after started supporting Finance and Merchandising as well. We had a Senior Data Architect I worked closely with, as well as a Data Scientist; both of these individuals left and were never backfilled so I'm back to where I started managing all of it, although we've dropped all projects the data scientist was running. I now fall under IT instead of Marketing, and I report to a Director of Analytics who reports to the CTO. We also have 3 offshore analyst resources for dashboard building and ad hoc requests, but they primarily focus on website analytics with GA4.\n\nI'm currently in the process of onboarding Fivetran for the bulk of our data going into BigQuery, and we just signed on with Tableau to consolidate dashboards and various spreadsheets. I will be rebuilding views to utilize the new data pipelines and rebuilding existing dashboards, much like my last company.\n\nWhat I love most about my work is writing SQL, building complex but clean views to normalize/standardize data to make it intuitive for downstream reporting and dashboard building. I loved building dashboards in Periscope because it was 100% SQL driven, most other BI tools I've found limiting by comparison. I know some python, but working in that environment doesn't come naturally to me and I'm way more comfortable writing everything directly in SQL, building dynamic dashboards, and piping my data into spreadsheets in a format the stakeholders like.\n\nI've never truly considered myself an 'analyst' as I don't feel comfortable providing analysis and recommendations, my brain thinks of a thousand different variables as to why that assumption could be misleading. Instead, I like working with the people asking the questions and understanding the nuances of the data being asked about in order to write targeted queries, and let those subject matter experts derive their own conclusions. And while I've always been intrigued by the deeper complexities of data engineering functions and capabilities, there are an endless number of tools and platforms out there that I haven't been exposed to and know little about so I'd feel like a fraud trying to call myself an engineer. At the end of the day I work in data with a mechanical engineering brain rather than a traditional software engineering type, and still struggle to understand what path I should be taking in the future.", "date_utc": 1761763171.0, "title": "Need advice on choosing a new title for my role", "upvote_ratio": 0.57, "score": 1, "url": "https://www.reddit.com/r/dataengineering/comments/1ojcrji/need_advice_on_choosing_a_new_title_for_my_role/"}, {"id": "1oigp5k", "name": "t3_1oigp5k", "content": "I've been following data contracts closely, and I wanted to share some of my research into real-world implementations I have come across over the past few years, along with the person who was part of the implementation.\n\n[**Hoyt Emerson**](https://www.linkedin.com/in/hoytemerson/) **@ Robotics Startup -** [**Proposing and Implementing Data Contracts with Your Team**](https://thefulldatastack.substack.com/p/proposing-and-implementing-data-contracts)\n\nImplemented data contracts not only at a robotics company, but went so far upstream that they were placed on data generated at the hardware level! This article also goes into the socio-technical challenges of implementation.  \n\n\n[**Zakariah Siyaji**](https://www.linkedin.com/in/zakariah-siyaji/) **@ Glassdoor -** [**Data Quality at Petabyte Scale: Building Trust in the Data Lifecycle**](https://medium.com/glassdoor-engineering/data-quality-at-petabyte-scale-building-trust-in-the-data-lifecycle-7052361307a4)\n\nImplemented data contracts at the code level using static code analysis to detect changes to event code, data contracts to enforce expectations, the write-audit-publish pattern to quarantine bad data, and LLMs for business context.  \n  \n[**Sergio Couto Catoira**](https://www.linkedin.com/in/sergiocoutocatoira/) **@ Adevinta Spain -** [**Creating source-aligned data products in Adevinta Spain**](https://adevinta.com/techblog/creating-source-aligned-data-products-in-adevinta-spain/)\n\nImplemented data contracts on segment events, but what's really cool is their emphasis on automation for data contract creation and deployment to lower the barrier to onboarding. This automated a substantial amount of the manual work they were doing for GDPR compliance.  \n  \n[**Andrew Jones**](https://www.linkedin.com/in/andrewrhysjones/) **@ GoCardless -** [**Implementing Data Contracts at GoCardless**](https://medium.com/gocardless-tech/implementing-data-contracts-at-gocardless-3b5c49074d13)[](https://medium.com/@andrew-jones?source=post_page---byline--3b5c49074d13---------------------------------------)\n\nThis is one of the OG implementations, when it was actually very much theoretical. Andrew Jones also wrote an entire book on data contracts ([https://data-contracts.com](https://data-contracts.com/))!  \n  \n[**Jean-Georges Perrin**](https://www.linkedin.com/in/jgperrin/) **@ PayPal -** [**How Data Mesh, Data Contracts and Data Access interact at PayPal**](https://youtu.be/cL-qMGqg4HA?si=6jrDUaTmH_i7gmMi)\n\nAnother OG in the data contract space, an early adopter of data contracts, who also made the contract spec at PayPal open source! This contract spec is now under the Linux Foundation ([bitol.io](http://bitol.io/))! I was able to chat with Jean-Georges at a conference earlier this year and it's really cool how he set up an interdisciplinary group to oversee the open source project at Linux.\n\n\\----\n\n[**GitHub Repo - Implementing Data Contracts**](https://github.com/data-contract-book/chapter-7-implementing-data-contracts)\n\nFinally, something that kept  coming up in my research was \"how do I get started?\" So I built an entire sandbox environment that you can run in the browser and will teach you how to implement data contracts fully with open source tools. Completely free and no signups required; just an open GitHub repo.", "date_utc": 1761674693.0, "title": "Five Real-World Implementations of Data Contracts", "upvote_ratio": 0.97, "score": 61, "url": "https://www.reddit.com/r/dataengineering/comments/1oigp5k/five_realworld_implementations_of_data_contracts/"}, {"id": "1oj088o", "name": "t3_1oj088o", "content": "I\u2019m trying to host Apache Airflow on ECS, but this time in a more structured setup. Our project is containerized into multiple Docker images for different environments and releases, and I\u2019m looking for best practices or references from anyone who\u2019s done something similar.\n\nI\u2019ve done this before in a sandbox AWS account, where I:\n\t\u2022\tCreated my own VPC\n\t\u2022\tSet up ECS services for the webserver and scheduler\n\t\u2022\tAttached the webserver to a public ALB, IP-restricted via security groups\n\nThat setup worked fine for experimentation, but now I\u2019m moving toward a more production-ready architecture.\nHas anyone here deployed Airflow on ECS with multiple Docker images (say, dev/stage/prod) in a clean and maintainable way?\nCurious how you handled:\n\t\u2022\tService segregation per environment (separate clusters vs same cluster with namespaces)\n\t\u2022\tImage versioning and tagging\n\t\u2022\tNetworking setup (VPCs, subnets, ALBs)\n\t\u2022\tManaging Airflow metadata DB and logs\n\nWould really appreciate any advice, architecture patterns, or gotchas from your experience.", "date_utc": 1761731361.0, "title": "Anyone hosting Apache Airflow on AWS ECS with multiple Docker images for different environments?", "upvote_ratio": 0.81, "score": 3, "url": "https://www.reddit.com/r/dataengineering/comments/1oj088o/anyone_hosting_apache_airflow_on_aws_ecs_with/"}, {"id": "1oishqf", "name": "t3_1oishqf", "content": "Struggling with where to start.\n\nWould love to learn more about methods you are using and benefits / shortcomings.\n\nHow long does it take and how accurate?", "date_utc": 1761703516.0, "title": "How are you matching ambiguous mentions to the same entities across datasets?", "upvote_ratio": 0.89, "score": 12, "url": "https://www.reddit.com/r/dataengineering/comments/1oishqf/how_are_you_matching_ambiguous_mentions_to_the/"}, {"id": "1oieor6", "name": "t3_1oieor6", "content": "Hi, I'm a senior analytics engineer - currently in Canada (but a US/Canada dual citizen, so looking at North America in general).\n\nI'm noticing more and more that in both my company, and many of my peers' companies, data roles that were once located in the US are being moved to low-cost (of employment) regions. These are roles that were once US-based, and are now being reallocated to low cost regions.\n\nMy company's CEO has even quietly set a target of having a minimum of 35% of the jobs in each department located in a low-cost region of the world, and is aggressively pushing to move more and more positions to low cost regions through layoffs, restructuring, and natural turnover/attrition. I've heard from several peers that their companies seem to be quietly reallocating many of their positions, as well, and it's leaving me uncertain about the future of this industry in a high-cost region like North America.\n\nThe macro-economic research does still seem to suggest that technical data roles (like a DE or analytics engineer) are still stable and projected to stay in-demand in North America, but \"from the ground\" I'm only seeing reallocations to low-cost regions en mass.\n\nCurious if anybody else is noticing this at their company, in their networks, on their feeds, etc.?\n\nI'm considering the long term feasibility of staying in this profession as executives, boards, and PE owners just get greedier and greedier, so just wanting to see what others are observing in the market.\n\nEdit: removed my quick off the cuff list of low cost countries because debating the definition and criteria for \u201clow cost\u201d wasn\u2019t really the point lol ", "date_utc": 1761670257.0, "title": "Are DE jobs moving?", "upvote_ratio": 0.89, "score": 65, "url": "https://www.reddit.com/r/dataengineering/comments/1oieor6/are_de_jobs_moving/"}, {"id": "1oip989", "name": "t3_1oip989", "content": "At my company, we\u2019ve got a main server that receives all the data from our ERP system and stores it in an Oracle database.  \nOn top of that, we have a separate PostgreSQL database that we use only for Power BI reports.\n\nWe built our whole ETL process in Pentaho. It reads from Oracle, writes to Postgres, and we run daily jobs to keep everything updated.\n\nEach Power BI dashboard basically has its own dedicated set of tables in Oracle, which are then moved to Postgres.  \nIt works, but I\u2019m starting to worry about how this will scale over time since every new dashboard means more tables, more ETL jobs, and more maintenance in general.\n\nIt all runs fine for now, but I keep wondering if this is really the best or most efficient setup. I don\u2019t have much visibility into how other teams handle this, so I\u2019m curious:  \nhow do you manage your ETL and reporting pipelines?  \nWhat tools, workflows, or best practices have worked well for you?", "date_utc": 1761694727.0, "title": "How do you guys handle ETL and reporting pipelines between production and BI environments?", "upvote_ratio": 0.93, "score": 18, "url": "https://www.reddit.com/r/dataengineering/comments/1oip989/how_do_you_guys_handle_etl_and_reporting/"}, {"id": "1oj7c1j", "name": "t3_1oj7c1j", "content": "Sorry for the very basic beginner question.\nI have this on my computer at work because I do analysis (usally GIS and excel), but I'm trying to expand my knowledge of SQL and filter data using this program. I see that people say that I need the developer addition, but I'm wondering if I can use the regular one because they don't give me the other one and I'm not allowed to download the dev one without permission from an admin. Seems people online say it's not possible to practice with the nondev one?\n\nWhen I log on I try to create a local server but I want to make sure I'm not going to ruin anything in prod. My boss doesn't use it but wants me to learn how so I can use it to clean up data. Do you have any tips? \n\nThanks!", "date_utc": 1761751158.0, "title": "Is it possible to create a local server if I have Microsoft SSMS 20 installed?", "upvote_ratio": 0.57, "score": 1, "url": "https://www.reddit.com/r/dataengineering/comments/1oj7c1j/is_it_possible_to_create_a_local_server_if_i_have/"}, {"id": "1ojcqsv", "name": "t3_1ojcqsv", "content": "Hello guys,\n\nI was assigned this project in which I have to develop a global finance data model to consolidate data in a company that has different sources with different schemas, table logic, etc,in a structured way, in databricks. \n\nIn the meantime, the finance business data team hired someone to take their current solution (excels and powerbi) and automate it. This person ended up  building a whole etl process in fabric for this with AI (no versioning, just single-cell notebooks, pipelines, data flows) \nSince they delivered fast, business sees no use in our model/framework.\n\nI'm kind of having a crisis because business just sees the final reports and how fast it is from excel data to dashboard now. And this has led to them not trusting me or my team to deliver and wanting to do eveything themselves with their guy. \n\nAs anyone gone through something similar and what did you do to gain trust back, or is that even worth it in this case? \n", "date_utc": 1761763122.0, "title": "Fast AI development vs Structured slow delivery", "upvote_ratio": 0.42, "score": 0, "url": "https://www.reddit.com/r/dataengineering/comments/1ojcqsv/fast_ai_development_vs_structured_slow_delivery/"}, {"id": "1oj5ilf", "name": "t3_1oj5ilf", "content": "I would like to share a story about my current experience and the difficulties I am encountering\u2014or rather, about how my expectations are different from reality.\ufeff\n\nI am a data engineer who has been working in the field of data processing for 25 years now. I believe I have a certain familiarity with these topics, and I have noticed the lack of some tools that would have saved me a lot of time.\ufeff\n\nAnd that\u2019s how I created a tool (but that\u2019s not the point) that essentially, by taking JSON or XML as input, automatically transforms them into a relational database. It also adapts automatically to changes, always preserving backward compatibility with previously loaded data.\ufeff\n\nAt the moment, the tool works with databases like PostgreSQL, Snowflake, and Oracle. In the future, I hope to support more (but actually, it could work for all databases, considering that one of these three could be used as a data source after running the tool).\ufeff\n\nLet me get to the point: in my mind, I thought this tool could be a breakthrough, and a similar product (which I won\u2019t mention here to avoid giving it promotion) actually received an award from Snowflake in 2025 because it was considered very innovative. Basically, that tool does much of what mine does, but mine still has some better features.\ufeff\n\nNowadays, JSON data is everywhere, and that has been the \u201cfuel\u201d that kept me going while developing it.\n\nA bit against the trend, my tool does not use AI\u2014maybe this is penalizing it, but I want to be genuine and not hide behind this topic just to get more attention. It is also very respectful of privacy, making it suitable for those dealing with personal or sensitive data (basically, part of the process runs on the customer\u2019s premises, and the result can be sent out to get the final product ready to be executed on their own database).\ufeff\n\nThe ultimate idea is to create a SaaS so that anyone who needs it can access the tool. At the moment, however, I don't have the financial resources to cover the costs of productization, legal fees, patents, and all the necessary expenses. That\u2019s why I thought about offering myself as a consultant providing the transformation service, so that once I receive the input data, clients can start viewing their information in a relational database format\n\nThe difficulties I am facing are surprising me. There are people who consider themselves experts and say that this tool doesn't make sense, preferring to write code themselves to extract the necessary information by reading the data directly from JSON\u2014using, in my opinion, syntaxes that are not easy even for those who know only SQL.\ufeff\n\nI am now wondering if there truly are people out there with expert knowledge of these topics (which are definitely niche), because I believe that not having to write a single line of code, being able to get a relational database ready for querying with simple queries, tables that are automatically linked in the same way (parent/child fields), and being able to create reports and dashboards in just a few minutes, is truly an added value that today can be found in only a few tools.\ufeff\n\nI\u2019ll conclude by saying that the estimated minimum ROI, in terms of time\u2014and therefore money\u2014saved for a developer is at least 10x.\n\nI am so confident in my solution that I would also love to hear the opinion of those who face this type of situation daily.\n\nThank you to everyone who has read this post and is willing to share their thoughts.\n\n  \n", "date_utc": 1761746893.0, "title": "The reality is different \u2013 From JSON/XML to relational DB automatically", "upvote_ratio": 0.5, "score": 0, "url": "https://www.reddit.com/r/dataengineering/comments/1oj5ilf/the_reality_is_different_from_jsonxml_to/"}, {"id": "1oj8cza", "name": "t3_1oj8cza", "content": "Just pushed live GenOps AI \u2192 [https://github.com/KoshiHQ/GenOps-AI](https://github.com/KoshiHQ/GenOps-AI)\n\nBuilt on OpenTelemetry, it\u2019s an open-source runtime governance framework for AI that standardizes cost, policy, and compliance telemetry across workloads, both internally (projects, teams) and externally (customers, features).\n\nFeedback welcome, especially from folks working on AI observability, FinOps, or runtime governance.  \n  \nContributions to the open spec are also welcome.", "date_utc": 1761753471.0, "title": "Open-source: GenOps AI \u2014 LLM runtime governance built on OpenTelemetry", "upvote_ratio": 0.43, "score": 0, "url": "https://www.reddit.com/r/dataengineering/comments/1oj8cza/opensource_genops_ai_llm_runtime_governance_built/"}, {"id": "1oinvj4", "name": "t3_1oinvj4", "content": "Starting a new role soon at a company that uses Redshift. I have a good few years of Redshift experience, but my most recent role has been BigQuery-focused, so I'm a little out-of-the-loop as to how Redshift has developed as a product over the past \\~2 years.\n\nAny notable changes I should be aware of? I've scanned the release notes but it's hard to tell which features are actually useful vs fluff.  \n", "date_utc": 1761691286.0, "title": "Moving back to Redshift after 2 years using BQ. What's changed?", "upvote_ratio": 0.83, "score": 11, "url": "https://www.reddit.com/r/dataengineering/comments/1oinvj4/moving_back_to_redshift_after_2_years_using_bq/"}, {"id": "1oistvd", "name": "t3_1oistvd", "content": "Do you review how your team use git merge and push to the remote? \n\nDo you discuss the versioning of your data pipeline and models?\n\nWhat interesting findings you usually find from such review?", "date_utc": 1761704492.0, "title": "How you review and discuss your codebase monthly and quarterly?", "upvote_ratio": 0.71, "score": 4, "url": "https://www.reddit.com/r/dataengineering/comments/1oistvd/how_you_review_and_discuss_your_codebase_monthly/"}, {"id": "1oip8s3", "name": "t3_1oip8s3", "content": "seeking a consulting firm reference to provide platform recommendations aligned with our current and future analytics needs. \n\nMuch of our existing analytics and reporting is performed using Excel and Power BI, and we\u2019re looking to transition to a modern, cloud-based data platform such as Microsoft Fabric or Snowflake.\n\nWe expect the selected vendor to conduct discovery sessions with key power user groups to understand existing reporting workflows and pain points, and then recommend a scalable platform that meets future needs with minimal operational overhead (we realize this might be like finding a unicorn!).\n\nIn addition to developing the platform strategy, we would also like the vendor to implement a small pilot use case to demonstrate the working solution and platform capabilities in practice.\n\nIf you\u2019ve worked with any vendors experienced in Snowflake or Microsoft Fabric and would highly recommend them, please share their names or contact details.\n\n", "date_utc": 1761694695.0, "title": "Data warehouse modernization- services provider", "upvote_ratio": 0.76, "score": 4, "url": "https://www.reddit.com/r/dataengineering/comments/1oip8s3/data_warehouse_modernization_services_provider/"}, {"id": "1oj85zo", "name": "t3_1oj85zo", "content": "Hey folks, I talk to a lot of data teams every week and something I am noticing is how, if a few months ago everyone was shouting \"LLM BAD\" now everyone is using copilot, cursor, etc and is on a spectrum between raving about their LLM superpowers or just delivering faster with less effort.\n\nAt the same time everyone seems also tired of what this may mean mid and long term for our jobs, about the dead internet, llm slop and diminishing of meaning.\n\nHow do you feel? am I in a bubble?", "date_utc": 1761753040.0, "title": "Did we stop collectively hating LLMs?", "upvote_ratio": 0.42, "score": 0, "url": "https://www.reddit.com/r/dataengineering/comments/1oj85zo/did_we_stop_collectively_hating_llms/"}, {"id": "1oic0e1", "name": "t3_1oic0e1", "content": "I've been benchmarking ClickHouse 25.9.4.58 against Exasol on TPC-H workloads and am looking for specific guidance to improve ClickHouse's performance. Despite enabling statistics and applying query-specific rewrites, I'm seeing ClickHouse perform 4-10x slower than Exasol depending on scale factor. If you've tuned ClickHouse for TPC-H-style workloads at these scales on r5d.\\* instances (or similar) and can share concrete settings, join rewrites, or schema choices that move the needle on **Q04/Q08/Q09/Q18/Q19/Q21** in particular, I'd appreciate detailed pointers.\n\nSpecifically, I'm looking for advice on:\n\n**1. Join strategy and memory**\n\n* Recommended settings for large, many-to-many joins on TPC-H shapes (e.g., guidance on `join_algorithm` choices and thresholds for spilling vs in-memory)\n* Practical values for `max_bytes_in_join`, `max_rows_in_join`, `max_bytes_before_external_*` to reduce spill/regressions on Q04/Q18/Q19/Q21\n* Whether using grace hash or partial/merge join strategies is advisable on SF30+ when relations don't fit comfortably in RAM\n\n**2. Optimizer + statistics**\n\n* Which statistics materially influence join reordering and predicate pushdown for TPC-H-like SQL (and how to scope them: which tables/columns, histograms, sampling granularity)\n* Any caveats where cost-based changes often harm (Q04/Q14 patterns), and how to constrain the optimizer to avoid those plans\n\n**3. Query-level idioms**\n\n* Preferred ClickHouse-native patterns for EXISTS/NOT EXISTS (especially Q21) that avoid full scans/aggregations while keeping memory under control\n* When to prefer IN/SEMI/ANTI joins vs INNER/LEFT; reliable anti-join idioms that plan well in 25.9\n* Safe uses of PREWHERE, `optimize_move_to_prewhere`, and read-in-order for these queries\n\n**4. Table design details that actually matter here**\n\n* Any proven primary key / partitioning / LowCardinality patterns for TPC-H `lineitem`/`orders`/`part*` tables that the optimizer benefits from in 25.9\n\n**So far I've been getting the following results**\n\n**Test environment**\n\n* **Systems under test:** Exasol 2025.1.0 and ClickHouse 25.9.4.58\n* **Hardware:** AWS r5d.4xlarge (16 vCPU, 124 GB RAM, eu-west-1)\n* **Methodology:** One warmup, 7 measured runs, reporting medians\n* **Data:** Generated with dbgen, CSV input\n\n**Full reports**\n\n* **SF1:** Exasol vs ClickHouse (baseline, stats-enabled, query-tuned) [https://exasol.github.io/benchkit/exa\\_vs\\_ch\\_1g/reports/2-results/REPORT.html](https://exasol.github.io/benchkit/exa_vs_ch_1g/reports/2-results/REPORT.html)\n* **SF10:** Exasol vs ClickHouse (same three variants) [https://exasol.github.io/benchkit/exa\\_vs\\_ch\\_10g/reports/2-results/REPORT.html](https://exasol.github.io/benchkit/exa_vs_ch_10g/reports/2-results/REPORT.html)\n* **SF30:** Exasol vs ClickHouse (same three variants) [https://exasol.github.io/benchkit/exa\\_vs\\_ch\\_30g/reports/2-results/REPORT.html](https://exasol.github.io/benchkit/exa_vs_ch_30g/reports/2-results/REPORT.html)\n\n**Headline results (medians; lower is better)**\n\n* **SF1 system medians:** Exasol 19.9ms; ClickHouse 86.2ms; ClickHouse\\_stat 89.4ms; ClickHouse\\_tuned 91.8ms\n* **SF10 system medians:** Exasol 63.6ms; ClickHouse\\_stat 462.1ms; ClickHouse 540.7ms; ClickHouse\\_tuned 553.0ms\n* **SF30 system medians:** Exasol 165.9ms; ClickHouse 1608.8ms; ClickHouse\\_tuned 1615.2ms; ClickHouse\\_stat 1659.3ms\n\n**Where query tuning helped**\n\n**Q21** (the slowest for ClickHouse in my baseline):\n\n* SF1: 552.6ms -> 289.2ms (tuned); Exasol 22.5ms\n* SF10: 6315.8ms -> 3001.6ms (tuned); Exasol 106.7ms\n* SF30: 20869.6ms -> 9568.8ms (tuned); Exasol 261.9ms\n\n**Where statistics helped (notably on some joins)**\n\n**Q08:**\n\n* SF1: 146.2ms (baseline) -> 88.4ms (stats); Exasol 17.6ms\n* SF10: 1629.4ms -> 353.7ms; Exasol 30.7ms\n* SF30: 5646.5ms -> 1113.6ms; Exasol 60.7ms\n\n**Q09** also improved with statistics at SF10/SF30, but remains well above Exasol.\n\n**Where tuning/statistics hurt or didn't help**\n\n* **Q04:** tuning made it much slower - SF10 411.7ms -> 1179.4ms; SF30 1410.4ms -> 4707.0ms\n* **Q18:** tuning regressed - SF10 719.7ms -> 1941.1ms; SF30 2556.2ms -> 6865.3ms\n* **Q19:** tuning regressed - SF10 547.8ms -> 1362.1ms; SF30 1618.7ms -> 3895.4ms\n* **Q20:** tuning regressed - SF10 114.0ms -> 335.4ms; SF30 217.2ms -> 847.9ms\n* **Q21** with statistics alone barely moved vs baseline (still multi-second to multi-tens-of-seconds at SF10/SF30)\n\n**Queries near parity or ClickHouse wins**\n\nQ15/Q16/Q20 occasionally approach parity or win by a small margin depending on scale/variant, but they don't change overall standings. Examples:\n\n* SF10 Q16: 192.7ms (ClickHouse) vs 222.7ms (Exasol)\n* SF30 Q20: 217.2ms (ClickHouse) vs 228.7ms (Exasol)\n\n**ClickHouse variants and configuration**\n\n1. **Baseline:** ClickHouse configuration remained similar to my first post; highlights below\n2. **ClickHouse\\_stat:** enabled optimizer with table/column statistics\n3. **ClickHouse\\_tuned:** applied ClickHouse-specific rewrites (e.g., EXISTS/NOT EXISTS patterns and alternative join/filter forms) to a subset of queries; results above show improvements on Q21 but regressions elsewhere\n\n**Current ClickHouse config highlights**\n\n    max_threads = 16\n    max_memory_usage = 45 GB\n    max_server_memory_usage = 106 GB\n    max_concurrent_queries = 8\n    max_bytes_before_external_sort = 73 GB\n    join_use_nulls = 1\n    allow_experimental_correlated_subqueries = 1\n    optimize_read_in_order = 1\n    allow_experimental_statistics = 1       # on ClickHouse_stat\n    allow_statistics_optimize = 1           # on ClickHouse_stat\n\n**Summary of effectiveness so far**\n\n* Manual query rewrites improved Q21 consistently across SF1/SF10/SF30 but were neutral/negative for several other queries; net effect on whole-suite medians is minimal\n* Enabling statistics helped specific join-heavy queries (notably Q08/Q09), but overall medians remained 7-10x behind Exasol depending on scale", "date_utc": 1761664307.0, "title": "ClickHouse tuning for TPC-H - looking for guidance to close the gap on analytic queries vs Exasol", "upvote_ratio": 0.82, "score": 10, "url": "https://www.reddit.com/r/dataengineering/comments/1oic0e1/clickhouse_tuning_for_tpch_looking_for_guidance/"}, {"id": "1ois420", "name": "t3_1ois420", "content": "Hey everyone\n\nWe are moving to Snowflake and currently investigating tools to help with CI/CD. The frontrunners for us are Terraform for managing the databases, warehouses, schemas and roles, GitHub for code repository and dbt projects for Snowflake to manage the tables and data as they all integrate well with Snowflake.\n\nI just wanted to find out everyone's experiences and pitfalls in using these tools to build out their CI/CD pipelines? In particular:\n\n* Will these tools give us everything we will need for CI/CD? Are there any gaps we'll find once we start using them?\n* Are snowflake roles easy to maintain via Terraform?\n* How best to use these tools to handle schema drift? I see dbt has an on schema change feature that can help with schema change by adding new columns and ignoring deleted ones. Does this work well? Is it best to use dynamic tables when landing data into Snowflake for the first time and then use dbt to move it to the data model?\n* I see some people use schema change but can dbt not do the same thing?\n* Terraform looks like it destroys and recreates objects but I have read we can set a flag to stop this on specific objects. Does this work well?\n\nThanks for any help", "date_utc": 1761702448.0, "title": "What are the best tools to use for Snowflake CI/CD", "upvote_ratio": 1.0, "score": 1, "url": "https://www.reddit.com/r/dataengineering/comments/1ois420/what_are_the_best_tools_to_use_for_snowflake_cicd/"}, {"id": "1oif45q", "name": "t3_1oif45q", "content": "Hello everyone, I'm doing a part time as a customer service for an online class. I basically manage the students, their related informations, sessions bought, etc. Also relates it to the class that they are enrolled in. At the moment, all this information is stored in a monolithic sheets (well I did divide atleast the student data and the class, connect them by id).\n\nBut, I'm a CS student, and I just studied dbms last semester, this whole premise sounds like a perfect case to implement what I learn and design a relational database!\n\nSo, I'm here to crosscheck my plan. I plan this with gpt.. btw, because I can't afford to spend too much time working on this side project, and I'm not going to be paid for this extra work either, but then I believe this will help me a ton at my work, and I will also learn a bunch after designing the schema and seeing in real time how the database grows.\n\nSo the plan is use a local instance of postgreSQL with a frontend like NocoDB for spreadsheets like interface. So then I have the fallback of using NocoDB to edit my data, or when I can, and I will try to, always use SQL, or atleast make my own interface to manage the data.\n\nHere's some considerations why I should move to this approach:\n1. The monolithic sheets, one spreadsheets have too much column (phone number, name, classes bought, class id, classes left, last class date, note, complains, (sales related data like age, gender, city, learning objective). And just yesterday, I had a call with my manager, and she says that I should also includes payment information, and 2 types of complains, and I was staring at the long list of the data in the spreadsheets..\n2. I have a pain point of syncing two different sheets. So my company uses other service of spreadsheets (not google) and there is coworker that can't access this site from their country. So, I, again, need to update both of this spreadsheet, and the issue is my company have trust issue with google, so I would also need to filter some data before putting it into the google spreadsheet, from the company one. Too much hassle.\nWhat I hope to achievr from migrating to sql, is that I can just sync them both to my local instance of SQL instead of from one to the other.\n\ncons of this approach (that i know of):\nThis infrastructure will then depends on me, and I think I would need a no-code solution in the future if there will be other coworker in my position. \n\nOther approach being considered:\n Just refactore the sheets that mimics relational db (students, classes, enrolls_in, teaches_in, payment, complains)\nBut then having to filter and sync across the other sheets will still be an issue.\n\nI've read a post somewhere about a teacher that tried to do this kind of thing, basically a student management system. And then it just became a burden for him, needing him to maintain an ecosystem without being paid for it.\n\nBut from what I see, this approach seems need little maintenance and effort to keep up, so only the initial setup will be hard. But feel free to prove me wrong!\n\nThat's about it, I hope you all can give me insights whether or not this journey I'm about to take will be fruitful. I'm open to other suggestions and critics!", "date_utc": 1761671204.0, "title": "Migrating from Spreadsheets to PostgreSQL", "upvote_ratio": 0.67, "score": 3, "url": "https://www.reddit.com/r/dataengineering/comments/1oif45q/migrating_from_spreadsheets_to_postgresql/"}, {"id": "1oixurq", "name": "t3_1oixurq", "content": "A new database has been released: LinearDB.\n\nThis is a small, embedded database with a log file and index.\n\nsrc: [https://github.com/pwipo/LinearDB](https://github.com/pwipo/LinearDB)\n\nAlso LinearDB part was created on the ShelfMK platform.\n\nThis is an object-oriented NOSQL DBMS for the LinearDB database.\n\nIt allows you to add, update, delete, and search objects with custom fields.\n\nsrc: [https://github.com/pwipo/smc\\_java\\_modules/tree/main/internalLinearDB](https://github.com/pwipo/smc_java_modules/tree/main/internalLinearDB)", "date_utc": 1761721785.0, "title": "LinearDB", "upvote_ratio": 0.22, "score": 0, "url": "https://www.reddit.com/r/dataengineering/comments/1oixurq/lineardb/"}, {"id": "1oihozf", "name": "t3_1oihozf", "content": "Dataform brings structure and version control to your SQL-based data workflows. Instead of manually managing dozens of BigQuery scripts, you define dependencies, transformations, and schedules in one place almost like Git for your data pipelines. It helps teams build reliable, modular, and testable datasets that update automatically. If you\u2019ve ever struggled with tangled SQL jobs or unclear lineage, Dataform makes your analytics stack cleaner and easier to maintain. To get hands-on experience building and orchestrating these workflows, check out the [Orchestrate BigQuery Workloads with Dataform](https://www.netcomlearning.com/course/orchestrate-bigquery-workloads-with-dataform) course, it\u2019s a practical way to learn how to streamline data pipelines on Google Cloud.", "date_utc": 1761676937.0, "title": "Making BigQuery pipelines easier (and cleaner) with Dataform", "upvote_ratio": 0.57, "score": 1, "url": "https://www.reddit.com/r/dataengineering/comments/1oihozf/making_bigquery_pipelines_easier_and_cleaner_with/"}, {"id": "1oinf7v", "name": "t3_1oinf7v", "content": "So I need to implement an AI tool that can connect to a Postgresql database and look at some views to analyze them and create tables and charts. I need this solution to be integrated into my product (an Angular app with a Spring Boot backend). The tool should be accessible to certain clients through the \"administrative\" web app. The idea is that instead of redirecting the client to another page, I would like to integrate the solution into the existing app.\n\nI\u2019ve tested tools like Julius AI, and it seems like the type of tool I need, but it doesn\u2019t have a way to integrate into a web app that I know of. Could anyone recommend one? or would i have to implement my own model?", "date_utc": 1761690176.0, "title": "Looking for an AI tool for data analysis that can be integrated into a product.", "upvote_ratio": 0.47, "score": 0, "url": "https://www.reddit.com/r/dataengineering/comments/1oinf7v/looking_for_an_ai_tool_for_data_analysis_that_can/"}, {"id": "1oilwmx", "name": "t3_1oilwmx", "content": "O Fabric Community, est\u00e1 disponibilizando conte\u00fado gratuito de certifica\u00e7\u00f5es Azure, em portugu\u00eas, e com voucher de 100% de desconto!\n\nAprimore suas habilidades em mais de 50 sess\u00f5es (e tamb\u00e9m incluindo conte\u00fado em ingl\u00eas), e o melhor: garanta sua Certifica\u00e7\u00e3o Microsoft! Obtenha 100% de desconto nos exames DP-600 e DP-700 e prepare-se com sess\u00f5es focadas na certifica\u00e7\u00e3o de Azure Data Engineer (DP-203), Fabric Data Engineer (DP-700), PL-300 e DP-600.\n\nN\u00e3o perca a chance de impulsionar sua carreira.\n\nRegistre-se nas sess\u00f5es de certifica\u00e7\u00e3o, pelo link https://aka.ms/FBC_T1_FabricDataDays.", "date_utc": 1761686587.0, "title": "Intensivo de 50 Dias em Dados e IA com Certifica\u00e7\u00e3o Sem Custo", "upvote_ratio": 0.5, "score": 0, "url": "https://gustavosantosio.com/aproveite-o-fabric-data-days-intensivo-de-50-dias-em-dados-e-ia-com-certificacao-sem-custo"}, {"id": "1oie75u", "name": "t3_1oie75u", "content": "It's from the Kimball methodology but I got the life of me can't find it or think of its name. We're struggling to document this in my company and I can't put my finger on it. \n\nOut model is so messed up. Dimensions in facts everywhere ", "date_utc": 1761669178.0, "title": "What's the documentation that has facts across the top and dimensions across the side with X's for intersections", "upvote_ratio": 0.64, "score": 3, "url": "https://www.reddit.com/r/dataengineering/comments/1oie75u/whats_the_documentation_that_has_facts_across_the/"}, {"id": "1oi676i", "name": "t3_1oi676i", "content": "Kafka to Pinecone Pipeline is a open source pre-built Apache Beam streaming pipeline that lets you consume real-time text data from Kafka topics, generate embeddings using OpenAI models, and store the vectors in Pinecone for similarity search and retrieval. The pipeline automatically handles windowing, embedding generation, and upserts to Pinecone vector db, turning live Kafka streams into vectors for semantic search and retrieval in Pinecone\n\nThis video demos how to run the pipeline on Apache Flink with minimal configuration. I'd love to know your feedback - [https://youtu.be/EJSFKWl3BFE?si=eLMx22UOMsfZM0Yb](https://youtu.be/EJSFKWl3BFE?si=eLMx22UOMsfZM0Yb)\n\ndocs - [https://ganeshsivakumar.github.io/langchain-beam/docs/templates/kafka-to-pinecone/](https://ganeshsivakumar.github.io/langchain-beam/docs/templates/kafka-to-pinecone/)", "date_utc": 1761648980.0, "title": "Stream realtime data from kafka to pinecone", "upvote_ratio": 0.68, "score": 3, "url": "https://www.reddit.com/r/dataengineering/comments/1oi676i/stream_realtime_data_from_kafka_to_pinecone/"}, {"id": "1ohqjar", "name": "t3_1ohqjar", "content": "We hit a weird stage in our data platform journey where we have too many catalogs.  \nWe have Unity Catalog for using Databricks, Glue for using AWS, Hive for legacy jobs, and MLflow for model tracking. Each one works fine in isolation, but they don\u2019t talk to each other.\u00a0\n\nWhen running into some problems with duplicated data, permission issues and just basic trouble in finding out what data is where.\n\nThe result: duplicated metadata, broken permissions, and no single view of what exists.\n\nI started looking into how other companies solve this, and found two broad paths:\n\n|**Approach**|**Description**|**Pros**|**Cons**|\n|:-|:-|:-|:-|\n|**Centralized (vendor ecosystem)**|Use one vendor\u2019s unified catalog (like Unity Catalog) and migrate everything there.|Simpler governance, strong UI/UX, less initial setup.|High vendor lock-in, poor cross-engine compatibility (e.g. Trino, Flink, Kafka).|\n|**Federated (open metadata layer)**|Connect existing catalogs under a single metadata service (e.g. [Apache Gravitino](https://gravitino.apache.org)).|Works across ecosystems, flexible connectors, community-driven.|Still maturing, needs engineering effort for integration.|\n\nRight now we\u2019re leaning toward the federated path , but not replacing existing catalogs, just connecting them together.\u00a0 feels more sustainable in the long-term, especially as we add more engines and registries.\n\nI\u2019m curious how others are handling the metadata sprawl. Has anyone else tried unifying Hive + Iceberg + MLflow + Kafka without going full vendor lock-in?", "date_utc": 1761599140.0, "title": "Dealing with metadata chaos across catalogs \u2014 what\u2019s actually working?", "upvote_ratio": 0.93, "score": 51, "url": "https://www.reddit.com/r/dataengineering/comments/1ohqjar/dealing_with_metadata_chaos_across_catalogs_whats/"}, {"id": "1oifg0i", "name": "t3_1oifg0i", "content": "Que libros, en lo posible en espa\u00f1ol, me recomienda para introducirme en el mundo de la ingenieria de datos?", "date_utc": 1761671934.0, "title": "Libros de Ingenier\u00eda de Datos", "upvote_ratio": 0.46, "score": 0, "url": "https://www.reddit.com/r/dataengineering/comments/1oifg0i/libros_de_ingenier\u00eda_de_datos/"}, {"id": "1oieqhi", "name": "t3_1oieqhi", "content": "Posting here to get some perspective:\n\nJust saw release of Apache Grails 7.0.0, which has lead me down a java rabbit hole utilizing something known as sdkman (https://sdkman.io/) .\n\nHoly shit does it have some absolutely rad things but there is soooo much. \n\nSo, I was wondering, why do things like this not have more relevance in the modern data ecosystem?\n", "date_utc": 1761670364.0, "title": "Java", "upvote_ratio": 0.5, "score": 0, "url": "https://www.reddit.com/r/dataengineering/comments/1oieqhi/java/"}, {"id": "1oi64c6", "name": "t3_1oi64c6", "content": "", "date_utc": 1761648699.0, "title": "Faster Database Queries: Practical Techniques", "upvote_ratio": 0.72, "score": 3, "url": "https://kapillamba4.medium.com/faster-database-queries-practical-techniques-074ba9afdaa3"}, {"id": "1ohz0ai", "name": "t3_1ohz0ai", "content": "Hi, I just got into this new project. Here we'll be moving two Glue jobs away from AWS. They want to use snowflake. These jobs, responsible for replication from HANA to Snowflake, uses spark.\n\nWhat's the best approaches to achive this? And I'm very confused about this one thing - How does this extraction from HANA part will work in new environemnt. Can we connect with hana there?\n\nHas anyone gone through this same thing?\u00a0**Please help.**", "date_utc": 1761621596.0, "title": "Moving away Glue jobs to Snowflake", "upvote_ratio": 0.93, "score": 12, "url": "https://www.reddit.com/r/dataengineering/comments/1ohz0ai/moving_away_glue_jobs_to_snowflake/"}, {"id": "1oicqfv", "name": "t3_1oicqfv", "content": "I have a vendor who stores data in an amazon redshift dw and I need to sync their data to my snowflake environment. I have the needed connection details. I could use fivetran but it doesnt seem like they have a redshift connector (port 5439). Anyone have suggestions on how to do this?", "date_utc": 1761665955.0, "title": "Syncing Data from Redshift SQL DB to Snowflane", "upvote_ratio": 0.4, "score": 0, "url": "https://www.reddit.com/r/dataengineering/comments/1oicqfv/syncing_data_from_redshift_sql_db_to_snowflane/"}, {"id": "1ohrt4e", "name": "t3_1ohrt4e", "content": "we are on some SSIS crap and trying to move away from that. we have a preexisting account with GCP and some other teams in the org have started to create VMs and bigquery databases for a couple small projects. if we went fully with GCP for our main pipelines and data warehouse it could look like:\n\n* bigquery target\n* data transfer service for ingestion (we would mostly use the free connectors)\n* dataform for transformations\n* cloud composer (managed airflow) for orchestration\n\nwe are weighing against a hybrid deployment:\n\n* bigquery target again\n* fivetran or sling for ingestion\n* dbt cloud for transformations\n* prefect cloud or dagster+ for orchestration\n\nas for orchestration, it's probably not going to be too crazy:\n\n* run ingestion for common dimensions -> run transformation for common dims\n* run ingestion for about a dozen business domains at the same time -> run transformations for these\n* run a final transformation pulling from multiple domains\n* dump out a few tables into csv files and email them to people\n\nhaving everything with a single vendor is more appealing to upper management, and the GCP tooling looks workable, but barely anyone here has used it before so we're not sure. the learning curve is important here. most of our team is used to the drag and drool way of doing things and nobody has any real python exposure, but they are pretty decent at writing SQL. are fivetran and dbt (with dbt mesh) that much better than GCP data transfer service and dataform? would airflow be that much worse than dagster or prefect? if anyone wants to tell me to run away from GCP and don't look back, now is your chance.", "date_utc": 1761602121.0, "title": "going all in on GCP, why not? is a hybrid stack better?", "upvote_ratio": 0.9, "score": 22, "url": "https://www.reddit.com/r/dataengineering/comments/1ohrt4e/going_all_in_on_gcp_why_not_is_a_hybrid_stack/"}, {"id": "1ohga81", "name": "t3_1ohga81", "content": "Hi!\n\nAs part of a client I\u2019m working with, I was planning to migrate quite an old data platform to what many would consider a modern data stack (dagster/airlfow + DBT + data lakehouse). Their current data estate is quite outdated (e.g. single step function manually triggered, 40+ state machines running lambda scripts to manipulate data. Also they\u2019re on Redshit and connect to Qlik for BI. I don\u2019t think they\u2019re willing to change those two), and as I just recently joined, they\u2019re asking me to modernise it. The modern data stack mentioned above is what I believe would work best and also what I\u2019m most comfortable with. \n\nNow the question is, as DBT has been acquired by Fivetran a few weeks ago, how would you tackle the migration to a completely new modern data stack? Would DBT still be your choice even if not as \u201copen\u201d as it was before and the uncertainty around maintenance of dbt-core? Or would you go with something else? I\u2019m not aware of any other tool like DBT that does such a good job in transformation.\n\nAm I unnecessarily worrying and should I still go with proposing DBT? Sorry if a similar question has been asked already but couldn\u2019t find anything on here.\n\nThanks!", "date_utc": 1761576225.0, "title": "Migrating to DBT", "upvote_ratio": 0.91, "score": 38, "url": "https://www.reddit.com/r/dataengineering/comments/1ohga81/migrating_to_dbt/"}, {"id": "1ohr37r", "name": "t3_1ohr37r", "content": "I\u2019ve built [Davia](https://davia.ai?utm_source=reddit) \u2014 an AI workspace where your internal technical documentation writes and updates itself automatically from your GitHub repositories.\n\nHere\u2019s the problem: The moment a feature ships, the corresponding documentation for the architecture, API, and dependencies is already starting to go stale. Engineers get documentation debt because maintaining it is a manual chore.\n\nWith Davia\u2019s GitHub integration, that changes. As the codebase evolves, background agents connect to your repository and capture what matters\u2014from the development environment steps to the specific request/response payloads for your API endpoints\u2014and turn it into living documents in your workspace.\n\nThe cool part? These generated pages are highly structured and interactive. As shown in the video, When code merges, the docs update automatically to reflect the reality of the codebase.\n\nIf you're tired of stale wiki pages and having to chase down the \"real\" dependency list, this is built for you.\n\nWould love to hear what kinds of knowledge systems you'd want to build with this. Come share your thoughts on our sub [r/davia\\_ai](https://www.reddit.com/r/davia_ai/)!", "date_utc": 1761600410.0, "title": "Your internal engineering knowledge base that writes and updates itself from your GitHub repos", "upvote_ratio": 0.67, "score": 11, "url": "https://v.redd.it/8ufz9srd2qxf1"}, {"id": "1ohwq97", "name": "t3_1ohwq97", "content": "My company lost a few experienced devs over the past few months - including our terraform expert. We\u2019re now facing the deadline of our Oracle linked services expiring (they\u2019re all still on v1) at the end of the week. I\u2019m needing to update the terraform to generate v2 linked services, but have no clue what I\u2019m doing. I finally got it making a v2 linked services, just it\u2019s not populated. \n\nIs there a mapping document I could find showing the terraform variable name as it corresponds to the ADF YAML object?\n\nOr maybe does anyone know of a sample terraform that generates an Oracle v2 successfully that I can mimic?\n\nThanks in advance!", "date_utc": 1761614961.0, "title": "Building ADF via Terraform", "upvote_ratio": 0.87, "score": 6, "url": "https://www.reddit.com/r/dataengineering/comments/1ohwq97/building_adf_via_terraform/"}, {"id": "1ogtum7", "name": "t3_1ogtum7", "content": "", "date_utc": 1761507631.0, "title": "Please keep your kids safe this Halloween", "upvote_ratio": 0.98, "score": 754, "url": "https://i.redd.it/kdurnundeixf1.png"}, {"id": "1oi8spz", "name": "t3_1oi8spz", "content": "Mods kicked the first post cause of AI slop - I think it's cause I spent too much time trying to get the post right. We spent time on this product so it mattered.\n\n  \nAnyway. We built this product because of our experience of wanting a teat data management tool that didn't cost the earth and that solved the problem of a tool that gets us the data we need in the manner we need it.\n\nIt's Schema-aware test data masking that preserves relationships. AI-powered synthetic data generation for edge cases. Real-time preview so you can check before deploying. Integrates with CI/CD pipelines. Compliance ready.\n\nYou can try it for free here [gomask.ai](http://gomask.ai)\n\nAlso happy to answer any questions, technical or otherwise.", "date_utc": 1761656619.0, "title": "We built GoMask for test data management - launched last week", "upvote_ratio": 0.11, "score": 0, "url": "https://www.reddit.com/r/dataengineering/comments/1oi8spz/we_built_gomask_for_test_data_management_launched/"}, {"id": "1ohjp62", "name": "t3_1ohjp62", "content": "Hey all. I was hoping you all could give me some insights on CI/CD pipelines in Oracle.\n\nI'm curious if anyone here has actually gotten a decent CI/CD setup working with Oracle r12/ebiz (we\u2019re mostly dealing with PL/SQL + schema changes like MV and View updates). Currently we don't have any sort of pipeline, absolutely no version control, and any sort of push to production is done manually. Currently the team deploys to production, and you gotta hope they backed up the original code before pushing the update. It's awful. \n\nhow are you handling stuff like:  \n\u2022 schema migrations  \n\u2022 rollback safety  \n\u2022 PL/SQL versioning  \n\u2022 testing (if you\u2019re doing any)  \n\u2022 branching strategies\n\nany horror stories or tips appreciated. just trying not to reinvent the wheel here. \n\nSide note, I\u2019ve asked this before but I got flagged as AI slop. \ud83d\ude05 please \ud83d\ude4f don\u2019t delete this post. I\u2019m legitimately trying to solve this problem. ", "date_utc": 1761583833.0, "title": "CI/CD Pipelines for an Oracle shop", "upvote_ratio": 1.0, "score": 6, "url": "https://www.reddit.com/r/dataengineering/comments/1ohjp62/cicd_pipelines_for_an_oracle_shop/"}, {"id": "1oh3kac", "name": "t3_1oh3kac", "content": "I\u2019m curious to understand the community\u2019s feedback on DBT after the merger. Is it feasible for a mid-sized company to build using DBT\u2019s core as an open-source platform? \n\n\nMy thoughts on their openness to contributing further and enhancing the open-source product.\n\n", "date_utc": 1761533681.0, "title": "DBT's future on opensource", "upvote_ratio": 0.95, "score": 37, "url": "https://www.reddit.com/r/dataengineering/comments/1oh3kac/dbts_future_on_opensource/"}, {"id": "1ohq4r5", "name": "t3_1ohq4r5", "content": "How is everyone dealing with spark 3.5 to ignore the zero byte file while writing from notebook?", "date_utc": 1761598178.0, "title": "Spark zero byte file on spark 3.5", "upvote_ratio": 0.6, "score": 1, "url": "https://www.reddit.com/r/dataengineering/comments/1ohq4r5/spark_zero_byte_file_on_spark_35/"}, {"id": "1ohkl4g", "name": "t3_1ohkl4g", "content": "I started a new job about a week ago. I have to work on a project that calculates a company's profitability at the country level. The tech lead gave me free rein to do whatever I want with the project, but the main idea is to take the pipeline from Pyspark directly to Google services (Dataform, Bigquery, Workflow). So far, I have diagrammed the entire process. The tech lead congratulated me, but now he wants me to map the standardization from start to finish, and I don't really understand how to do it. It's my first job, and I feel a little confused and afraid of making mistakes. I welcome any advice and recommendations on how to function properly in the corporate world.\n\nMy position is process engineer, just in case you're wondering.", "date_utc": 1761585789.0, "title": "Entering this world with many doubts", "upvote_ratio": 0.5, "score": 0, "url": "https://www.reddit.com/r/dataengineering/comments/1ohkl4g/entering_this_world_with_many_doubts/"}, {"id": "1ogirla", "name": "t3_1ogirla", "content": "Hey,\n\nI have to rant a bit, since i've seen way too much posts in this reddit who are all like \"What certifications should i do?\" or \"what tools should i learn?\" or something about personal big data projects. What annoys me are not the posts themselves, but the culture and the companies making believe that all this is necessary. So i feel like people need to manage their expectations. In themselves and  in the companies they work for. The following are OPINIONS of mine that help me to check in with myself.\n\n1. You are not the company and the company is not you. If they want you to use a new tool, they need to provide PAID time for you to learn the tool.\n\n2. Don't do personal projects (unless you REALLY enjoy it). It just takes time you could have spend doing literally anything else. Personal projects will not prepare you for the real thing because the data isn't as messy,  the business is not as annoying and you want have to deal with coworkers breaking production pipelines. \n\n3. Nobody cares about certifications. If I have to do a certification, I want to be paid for it and not pay for it.\n\n4. Life over work. Always.\n\n5. Don't beat yourself up, if you don't know something. It's fine. Try it out and fail. Try again. (During work hours of course)\n\n\nDon't get me wrong, i read stuff in my offtime as well and i am in this reddit. But i only as long I enjoy it. Don't feel pressured to do anything because you think you need it for your career or some youtube guy told you to.\n\n\n\n", "date_utc": 1761479745.0, "title": "Rant: Managing expectations", "upvote_ratio": 0.87, "score": 68, "url": "https://www.reddit.com/r/dataengineering/comments/1ogirla/rant_managing_expectations/"}, {"id": "1oh6und", "name": "t3_1oh6und", "content": "I\u2019m working with IBM InfoSphere DataStage 11.7.\n\nI exported several jobs as XML files .\nThen, using a Python script, I modified the XML to add another database stage in parallel to an existing one (essentially duplicating and renaming a stage node).\n\nAfter saving the modified XML, I re-imported it back into the project.\nThe import completed without any errors, but when I open the job in the Designer, the new stage doesn\u2019t appear.\n\nMy questions are:\n\nDoes DataStage simply not support adding new stages by editing the XML directly?\nIs there any supported or reliable programmatic method to add new stages automatically because we have around 500 jobs?", "date_utc": 1761544963.0, "title": "DataStage XML export modified via Python \u2014 new stage not appearing after re-import", "upvote_ratio": 0.75, "score": 2, "url": "https://www.reddit.com/r/dataengineering/comments/1oh6und/datastage_xml_export_modified_via_python_new/"}, {"id": "1ogll5t", "name": "t3_1ogll5t", "content": "From devops, should I switch, to DE? \n\nIm a 4 yoe devops, and recently looking out. Tbh, i just spam my cv all the places for Data jobs. \n\n\nWhy im considering a transition is because I was involved with a DE project and I found out how calm and non toxic de environment in DE is. I would say due to most of the projects are not as critical in readiness compared to infra projects where people will ping you like crazy when things are broken or need attention. Not to mention late oncalls. \n\n\nAdditionally, ive found that devops openings are reducing in the market. I found like 3 new jobs monthly thats match my skillset. Besides, people are saying that devops scopes will probably be absorbed by developers and software engineer. Hence im feeling a bit of insecurity in terms of prospect there. \n\n\nSo ill be honest, i have a decent idea of what the fundamentals of being a de. But at the same time, i wanted to make sure that i have the right reasons to get into de. ", "date_utc": 1761487825.0, "title": "From devops to DE, good choice?", "upvote_ratio": 0.87, "score": 32, "url": "https://www.reddit.com/r/dataengineering/comments/1ogll5t/from_devops_to_de_good_choice/"}, {"id": "1ogop8w", "name": "t3_1ogop8w", "content": "Setting up a small e-commerce data stack. Sources are REST APIs (Python). Today: CSVs on SharePoint + Power BI. Goal: reliable ELT \u2192 warehouse \u2192 BI; easy to add new sources; low ops.\n\n**Considering:** Prefect (or Airflow), object storage as landing zone, ClickHouse vs Postgres/SQL Server/Snowflake/BigQuery, dbt, Great Expectations/Soda, DataHub/OpenMetadata, keep Power BI.\n\n**Questions:**\n\n1. Would you run ClickHouse as the main warehouse for API/event data, or pair it with Postgres/BigQuery?\n2. Anyone using Power BI on ClickHouse?\n3. For a small team: Prefect or Airflow (and why)?\n4. Any dbt/SCD patterns that work well with ClickHouse, or is that a reason to choose another WH?\n\nHappy to share our v1 once live. Thanks!", "date_utc": 1761495451.0, "title": "Looking for lean, analytics-first data stack recs", "upvote_ratio": 0.9, "score": 20, "url": "https://www.reddit.com/r/dataengineering/comments/1ogop8w/looking_for_lean_analyticsfirst_data_stack_recs/"}, {"id": "1ogqupx", "name": "t3_1ogqupx", "content": "I've been working on a gradient boosting implementation that handles two problems I kept running into with XGBoost/LightGBM in production:\n\n1. **Performance collapse on extreme imbalance (under 1% positive class)**\n2. **Silent degradation when data drifts (sensor drift, behavior changes, etc.)**\n\nKey Results\n\nImbalanced data (Credit Card Fraud - 0.2% positives):\n\n**- PKBoost: 87.8% PR-AUC**\n\n**- LightGBM: 79.3% PR-AUC**\n\n**- XGBoost: 74.5% PR-AUC**\n\nUnder realistic drift (gradual covariate shift):\n\n\\- PKBoost: 86.2% PR-AUC (\u22122.0% degradation)\n\n\\- XGBoost: 50.8% PR-AUC (\u221231.8% degradation)\n\n\\- LightGBM: 45.6% PR-AUC (\u221242.5% degradation)\n\n\n\n **What's Different**\n\nThe main innovation is using Shannon entropy in the split criterion alongside gradients. Each split maximizes:\n\n\n\nGain = GradientGain + \u03bb\u00b7InformationGain\n\n\n\nwhere \u03bb adapts based on class imbalance. This explicitly optimizes for information gain on the minority class instead of just minimizing loss.\n\nCombined with:\n\n\\- Quantile-based binning (robust to scale shifts)\n\n\\- Conservative regularization (prevents overfitting to majority)\n\n\\- PR-AUC early stopping (focuses on minority performance)\n\nThe architecture is inherently more robust to drift without needing online adaptation.\n\n Trade-offs\n\nThe good:\n\n\\- Auto-tunes for your data (no hyperparameter search needed)\n\n\\- Works out-of-the-box on extreme imbalance\n\n\\- Comparable inference speed to XGBoost\n\nThe honest:\n\n\\- \\~2-4x slower training (45s vs 12s on 170K samples)\n\n\\- Slightly behind on balanced data (use XGBoost there)\n\n\\- Built in Rust, so less Python ecosystem integration\n\n Why I'm Sharing\n\nThis started as a learning project (built from scratch in Rust), but the drift resilience results surprised me. I haven't seen many papers addressing this - most focus on online learning or explicit drift detection.\n\nLooking for feedback on:\n\n\\- Have others seen similar robustness from conservative regularization?\n\n\\- Are there existing techniques that achieve this without retraining?\n\n\\- Would this be useful for production systems, or is 2-4x slower training a dealbreaker?\n\n\n\n Links\n\n\\- GitHub: [https://github.com/Pushp-Kharat1/pkboost](https://github.com/Pushp-Kharat1/pkboost)\n\n\\- Benchmarks include: Credit Card Fraud, Pima Diabetes, Breast Cancer, Ionosphere\n\n\\- MIT licensed, \\~4000 lines of Rust\n\nHappy to answer questions about the implementation or share more detailed results. Also open to PRs if anyone wants to extend it (multi-class support would be great).\n\n\\---\n\n**Edit**: Built this on a 4-core Ryzen 3 laptop with 8GB RAM, so the benchmarks should be reproducible on any hardware.", "date_utc": 1761500571.0, "title": "[R] PKBoost: Gradient boosting that stays accurate under data drift (2% degradation vs XGBoost's 32%)", "upvote_ratio": 0.92, "score": 11, "url": "https://www.reddit.com/r/dataengineering/comments/1ogqupx/r_pkboost_gradient_boosting_that_stays_accurate/"}, {"id": "1oge8vq", "name": "t3_1oge8vq", "content": "Snowflake and Databricks doesn't do partitioning anymore. Both use clustering to co-locate data and they seem to be performant enough.   \n  \nDatabricks Liquid clustering page (https://docs.databricks.com/aws/en/delta/clustering#enable-liquid-clustering) specifies clustering as the best method to go with and avoid partitioning. \n\nSo when someone implements plain Vanilla Spark with Data Lake - Delta Lake or Iceberg - Still partitioning is best practice, but is it possible to implement clustering in a way that replicates the performance of Snowflake or Databricks. \n\nZORDER is basically the clustering technique - But what does Snowflake or Databricks do differently that avoids partitioning entirely? ", "date_utc": 1761462919.0, "title": "Is Partitioning data in Data Lake still the best practice?", "upvote_ratio": 0.97, "score": 76, "url": "https://www.reddit.com/r/dataengineering/comments/1oge8vq/is_partitioning_data_in_data_lake_still_the_best/"}, {"id": "1oglc5s", "name": "t3_1oglc5s", "content": "Background: the enterprise DE in my org manages the big data environment.  He uses nifi for orchestration and snowflake for the data warehouse. As far as how his environment is actually put together and communicating all I know is that he uses zookeeper for his nifi cluster and it\u2019s on the cloud (Azure). There is no one who knows anything more than that. No one in IT. Not his boss. Not his one employee. No one knows and his reason is that he doesn\u2019t trust anyone and they aren\u2019t good enough, not even his employee.\n\n\nThe discussion. Have you dealt with such a person? How has your org dealt with people gatekeeping like this? \n\nFrom my perspective this is a massive problem and basically means that this guy is a massive walking pile of technical debt. If he leaves then the clean up and troubleshooting to figure out what he did would be immense.  On top of that he now has suggested taking over smaller DE processes from other outside IT as a play to \u201ccentralize\u201d data engineering work. He won\u2019t let them migrate their stuff to his environment as again he doesn\u2019t rust them to be good enough and doesn\u2019t want to teach them how to use his environment. So he is just safe guarding his job really and taking away others jobs in my opinion.  I also recently got some people in IT to approve me setting up Airflow outside of IT and to do data engineering (which I was already doing but just with cron). He has thrown some shots at me but I ignored him because I\u2019m trying to set something up for other people to use to and document it so that it can be maintained should I leave.\n\nTLDR have you dealt with people gatekeeping knowledge and what happened to them?", "date_utc": 1761487201.0, "title": "DE Gatekeeping and Training", "upvote_ratio": 0.89, "score": 7, "url": "https://www.reddit.com/r/dataengineering/comments/1oglc5s/de_gatekeeping_and_training/"}, {"id": "1ogce2i", "name": "t3_1ogce2i", "content": "Hello everyone, I am a second-year computer science student. After some research, I chose data engineering as my main focus. However, during my learning process, I noticed that data scientists also do data engineering tasks, and software engineers often build pipelines too. I would like advice on how the real job market works: should I focus on learning both data science and data engineering? Also, which problems should I focus on learning and practicing, because working with data feels boring when it\u2019s not tied to a full project or real problem-solving?", "date_utc": 1761455914.0, "title": "Should I focus on both data science and data engineering?", "upvote_ratio": 0.86, "score": 24, "url": "https://www.reddit.com/r/dataengineering/comments/1ogce2i/should_i_focus_on_both_data_science_and_data/"}, {"id": "1ogohea", "name": "t3_1ogohea", "content": "We have data flowing through a Kinesis stream and we are currently using Firehose to write that data to S3. The cost seems high,  Firehose is costing us about twice as much as the Kinesis stream itself. Is that expected or are there more cost-effective and reliable alternatives for sending data from Kinesis to S3? \nEdit: No transformation, 128 MB Buffer size and 600 sec Buffer interval. Volume is high and it writes 128 MB files before 600 seconds.", "date_utc": 1761494933.0, "title": "Do I need Kinesis Data Firehose?", "upvote_ratio": 0.62, "score": 3, "url": "https://www.reddit.com/r/dataengineering/comments/1ogohea/do_i_need_kinesis_data_firehose/"}, {"id": "1og2l5l", "name": "t3_1og2l5l", "content": "I have 6+ years of experience in data analytics and have worked on multiple projects mostly related to data quality and process automation. I always wanted to work in a data engineering project and recently i got an opportunity to work on a project which seem to be exciting with GenAI & Python stuff. My role here is to develop python scripts to integrate multiple sources and LLM outputs and package everything into a solution. I designed a config driven ETL code using python and wrote multiple classes to package everything into a single codebase. I used LLM chats to optimise my code. Due to very tight deadlines I had to rush the development without realising the whole thing would turn into a nightmare. I have tried my best to follow the coding standards but the client is very upset about few parts of the design. A couple of days ago, I had a code review meeting with my client team where I had to walk through my code and answer questions inorder to get the approval for QA. The client team had an architect level manager who had already gone through the repository and had a lot of valid questions about the design flaws in the code. I felt very embarrassed during the meeting and it was a very awkward conversation. Everytime he had pointed out something wrong, I had no answers to it and there was silence for about half a minute before I say \" Ok I can implement that\". I know it is my fault that I didn't have enough knowledge about designing data systems but I'm worried more about tarnishing my companies' reputation by providing a low quality deliverable. I just wanted to rant about how disappointed I feel about myself. Have you ever been in a situation like this?", "date_utc": 1761426164.0, "title": "Rant: Excited to be a part of a project that turned out to be a nightmare", "upvote_ratio": 0.91, "score": 39, "url": "https://www.reddit.com/r/dataengineering/comments/1og2l5l/rant_excited_to_be_a_part_of_a_project_that/"}, {"id": "1ofpdbw", "name": "t3_1ofpdbw", "content": "I\u2019m a 27M working in data (currently in a permanent position). I started out as a data analyst, but now I handle end-to-end stuff: managing data warehouses (dev/prod), building pipelines, and maintaining automated reporting systems in BI tools.\n\nIt\u2019s quite a lot. I really want to improve my career, so I study every time I have free time: after work, on weekends, and so on.\n\nI\u2019ve been learning tools like Jira, Confluence, Git, Jinja, etc. They all serve different purposes, and it takes time to learn and use them effectively and securely. \n\nBut lately, I\u2019ve realized it\u2019s taking up too much of my time, the time I could use to hang out with friends or just *live*. It\u2019s not like I have that many friends (haha). Well, most of them are already married with families so... \n\nStill, I feel like I\u2019m missing out on the people around me, and that\u2019s not healthy.\n\nMy girlfriend even pointed it out. She said I need to scroll social media more, find fun activities, etc. She\u2019s probably right (except for the social media part, hehe).\n\nWhen will I exercise? When will I hit the gym? Why do I only hang out when it\u2019s with my girlfriend? When will I explore the city again? When will I get back to reading books I have bought? It\u2019s been ages since I read anything for fun.\n\nThat\u2019s what\u2019s been running through my mind lately.\n\nI\u2019ve realized my lifestyle isn't healthy, and I want to change.\n\n**TL;DR:** Any advice on how to stay focused on earning certifications and improving my skills *while still having time for personal, social, and family life?*", "date_utc": 1761392332.0, "title": "How do you balance learning new skills/getting certs with having an actual life?", "upvote_ratio": 0.99, "score": 106, "url": "https://www.reddit.com/r/dataengineering/comments/1ofpdbw/how_do_you_balance_learning_new_skillsgetting/"}, {"id": "1oft9b4", "name": "t3_1oft9b4", "content": "Hello, I'm new to aws and dbt and very confused of how dbt and aws stuck together?\n\nRaw data let's say transaction and other data go from an erp system to s3, then from there you use aws glue to make tables so you are able to query with athena to push clean tables into redshift and then you use dbt to make \"views\" like joins, aggregations to redshift again to be used for analytic purposes?\n\nSo s3 is the raw storage, glue is the ETL tool, then lambda or step functions are used to trigger etl jobs to transfer data from s3 to redshift using glue, and then use dbt for other transformations?\n\nPlease correct me if im wrong, I'm just starting using these tools. ", "date_utc": 1761403350.0, "title": "AWS + dbt", "upvote_ratio": 0.97, "score": 25, "url": "https://www.reddit.com/r/dataengineering/comments/1oft9b4/aws_dbt/"}, {"id": "1ofi6l7", "name": "t3_1ofi6l7", "content": "I\u2019m dealing with a colleague who\u2019s honestly becoming a pain to work with. He\u2019s in his mid-career as a data engineer, and he acts like he knows everything already. The problem is, he\u2019s incredibly lazy when it comes to actually doing the work.\n\nHe avoids writing code whenever he can, only picks the easy or low-effort tasks, and leaves the more complex or critical problems for others to handle. When it comes to operational stuff \u2014 like closing tickets, doing optimization work, or cleaning up pipelines \u2014 he either delays it forever or does it half-heartedly.\n\nWhat\u2019s frustrating is that he talks like he\u2019s the most experienced guy on the team, but his output and initiative don\u2019t reflect that at all. The rest of us end up picking up the slack, and it\u2019s starting to affect team morale and delivery.\n\nHas anyone else dealt with a \u201cknow-it-all but lazy\u201d type like this? How do you handle it without sounding confrontational or making it seem like you\u2019re just complaining?", "date_utc": 1761365533.0, "title": "How you deal with a lazy colleague", "upvote_ratio": 0.91, "score": 82, "url": "https://www.reddit.com/r/dataengineering/comments/1ofi6l7/how_you_deal_with_a_lazy_colleague/"}, {"id": "1oghz5q", "name": "t3_1oghz5q", "content": "Any recommendations for learning first  ETL tool ?", "date_utc": 1761477063.0, "title": "ETL Tools", "upvote_ratio": 0.29, "score": 0, "url": "https://www.reddit.com/r/dataengineering/comments/1oghz5q/etl_tools/"}, {"id": "1of3kqp", "name": "t3_1of3kqp", "content": "Just out of curiosity, I setup a simple benchmark that calculates a\u00a0Mandelbrot fractal in plain SQL using DataFusion and DuckDB \u2013 no loops, no UDFs, no procedural code.  \n  \nI honestly expected it to crawl. But the results are \u2026 surprising:  \n  \n Numpy (highly optimized) 0,623 sec (0,83x)  \n\ud83e\udd47DataFusion (SQL) 0,797 sec (baseline)  \n\ud83e\udd48DuckDB (SQL) 1,364 sec (\u00b12x slower)  \n Python (very basic) 4,428 sec (\u00b15x slower)  \n\ud83e\udd49 SQLite (in-memory)\u00a0 44,918\u00a0sec (\u00b156x times slower)  \n  \nTurns out,\u00a0modern SQL engines are nuts\u00a0\u2013 and Fractals are actually a fun way to benchmark the recursion capabilities and query optimizers of modern SQL engines. Finally a great exercise to improve your SQL skills.  \n  \nTry it yourself (GitHub repo): [https://github.com/Zeutschler/sql-mandelbrot-benchmark](https://github.com/Zeutschler/sql-mandelbrot-benchmark)  \n  \nAny volunteers to prove DataFusion isn\u2019t the fastest fractal SQL artist in town? PR\u2019s are very welcome\u2026", "date_utc": 1761326097.0, "title": "Modern SQL engines draw fractals faster than Python?!?", "upvote_ratio": 0.85, "score": 175, "url": "https://i.redd.it/yklx634ke3xf1.png"}, {"id": "1of9y36", "name": "t3_1of9y36", "content": "Hey everyone,\nI\u2019m a data engineer with about 1 year of experience working in a 7 persons' BI team, and I\u2019m the only data engineer there.\n\nRecently I realized I\u2019ve been working extra hours for free. I deployed a local Git server, maintain and own the DB instance that hosts our DWH, re-implemented and redesigned Python dashboards because the old implementation was slow and useless, deployed some infrastructure for data engineering workloads, developed cli frameworks to cut-off manual work and code redundancy, and harmonized inconsistent sources to produce accurate insights (they used to just dump Excel files and DB tables into SSIS, which generated wrong numbers) all locally.\n\nLast Thursday, we got a request with a deadline on Sunday, even though Friday and Saturday are our weekend (I\u2019m in Egypt, and my team is currently working from home to deliver it, for free).\n\nAt first, I didn\u2019t mind because I wanted to deliver and learn, but now I\u2019m getting frustrated. I barely have time to rest, let alone learn new things that could actually help me grow (technically or financially).\n\nUnpaid overtime is normalized here, and changing companies locally won\u2019t fix that. So I\u2019ve started thinking about moving to Europe, but I\u2019m not sure I\u2019m ready for such a competitive market since everything we do is on-prem and I\u2019ve never touched cloud platforms.\n\nAnother issue: I feel like the only technical person in the office. When I talk about software design, abstraction, or maintainability, nobody really gets it. They just think I\u2019m \u201cgoing fancy,\u201d which leaves me on-call. \n\nOne time, I recommended loading all our sources into a 3rd normal form schema as a single source of truth, because the same piece of information was scattered across multiple systems and needed tracking, enforcement, and auditing before hitting our Kimball DWH. They looked at me like I was a nerd trying to create extra work.\n\nI\u2019m honestly feeling trapped.\nShould I keep grinding, or start planning my exit to a better environment (like Europe or remote)?\nAny advice from people who\u2019ve been through this?\n\n\nEdit:\nThe management decided to compensate us with additional annual leave, and I found out that our senior engineer has been negotiating with management for a salary raise for the entire department after these rough days. I think there\u2019s something I\u2019m missing.", "date_utc": 1761341198.0, "title": "Feeling stuck as the only data engineer, unpaid overtime, no growth, and burnout creeping in", "upvote_ratio": 0.92, "score": 44, "url": "https://www.reddit.com/r/dataengineering/comments/1of9y36/feeling_stuck_as_the_only_data_engineer_unpaid/"}, {"id": "1og1dx4", "name": "t3_1og1dx4", "content": "Curious to read thriller stories, anecdotes, real-life examples about AI systems (agentic or not):\n\n- epic AI system crashes\n\n- infra costs that took you by surprise\n\n- people getting fired, replaced by AI systems, only to be called back to work due to major failures, etc.\n\n", "date_utc": 1761423118.0, "title": "Halloween stories with (agentic) AI systems", "upvote_ratio": 0.44, "score": 0, "url": "https://www.reddit.com/r/dataengineering/comments/1og1dx4/halloween_stories_with_agentic_ai_systems/"}, {"id": "1of1xxl", "name": "t3_1of1xxl", "content": "Disclaimer: I'm the author of the blog post and I work for e6data.\n\nIf you work with a lot of JSON string columns, you might have heard of the Variant data type (in Snowflake, Databricks or Spark). I recently implemented this type in e6data's query engine and I realized that resources on the implementation details are scarce. The parquet variant spec is great, but it's quite dense and it takes a few reads to build a mental model of variant's binary format.\n\nThis blog is an attempt to explain why variant is so much faster than JSON strings (Databricks says it's 8x faster on their engine). AMA!", "date_utc": 1761322332.0, "title": "7x faster JSON in SQL: a deep dive into Variant data type", "upvote_ratio": 0.91, "score": 47, "url": "https://www.e6data.com/blog/faster-json-sql-variant-data-type"}, {"id": "1oexofz", "name": "t3_1oexofz", "content": "So, my question is intended to generate a discussion about cloud, tools and services in order do achieve this (taking IA into consideration).\n\nIs the Apache Airflow gang still the best? Or do reliable companies build from scratch using SQS / S3 / etc or PubSub / Google equivalent ?\n\nBy the way, it would be a function to extract data from third-party APIs, save raw response, then another function to transform data and then another one to load on DB\n\nEdit:\n\n- Hourly updates intraday\n- Daily updates last 15 days\n- Monthly updates last 3 months", "date_utc": 1761312283.0, "title": "You need to build a robust ETL pipeline today, what would you do?", "upvote_ratio": 0.94, "score": 75, "url": "https://www.reddit.com/r/dataengineering/comments/1oexofz/you_need_to_build_a_robust_etl_pipeline_today/"}, {"id": "1ofi2l0", "name": "t3_1ofi2l0", "content": "So I was recently laid off but have been very fortunate in getting tons of interviews for DE position. I failed a bunch but recently passed two. Spouse is fine with relocation as he is fully remote. \n\nI have 5 years in consulting (1 real year in DE based consulting).  I have masters degree as well. I was making 130k. So I\u2019m definitely breaking into the industry. \n\nTwo options: \n\n1. I\u2019ve recently gotten a contract to hire position in HCOL city (sf, nyc). 150k no benefits. Company is big retail. I am married so I would get benefits through my spouse. Really nice people but don\u2019t love the DE team as much. Business team is great. \n\n2. Big pharma/med device company in chi. This is  only 100k but  great benefits package. It is also closer to family and would be good for long term family planning. I actually really love the team and they\u2019re going to do a full overhaul and go into cloud and I would love to be part of it from the ground up experience. \n\nIn a way I am definitely breaking into the industry. My consulting gigs didn\u2019t give me enough experience and I\u2019m shy when I even refer to myself as a DE. It\u2019s also at a time when many don\u2019t have a job. So I am very very grateful that I even have the options. \n\nI\u2019m open to any advice! \n\n", "date_utc": 1761365147.0, "title": "100k offer in Chicago for DE? Or take higher contract in HCOL?", "upvote_ratio": 0.62, "score": 3, "url": "https://www.reddit.com/r/dataengineering/comments/1ofi2l0/100k_offer_in_chicago_for_de_or_take_higher/"}, {"id": "1of9pi5", "name": "t3_1of9pi5", "content": "As part of a wider move towards data products as well as building better controls into our pipelines, we\u2019re looking at how we can implement data contracts as code. I\u2019ve done a number of proof of concepts across various options and currently the Open Data Contract Specification alongside datacontract-cli is looking good. However, while I see how it can work well with \u201cfrozen\u201d contracts, I start getting lost on how to allow schema evolution. \n\nOur typical scenarios for Python-based data ingestion pipelines are all batch-based, consisting of files being pushed to us or our pulling from database tables. Our ingestion pattern is to take the producer dataset, write it to parquet for performant operations, and then validate it with schema and quality checks. The write to parquet (with PyArrow\u2019s ParquetWriter) should include the contract schema to enforce the agreed or known datatypes. \n\nHowever, with dynamic schema evolution, you ideally need to capture the schema of the dataset to be able to compare it to your current contract state to alert for breaking changes etc. Contract-first formats like ODCS take a bit of work to define, plus you may have zero-padded numbers defined as varchar in the source data you want to preserve, so inferring that schema for comparison becomes challenging. \n\nI\u2019ve gone down quite a rabbit hole now and am likely overcooking it, but my current thinking is to write all dataset fields to parquet as string, validate the data formats are as expected, and then subsequent pipeline steps can be more flexible with inferred schemas. I think I can even see a way to integrate this with dlt. \n\nHow do others approach this?", "date_utc": 1761340602.0, "title": "Implementing data contracts as code", "upvote_ratio": 0.92, "score": 10, "url": "https://www.reddit.com/r/dataengineering/comments/1of9pi5/implementing_data_contracts_as_code/"}, {"id": "1ofgl2p", "name": "t3_1ofgl2p", "content": "I would be grateful if anyone could share any practise questions for the Snowpro core certification. A lot of websites have paid options but I\u2019m not sure if the material is good. You can send me message if you like to share privately \nThanks a lot ", "date_utc": 1761360282.0, "title": "Snowflake snow pro core certification", "upvote_ratio": 0.7, "score": 4, "url": "https://www.reddit.com/r/dataengineering/comments/1ofgl2p/snowflake_snow_pro_core_certification/"}, {"id": "1of3og7", "name": "t3_1of3og7", "content": "We inherited an older ETL setup that uses desktop based designer, local XML configs and manual deployments through scripts. It works fine I would say but getting changes live is incredibly complex. Need to make the stack ready for faster iterations and cloud native deployment. We also need to use API sources like Salesforce and Shopify. \n\nThere's also a requiremnet to handle schema drift correctly as now even small column changes cause errors. I think Talend is the closes fit to what we need but it is still very bulky for our requirements (correct me if I am wrong). Lots of setup, dependency handling and also maintenance overhead which we would ideally like to avoid. \n\nWhat Talend alternatives should be look at? The ones that support conditional logic and also solve our requirement.", "date_utc": 1761326337.0, "title": "Suggest Talend alternatives", "upvote_ratio": 0.94, "score": 14, "url": "https://www.reddit.com/r/dataengineering/comments/1of3og7/suggest_talend_alternatives/"}, {"id": "1ofequj", "name": "t3_1ofequj", "content": "Hello everyone,\n\nI am a beginner data engineer (~1 yoe in DE), we have built a python ingestion framework that does the following:\n\n1. Fetches data in chunks from RDS table\n2. Loads dataframes to Snowflake tables using put stream to SF stage and COPY INTO.\n\nConfig for each source table in RDS, target table in Snowflake, filters to apply etc are maintained in a snowflake table which is fetched before each Ingestion Job.\nThese ingestion jobs need to run on a schedule, therefore we created cronjobs on an on-prem VM (yes, 1 VM) that triggers the python ingestion script (daily, weekly, monthly for different source tables).\nWe are moving to EKS by containerizing the ingestion code and using Kubernetes Cronjobs to achieve the same behaviour as earlier (cronjobs in VM). There are other options like Glue, Spark etc but client wants EKS, so we went with it. Our team is also pretty new, so we lack experience to say \"Hey, instead of EKS, use this\". \nThe ingestion module is just a bunch of python scripts with some classes and functions. How much can performance be improved if I follow a worker pattern where workers pull from a job queue (AWS SQS?) and do just plain extract and load from rds to snowflake. The workers can be deployed as a kubernetes deployment with scalable replicas of workers. A master pod/deployment can handle orchestration of job queue (adding, removing, tracking ingestion jobs).\nI beleive this approach can scale well compared to Cronjobs approach where each pod that handles ingestion job can only have access to finite resources enforced by resources.limits.cpu and mem.\n\nPlease give me your suggestions regarding the current approach and new design idea. Feel free to ridicule, mock, destroy my ideas. As a beginner DE i want to learn best practices when it comes to data ingestion particularly at scale. At what point do i decide to switch from existing to a better pattern?\n\nThanks in advance!!!", "date_utc": 1761354634.0, "title": "Python Data Ingestion patterns/suggestions.", "upvote_ratio": 1.0, "score": 3, "url": "https://www.reddit.com/r/dataengineering/comments/1ofequj/python_data_ingestion_patternssuggestions/"}, {"id": "1of2i5r", "name": "t3_1of2i5r", "content": "I feel struggle using Genie, anyone has alternative recommend choice? Open source is also fine.", "date_utc": 1761323627.0, "title": "What is the best alternative genie for data in databricks", "upvote_ratio": 0.92, "score": 9, "url": "https://www.reddit.com/r/dataengineering/comments/1of2i5r/what_is_the_best_alternative_genie_for_data_in/"}, {"id": "1ofbeax", "name": "t3_1ofbeax", "content": "Recently I have been contemplating the idea of a \"data ontology\" on top of Apache Iceberg. The idea is that within a domain you can change data schema in any way you intend using default Apache Iceberg functionality. However, when you publish a data product such that it can be consumed by other data domains then the schema of your data product is frozen, and there is some technical enforcement of the data schema such that the upstream provider domain cannot simply break the schema of the data product thus causing trouble for the downstream consumer domain. Whenever a schema change of the data product is required then the upstream provider domain must go through an official change request with version control etc. that must be accepted by the downstream consumer domain.\n\nObviously, building the full product would be highly complicated with all the bells and whistles attached. But building a small PoC to showcase could be achievable in a realistic timeframe.\n\nNow, I have been wondering:\n\n1. What do you generally think of such an idea? Am I onto something here? Would there be demand for this? Would Apache Iceberg be the right tech for that?\n\n2. I could not find this idea implemented anywhere. There are things that come close (like Starburst's data catalogue) but nothing that seems to actually technically enforce schema change for data products. From what I've seen most products seem to either operate at a lower level (e.g. table level or file level), or they seem to not actually enforce data product schemas but just describe their schemas. Am I missing something here?", "date_utc": 1761345002.0, "title": "Enforced and versioned data product schemas for data flow from provider to consumer domain in Apache Iceberg?", "upvote_ratio": 1.0, "score": 3, "url": "https://www.reddit.com/r/dataengineering/comments/1ofbeax/enforced_and_versioned_data_product_schemas_for/"}, {"id": "1oevtwo", "name": "t3_1oevtwo", "content": "I work in a highly regulated industry. Security says that we can\u2019t use Gemini for analytics due to compliance concerns. The issue is sensitive data leaving our governed environment.   \n  \nHow are others here handling this? Especially if you\u2019re in a regulated industry. Are you banning LLMs outright, or is there a compliant way to get AI assistance without creating a data leak?", "date_utc": 1761307285.0, "title": "How are you handling security compliance with AI tools?", "upvote_ratio": 0.85, "score": 17, "url": "https://www.reddit.com/r/dataengineering/comments/1oevtwo/how_are_you_handling_security_compliance_with_ai/"}, {"id": "1oetsdv", "name": "t3_1oetsdv", "content": "Hey everyone,\n\nI\u2019ve been working on a small Python package called df2tables that lets you display interactive, filterable, and sortable HTML tables directly **i**nside notebooks Jupyter, VS Code, Marimo (or in a separate HTML file).\n\nIt\u2019s also handy if you\u2019re someone who works with DataFrames but doesn\u2019t love notebooks. You can render tables straight from your source code to a standalone HTML file - no notebook needed.\n\nThere\u2019s already the well-known *itables* package, but `df2tables` is a bit different:\n\n* Fewer dependencies (just pandas *or* polars)\n* Column controls automatically match data types (numbers, dates, categories)\n* can outside notebooks \u2013 render directly to HTML\n* customize DataTables behavior **directly from Python**\n\nRepo: [https://github.com/ts-kontakt/df2tables](https://github.com/ts-kontakt/df2tables)", "date_utc": 1761300503.0, "title": "df2tables - Interactive DataFrame tables inside notebooks", "upvote_ratio": 0.86, "score": 14, "url": "https://i.redd.it/u8yjld8ia1xf1.gif"}, {"id": "1offh3z", "name": "t3_1offh3z", "content": "Hey guys,\n\nEvery now and then we encounter a large report with a lot of useful data but that would be pain to read. Would be cool if you could quickly gather the key points and visualise it.\n\nCheck out Visual Book:\n\n1. You upload a PDF\n2. Visual Book will turn it into a presentation with illustrations and charts\n3. Generate more slides for specific topics where you want to learn more\n\nLink is available in the first comment.", "date_utc": 1761356834.0, "title": "Data is great but reports are boring", "upvote_ratio": 0.44, "score": 0, "url": "https://v.redd.it/mw0ov9v1y5xf1"}, {"id": "1oeuq1d", "name": "t3_1oeuq1d", "content": "I am looking for libraries or frameworks (Python or JavaScript) for interactive graphing. Need something that is very tactile (NOT static charts) where end users can zoom, pan, and explore different timeframes.\n\nIdeally, I don\u2019t want to build this functionality from scratch; I\u2019m hoping for something out-of-the-box so I can focus on ETL and data prep for the time being.\n\nHas anyone used or can recommend tools that fit this use case?\n\nThanks in advance.", "date_utc": 1761303777.0, "title": "Interactive graphing in Python or JS?", "upvote_ratio": 1.0, "score": 7, "url": "https://www.reddit.com/r/dataengineering/comments/1oeuq1d/interactive_graphing_in_python_or_js/"}, {"id": "1of07iu", "name": "t3_1of07iu", "content": "If you are a data engineer, and your biggest issue is getting insights to your business users faster, do you mean:\n\n1. the infrastructure of your data platform sucks and it takes too much time of your data team to deal with it? or\n\n2. your business is asking to onboard new datasets, and this takes too long?\n\nHonest question.\n", "date_utc": 1761318343.0, "title": "Faster insights: platform infrastructure or dataset onboarding problems?", "upvote_ratio": 0.81, "score": 3, "url": "https://www.reddit.com/r/dataengineering/comments/1of07iu/faster_insights_platform_infrastructure_or/"}, {"id": "1oed53z", "name": "t3_1oed53z", "content": "I have a few, potentially false beliefs about MDM. I'm being hot-takey on purpose. Would love a slap in the face.\n\n1. Data Products contextualize dims/descriptive data, in the context of the product, and as such they might not need a MDM tool to master it at the full/edw/firm level.\n2. Anything with \"Master blah Mgmt\"  w/r/t Modern Data ecosystems overall is probably dead just out of sheer organizational malaise, politics, bureaucracy and PMO styles of trying to \"get everyone on board\" with such a concept, at large.\n3. Even if you bought a tool and did MDM well - on core entities of your firm (customer, product, region, store, etc..) - I doubt IT/business leaders would dedicated the labor discipline to keeping it up. It would become a key-join nightmare at some point.\n4. Do \"MDM\" at the source. E.g. all customers come from CRM. use the account\\_key and be done with it. If it's wrong in SalesForce, get them to fix it.\n\nNo?\n\nEDIT: MDM == Master Data Mgmt. See Informatica, Profisee, Reltio", "date_utc": 1761248823.0, "title": "MDM Is Dead, Right?", "upvote_ratio": 0.92, "score": 100, "url": "https://www.reddit.com/r/dataengineering/comments/1oed53z/mdm_is_dead_right/"}, {"id": "1of2nm4", "name": "t3_1of2nm4", "content": "Some fact tables are fairly straightforward, others can be very complicated. I'm working on a extremely complicated composite metric fact table, the output metric is computed queries/combinations/logic from \\~15 different business process fact tables. From a quality standpoint I am very concerned about transparency and explainability of this final metric. So, in addition to the metric value, I'm also considering writing to the fact the values which were used to create the desired metric, with their vintage and other characteristics. So, for example if the metric M=A+B+C-D-E+F-G+H-I; then I would not only store each value, but also the point in time it was pulled from source \\[some of these values are very volatile and are essentially sub queries with logic/filters\\]. For example: A\\_Value = xx, B\\_Value = yyy, C\\_value = zzzz, A\\_TimeStamp = 10/24/25 3:56AM, B\\_Timestamp = 10/24/25 1:11AM, C\\_Timestamp = 10/24/25 6:47AM.\n\nYou can see here that M was created using data from very different points of time, and in this case the data can change a lot within a few hours. \\[data is not only being changed by a 24x7 global business, but also by system batch processing on schedule\\] If someone else uses the same formula, but data from later points in time they might get a different result (and yes, we would ideally wish A,B,C...  to be from the same point in time). \n\n\n\nIs this a design pattern being used? Is there a better way? Is there resources I can use to learn more about this? \n\n  \n\n\nAgain, I wouldn't use this in all designs, only those of sufficient complexity to create better visibility as to \"why the value is what it is\" (when others might disagree and argue because they used the same formula with data from different points in time or filters). \n\n\\*\\* note: I'm considering techniques to ensure all formula components are from the same \"time\" (aka: using time travel in Snowflake, or similar techniques) - but for this question, I'm only concerned about the data modeling to capture/record artifacts used for data quality / explainability. Thanks in advance!", "date_utc": 1761323987.0, "title": "Writing artifacts on a complex fact for data quality / explainability?", "upvote_ratio": 0.67, "score": 2, "url": "https://www.reddit.com/r/dataengineering/comments/1of2nm4/writing_artifacts_on_a_complex_fact_for_data/"}, {"id": "1oe9gn9", "name": "t3_1oe9gn9", "content": "cause it\u2019s not\n\npipelines break, schemas drift, apis get deprecated, a marketing team renames one column and suddenly the \u201cbulletproof\u201d dashboard that execs stare at every morning is just... blank\n\nthe job isn\u2019t to build a perfect system once and ride into the sunset. the job is to\u00a0*own*\u00a0the system \u2014 babysit it, watch it, patch it before the business even realizes something\u2019s off. it\u2019s less \u201cbuild once\u201d and more \u201ckeep this fragile ecosystem alive despite everything trying to kill it\u201d\n\ngood data engineers already know this. code fails \u2014 the question is how fast you notice. data models drift \u2014 the question is how fast you adapt. requirements change every quarter -- the question is how fast you can ship the new version without taking the whole thing down\n\nthis is why \u201cset and forget\u201d data stacks always end up as \u201cset and regret.\u201d the people who treat their stack like software \u2014 with monitoring, observability, contracts, proper version control \u2014 they sleep better (well, most nights)\n\ndata is infrastructure. and infrastructure needs maintenance. nobody builds a bridge and says \u201ccool, see you in five years\u201d\n\nso yeah. next time someone says \u201ccan we just automate this pipeline and be done with it?\u201d -- maybe remind them of that bridge", "date_utc": 1761240394.0, "title": "I wish business people would stop thinking of data engineering as a one-time project", "upvote_ratio": 0.94, "score": 136, "url": "https://www.reddit.com/r/dataengineering/comments/1oe9gn9/i_wish_business_people_would_stop_thinking_of/"}, {"id": "1oexqja", "name": "t3_1oexqja", "content": "\nHey guys! Happy to be part of the discussion. I have 2 year of experience in data engineering, data architecture and data analysis. I really enjoy doing this but want to see if there are better ways to do an ETL. I don\u2019t know who else to talk to! \n\nI would love to learn how you all automate you ETL process ? I know this process is very time consuming and requires a lot of small steps, such as removing duplicates and applying dictionaries. My team currently uses an excel file to track parameters such as the name of the tables, column names, column renames, unpivot tables, etc. Honestly, the excel file gives us enough flexibility to make changes to the data frame.\n\nAnd while our process is mostly automated and we only have one python notebook doing the transformation, filling the excel file is very painful and time Consuming. I just wanted to hear some different point of view? Thank you!!! ", "date_utc": 1761312433.0, "title": "ETL help", "upvote_ratio": 0.83, "score": 4, "url": "https://www.reddit.com/r/dataengineering/comments/1oexqja/etl_help/"}, {"id": "1oetgr7", "name": "t3_1oetgr7", "content": "Join our webinar in November guyss!", "date_utc": 1761299295.0, "title": "Webinar: How clean product data + event pipelines keep composable systems from breaking.", "upvote_ratio": 1.0, "score": 7, "url": "https://us06web.zoom.us/webinar/register/WN_oqXZllqeSJWYdVGHWBdNGA#/registration"}, {"id": "1oewno2", "name": "t3_1oewno2", "content": "Hi all, I'm a new DE that's learning a lot about data pipelines. I've taught myself how to spin up a server and run a pretty decent pipeline for a startup. However, I'm using the LocalExecutor which runs everything on a single machine. With multiple CPU bound tasks running in parallel, my machine can't handle them all and as a results the tasks become really slow.\n\nI've read the docs and asked AI on how to setup a cluster with Celery, but all of this is quite confusing. After setting up a celery broker, how can I tell Airflow which servers to connect to? For me, I can't grasp the concept just by reading the docs. Looking online only have introductions about how the Executor works, not in detail and not going into the code much.\n\nAll of my tasks are docker containers run with DockerOperators, so I think running on a different machine would be easy. I just can't figure out how to set them up. Any experienced DEs know some tips/sources that could be of help? ", "date_utc": 1761309653.0, "title": "Help with running Airflow tasks on remote machines (Celery or Kubernetes)?", "upvote_ratio": 0.6, "score": 1, "url": "https://www.reddit.com/r/dataengineering/comments/1oewno2/help_with_running_airflow_tasks_on_remote/"}, {"id": "1oeh29i", "name": "t3_1oeh29i", "content": "I recently started a project with two data scientists and it\u2019s been a bit difficult because they both prioritize things other than getting a working product. My main focus is usually to get the output correct first and foremost in a pipeline. I do a lot of testing and iterating with code snippets outside functions for example as long as it gets the output correct. From there, I put things in functions/classes, clean it up, put variables in scopes/envs, build additional features, etc. These two have been very adamant about doing everything in the correct format first, adding in all the features, and we haven\u2019t got a working output yet. I\u2019m trying to catch up but it keeps getting more complicated the more we add. I really dislike this but I\u2019m not sure what\u2019s standard or if I need to learn to work in a different way.\n\nWhat do you all think? ", "date_utc": 1761258187.0, "title": "Teamwork/standards question", "upvote_ratio": 1.0, "score": 6, "url": "https://www.reddit.com/r/dataengineering/comments/1oeh29i/teamworkstandards_question/"}, {"id": "1oe92wt", "name": "t3_1oe92wt", "content": "anyone here worked with a good insuretech software development partner before? trying to build something for a small insurance startup and dont want to waste time with generic dev shops that dont understand the industry side. open to recommendations or even personal experiences if you had a partner that actually delivered.", "date_utc": 1761239547.0, "title": "looking for a solid insuretech software development partner", "upvote_ratio": 0.94, "score": 15, "url": "https://www.reddit.com/r/dataengineering/comments/1oe92wt/looking_for_a_solid_insuretech_software/"}, {"id": "1odp2y6", "name": "t3_1odp2y6", "content": "Oh boy, somehow I got myself into the sweet ass job. I\u2019ve never held the title of Data Engineer however I\u2019ve held several other \u201cdata\u201d roles/titles. I\u2019m joining a small, growing digital marketing company here in San Antonio. Freaking JAZZED to be joining the ranks of Data Engineers. And I can now officially call myself a professional engineer! ", "date_utc": 1761178876.0, "title": "Just got hired as a Senior Data Engineer. Never been a Data Engineer", "upvote_ratio": 0.91, "score": 327, "url": "https://www.reddit.com/r/dataengineering/comments/1odp2y6/just_got_hired_as_a_senior_data_engineer_never/"}, {"id": "1of4w2i", "name": "t3_1of4w2i", "content": "\nAirflow 2.x \n\n**What did i learn :**\n\n- about airflow (what, why, limitation, features) \n- airflow core components\n  - scheduler\n  - executors\n  - metadata database\n  - webserver \n  - DAG processor \n  - Workers \n  - Triggerer\n  - DAG\n  - Tasks\n  - operators\n- airflow CLI ( list, testing tasks etc..) \n- airflow.cfg\n- metadata base(SQLite, Postgress) \n- executors(sequential, local, celery kubernetes) \n- defining dag (traditional way)\n- type of operators (action, transformation, sensor)\n- operators(python, bash etc..)\n- task dependencies \n- UI\n- sensors(http,file etc..)(poke, reschedule) \n- variables and connections\n- providers \n- xcom\n- cron expressions\n- taskflow api (@dag,@task)\n\n\n\n1. Any tips or best practices for someone starting out ?  \n  \n2- Any resources or things you wish you knew when starting out ?\n\nPlease guide me.  \nYour valuable insights and informations are much appreciated,  \nThanks in advance\u2764\ufe0f ", "date_utc": 1761329101.0, "title": "Week 1 of Learning Airflow", "upvote_ratio": 0.4, "score": 0, "url": "https://i.redd.it/y8p3hxgpn3xf1.png"}, {"id": "1oe4pp8", "name": "t3_1oe4pp8", "content": "I've been thinking about how crucial data quality is as our pipelines get more complex. With the rise of data lakes and various ingestion methods, it feels like there\u2019s a higher risk of garbage data slipping through.  \n  \nWhat strategies or tools are you all using to ensure data quality in your workflows? Are you relying on automated tests, manual checks, or some other method? I\u2019d love to hear what\u2019s working for you and any lessons learned from the process.", "date_utc": 1761229617.0, "title": "What strategies are you using for data quality monitoring?", "upvote_ratio": 0.88, "score": 18, "url": "https://www.reddit.com/r/dataengineering/comments/1oe4pp8/what_strategies_are_you_using_for_data_quality/"}, {"id": "1oe6h0b", "name": "t3_1oe6h0b", "content": "So currently, I'm a DE at a fairly large healthcare company, where my entire experience thus far has been in insurance and healthcare data. Problem is, I find healthcare REALLY boring. So I was wondering, how have you guys managed switching between domains?", "date_utc": 1761233647.0, "title": "How difficult is it to switch domains?", "upvote_ratio": 0.88, "score": 13, "url": "https://www.reddit.com/r/dataengineering/comments/1oe6h0b/how_difficult_is_it_to_switch_domains/"}, {"id": "1oep2w3", "name": "t3_1oep2w3", "content": "26M\n\nCurrently at a 1.5B valued private financial services company in a LCOL area. Salary is good. Team is small. More work that goes around than can be done. I have a long term project (go live expected March 1st 2026) I've made some mistakes and about a month past deadline. Some my fault, mostly we are catering to data requirements with data we simply dont have and have to create with lots of business logic. Overall, I have never had this happen and have been eating myself alive trying to finish it.\n\nManager said she recommended me for a senior postion with likely management positions to open. The referenced vendor in above paragraph where my work is a month late on has given me high praise.\n\nI am beginning 2nd stage hiring process with a spectator sports company (major NFL, NBA, NBA, NHL team). It is a 5k salary drop. Same job, similar benefits. Likely more of a demographic that matches my personality/age.\n\nIm conflicted, on one side I have a company that has said there is growth but I personally feel like im a failure.\n\nOn the other, there's a salary drop and no guarantee things are any better. Also, no guarantee I can grow.\n\nWhat would you do?? Losing sleep over all decisions and appreciate some direction.", "date_utc": 1761282159.0, "title": "Career Advice", "upvote_ratio": 0.4, "score": 0, "url": "https://www.reddit.com/r/dataengineering/comments/1oep2w3/career_advice/"}, {"id": "1oejf2i", "name": "t3_1oejf2i", "content": "Hi everyone,\n\nI need some advice on handling deletions occurring in source tables. Below are some of the tables in my data warehouse:\n\nhttps://preview.redd.it/nlen7esrbywf1.png?width=530&format=png&auto=webp&s=41d8358c9d9b58169d1a52d6d049f5dcba224500\n\nExam Table: This isn\u2019t a typical dimension table. Instead, it acts like a profile table that holds the source exam IDs and is used as a lookup to populate exam keys in other fact tables.\n\nLet\u2019s say the source system permanently deletes an exam ID (for example, DataSourceExamID = 123). How should I handle this in our data warehouse?\n\nI\u2019m thinking of updating the ExamKey value in Fact\\_Exam and Fact\\_Result to a default value like -1 that corresponds to Exam ID 123, and then deleting that Exam ID 123 row from the Exam table.\n\nI\u2019m not sure if this is even the correct approach. Also, considering that the ExamKey is used in many other fact tables, I don\u2019t think this is an efficient process, as I\u2019d have to check and update several fact tables before deleting. Marking the records in the Exam table is not an option for me.\n\nPlease suggest any best approaches to handle this.", "date_utc": 1761264619.0, "title": "How to Handle deletes in data warehouse", "upvote_ratio": 0.67, "score": 2, "url": "https://www.reddit.com/r/dataengineering/comments/1oejf2i/how_to_handle_deletes_in_data_warehouse/"}, {"id": "1odfg0o", "name": "t3_1odfg0o", "content": "Hey all,\n\nRecently there is increased concerns about the future of the dbt-core. To be honest regardless of the the fivetran acquisition, dbt-core never got any improvement over time. And it always neglected community contributions. \n\n[OpenDBT](https://github.com/memiiso/opendbt) fork is created to solve this problem. Enabling community to extend dbt to their own needs and evolve opensource version and make it feature rich.\n\n[OpenDBT](https://github.com/memiiso/opendbt) dynamically extends dbt-core. It's already adding significant features that aren't in the dbt-core. This is a path toward a complete community-driven fork.\n\nWe are inviting developers and the wider data community to collaborate. \n\nPlease check out the features we've already added, star the repo, and feel free to submit a PR! \n\n[https://github.com/memiiso/opendbt](https://github.com/memiiso/opendbt)", "date_utc": 1761155696.0, "title": "dbt-core fork: OpenDBT is here to enable community", "upvote_ratio": 0.97, "score": 349, "url": "https://www.reddit.com/r/dataengineering/comments/1odfg0o/dbtcore_fork_opendbt_is_here_to_enable_community/"}, {"id": "1oeflsb", "name": "t3_1oeflsb", "content": "At work (30-person B2B SaaS), we\u2019re currently debating evolving our data schema. The founders cobbled something together 10 years ago on AWS and through some patching and upgrading, we\u2019ve scaled to 10,000 users, typically sales reps.\n\nOne challenge we\u2019ve long faced is data analysis. We take raw JSON records from CRMs/VOIPs/etc, filter them using conditions, and turn them into performance records on another table. These \u201cpromoted\u201d JSON records are then pushed to RedShift where we can do some deeper analysis (such as connecting companies and contacts together, or tying certain activities back to deals, and then helping clients to answer more complex questions than \u201chow many meetings have my team booked this week?\u201d). Without going much deeper, going from performance records back to JSON records and connecting them to associated records but only those that have associated performance\u2026 Yeah, it\u2019s not great. \n\nThe evolved data schema we\u2019re considering is a star schema making use of our own model that can transform records from various systems into this model\u2019s common format. So \u201ccompany\u201d records from Salesforce, HubSpot, and half a dozen all CRMs are all represented relatively similarly (maybe a few JSON properties we\u2019d keep in a JSON column for display only). \n\nCurrent tables we\u2019re sat on are dimensions for very common things like users, companies, and contacts. Facts are for activities (calls, emails, meetings, tasks, notes etc) and deals.\n\nMy worry is that any case of a star schema being used that I\u2019ve come across has been for internal analytics - very rarely a multi-tenant architecture for customer data. We\u2019re prototyping with Tinybird which sits on top of Clickhouse. There\u2019s a lot of stuff for us to consider around data deletion, custom properties per integration and so on, but that\u2019s for another day. \n\nDoes this overall approach sit ok with you? Anything feel off or set off alarm bells? \n\nAppreciate any thoughts or comments!", "date_utc": 1761254516.0, "title": "Multi-tenant schema on Clickhouse - are we way off?", "upvote_ratio": 0.76, "score": 2, "url": "https://www.reddit.com/r/dataengineering/comments/1oeflsb/multitenant_schema_on_clickhouse_are_we_way_off/"}, {"id": "1odz8e9", "name": "t3_1odz8e9", "content": "Hi everyone, hope get some advice from you guys.\n\nRecently I joined a company where the current project I\u2019m working on goes like this:\n\nData lake store daily snapshots of the data source as it get updates from users and we store them in parquet files, partition by date. From there so far so good.\n\nIn dbt, our source points only to the latest file.\nThen we have an incremental model that:\nApply business logic , detected updated columns, build history columns (valid from valid to etc)\n\nMy issue: our history is only inside an incremental model , we can\u2019t do full refresh. The pipeline is not reproducible\n\nMy proposal: add a raw table in between the data lake and dbt \n\nBut received some pushback form business: \n1. We will never do a full refresh \n2. If we ever do, we can just restore the db backup \n3. You will increase dramatically the storage on the db \n4. If we lose the lake or the db, it\u2019s the same thing anyway \n5. We already have the data lake to need everything \n\nHow can I frame my argument to the business ? \n\nIt\u2019s a huge company with tons of business people watching the project burocracy etc.\n\nEDIT: my idea to create another table will be have a \u201cbronze layer\u201d raw layer whatever you want to call it to store all the parquet data, at is a snapshot , add a date column. With this I can reproduce the whole dbt project ", "date_utc": 1761213620.0, "title": "Argue dbt architecture", "upvote_ratio": 0.88, "score": 12, "url": "https://www.reddit.com/r/dataengineering/comments/1odz8e9/argue_dbt_architecture/"}, {"id": "1oe40en", "name": "t3_1oe40en", "content": "Is anyone here working with real HPC supercomputers?\n\n  \nMaybe you find my new project useful: [https://github.com/ascii-supply-networks/dagster-slurm/](https://github.com/ascii-supply-networks/dagster-slurm/) it bridges the domains of HPC and the convenience of data stacks from industry\n\n  \nIf you prefer slides over code: [https://ascii-supply-networks.github.io/dagster-slurm/docs/slides](https://ascii-supply-networks.github.io/dagster-slurm/docs/slides) here you go\n\n  \nIt is built around:\n\n  \n\\- [https://dagster.io/](https://dagster.io/) with [https://docs.dagster.io/guides/build/external-pipelines](https://docs.dagster.io/guides/build/external-pipelines)\n\n\\- [https://pixi.sh/latest/](https://pixi.sh/latest/) with [https://github.com/Quantco/pixi-pack](https://github.com/Quantco/pixi-pack)\n\n  \nwith a lot of glue to smooth some rough edges\n\n  \nWe have a script and ray (https://www.ray.io/) run launcher already implemented. The system is tested on 2 real supercomputers VSC-5 and Leonardo as well as our small CI-single-node SLURM machine.\n\n  \nI really hope some people find this useful. And perhaps this can path the way to a European sovereign GPU cloud by increasing HPC GPU accessibility.", "date_utc": 1761227959.0, "title": "bridging orchestration and HPC", "upvote_ratio": 0.71, "score": 4, "url": "https://www.reddit.com/r/dataengineering/comments/1oe40en/bridging_orchestration_and_hpc/"}, {"id": "1oe8w2c", "name": "t3_1oe8w2c", "content": "I am doing a migration to Salesforce from an external database. Client didn't provide any write access to create staging tables and instead said they have a mirror copy of production system db and fetch data from it for initial load based and fetch delta load based on the last run date(migration)and last modified date on records.\n\nI am unable to understand the risks of using it as in my earlier projects I have separate staging db and client used to refresh the data whenever we requested for.\n\nNeed opinions on the approach to follow", "date_utc": 1761239111.0, "title": "Delta load for migration", "upvote_ratio": 0.76, "score": 2, "url": "https://www.reddit.com/r/dataengineering/comments/1oe8w2c/delta_load_for_migration/"}, {"id": "1odk2ry", "name": "t3_1odk2ry", "content": "It feels to me that semantic layers are having a renaissance these days, largely driven by the need to enable AI automation in the BI layer. \n\nI'm trying to separate hype from signal and my feeling is that the community here is a great place to get help on that.\n\n  \nDo you currently have a semantic layer or do you plan to implement one? \n\nWhat's the primary reason to invest into one?\n\nI'd love to hear about your experience with semantic layers and any blockers/issues you have faced.\n\n  \nThank you!", "date_utc": 1761166082.0, "title": "What's the community's take on semantic layers?", "upvote_ratio": 0.98, "score": 62, "url": "https://www.reddit.com/r/dataengineering/comments/1odk2ry/whats_the_communitys_take_on_semantic_layers/"}, {"id": "1odqlcd", "name": "t3_1odqlcd", "content": "I\u2019ve been thinking a lot about how teams handle lineage when the stack is split across tools like dbt, Airflow, and Snowflake. It feels like everyone wants end-to-end visibility, but most solutions still need a ton of setup or custom glue.\n\nCurious what people here are actually doing. Are you using something like OpenMetadata or Marquez, or did you just build your own? What\u2019s working and what isn\u2019t?", "date_utc": 1761183210.0, "title": "How are you tracking data lineage across multiple platforms (Snowflake, dbt, Airflow)?", "upvote_ratio": 1.0, "score": 23, "url": "https://www.reddit.com/r/dataengineering/comments/1odqlcd/how_are_you_tracking_data_lineage_across_multiple/"}, {"id": "1odyoh5", "name": "t3_1odyoh5", "content": "Hello all!\n\nSo, background to my question is that I on my F2 capacity have the task of fetching data from a source, converting the parquet files that I receive into CSV files, and then uploading them to Google Drive through my notebook.\n\n  \nBut the issue that I first struck was that the amount of data downloaded was too large and crashed the notebook because my F2 ran out of memory (understandable for 10GB files). Therefore, I want to download the files and store them temporarily, upload them to Google Drive and then remove them.\n\nFirst, I tried to download them to a lakehouse, but I then understood that removing files in Lakehouse is only a soft-delete and that it still stores it for 7 days, and I want to avoid being billed for all those GBs...\n\nSo, to my question. ChatGPT proposed that I download the files into a folder like \"/tmp/\\*filename.csv\\*\", and supposedly when I do that I use the ephemeral memory created when running the notebook, and then the files will be automatically removed when the notebook is finished running.\n\nThe solution works and I cannot see the files in my lakehouse, so from my point of view the solution works. BUT, I cannot find any documentation of using this method, so I am curious as to how this really works? Have any of you used this method before? Are the files really deleted after the notebook finishes? Is there any better way of doing this? \n\n  \nThankful for any answers!\n\n\u00a0", "date_utc": 1761211520.0, "title": "Notebook memory in Fabric", "upvote_ratio": 0.83, "score": 4, "url": "https://www.reddit.com/r/dataengineering/comments/1odyoh5/notebook_memory_in_fabric/"}, {"id": "1oe1lou", "name": "t3_1oe1lou", "content": "My product would be exposing analytics dashboards and a notebook-style exploration interface to customers. Note that it is a multi-tenant application, and I want isolation at the data layer across different customers. My web app is currently running on Vercel, and looking for options for a good cloud data warehouse that integrates well with Vercel. While I am currently using Postgres, my needs are better suited for an OLAP datababase so I am curious if this is still the best option. What are the good options on Vercel for this?\n\nI looked at Motherduck and looks like it is a good option, but one challenge I am seeing is that the WASM client would be exposing the tokens to the customer. Given that it is a multi-tenant applications, I would need to create a user per tennant and do that user management myself. If I go with MotherDuck, my alternative is to move my webapp to a proper nodejs deployment where I don't need to depend on WASM client. Its doable but a lot of overhead to manage.\n\nThis seems like a problem that should already be solved in 2025, AGI is around the corner, this should be easy :D . So curious, what are some other good options out for this?", "date_utc": 1761221523.0, "title": "Data warehouse options for building customer-facing analytics on Vercel", "upvote_ratio": 0.67, "score": 2, "url": "https://www.reddit.com/r/dataengineering/comments/1oe1lou/data_warehouse_options_for_building/"}, {"id": "1oe5sx2", "name": "t3_1oe5sx2", "content": "I'm a senior dev/solution architect working at a decent size consulting company. I'm conflicted because I just received an offer from another much smaller consulting company with the promise of working on new client projects and working with a variety of tools, one of which is snowflake (which I have a great deal of experience with - I'm snowflake certified fyi). This new company is a snowflake elite partner and is being given lots of new client work.  \nHowever my manager just told me as of yesterday that my role is going to change and I'm going to get to drop my current client projects in order to learn/leverage palantir for some of our sister companies. This has me intrigued because I've been very interested in Palantir and what they have to offer compared to the other big cloud based companies. Likewise my company would match my current offer and allow me a change of pace so I don't have to support my current clients any longer (which I was getting tired of in the first place).  \nThe issue is I genuinely enjoy my current company and my manager is probably one of the best guys I've had to report to.   \nI have to make a decision ASAP. Anyone have thoughts, specifically about working with Palantir? My background is data analytics and warehousing/modeling and Palantir seems like it's really growing (would be good to have on my res). Thoughts?", "date_utc": 1761232103.0, "title": "Opportunity to learn/use Palantir vs leaving for another consultancy?", "upvote_ratio": 0.4, "score": 0, "url": "https://www.reddit.com/r/dataengineering/comments/1oe5sx2/opportunity_to_learnuse_palantir_vs_leaving_for/"}, {"id": "1odwup9", "name": "t3_1odwup9", "content": "After yesterday's thread about non-prod data being a nightmare, it turns out loads of you are also secretly using prod because everything else is broken. I am quite new to this posting thing, always been a bit of lurker, but it was really quite cathartic, and very useful. \n\nHalloween's round the corner, so time for some therapeutic actual horror stories. \n\nI'll start: Recently spent three days debugging why a customer's transactions weren't summing correctly in our dev environment. Turns out our snapshot was six weeks old, and the customer had switched payment processors in that time. \n\nThe data I was testing against literally couldn't produce the bug I was trying to fix. \n\nLet's hear them.", "date_utc": 1761204262.0, "title": "Horror Stories (cause you know, Halloween and all) - I'll start", "upvote_ratio": 0.86, "score": 5, "url": "https://www.reddit.com/r/dataengineering/comments/1odwup9/horror_stories_cause_you_know_halloween_and_all/"}, {"id": "1ody8jg", "name": "t3_1ody8jg", "content": "Hi,\n\nI come from a Azure background and am very new to GCP. I have a requirement to copy some tables from BiqQuery to an on-prem SQL server. The existing pipeline is in cloud composer.  \nCan someone help with what steps should I do to make it happen? what are the permissions and configurations that need be set at the SQL server. Thanks in advance.", "date_utc": 1761209791.0, "title": "BiqQuery to on-prem SQL server", "upvote_ratio": 0.67, "score": 2, "url": "https://www.reddit.com/r/dataengineering/comments/1ody8jg/biqquery_to_onprem_sql_server/"}, {"id": "1odtiql", "name": "t3_1odtiql", "content": "Hello, my background is mostly Cloudera (on-prem) and AWS (EMR and Refshift). \n\nI\u2019m trying to read the docs, and see some youtube tutorials, but nothing helps. I followed the docs but its mostly just clickops.\n\nI may move to a new job, and this is their stack. \n\nWhat I\u2019m struggling is that I\u2019m used to a typical architecture;\n\nI have a job that replicates data to HDFS/S3\nUse Apache Spark/Hive to transform data \nConnect BI tool to Hive/Impala/Redshift\n\nFabric is quite overwhelming. I feel like it is doing a whole lot of things and I don\u2019t know where to get started. ", "date_utc": 1761192107.0, "title": "Get started with Fabric", "upvote_ratio": 0.65, "score": 4, "url": "https://www.reddit.com/r/dataengineering/comments/1odtiql/get_started_with_fabric/"}, {"id": "1odlsyu", "name": "t3_1odlsyu", "content": "If anyone wants to run some science fair experiments with Iceberg v3 features like binary deletion vectors, the variant datatype, and row-level lineage, I stood up a hands-on tutorial at [https://lestermartin.dev/tutorials/trino-iceberg-v3/](https://lestermartin.dev/tutorials/trino-iceberg-v3/) that I'd love to get some feedback on.\n\nYes, I'm a Trino DevRel at Starburst and YES... this currently only runs on Starburst, BUT today our CTO announced publicly at our Trino Day conference that will are going to commit these changes back to the open-source Trino Iceberg connector.\n\nCan't wait to do some interoperability tests with other engines that can read/write Iceberg v3. Any suggestions what engine I should start with first that has announced their v3 support?", "date_utc": 1761170190.0, "title": "hands-on Iceberg v3 tutorial", "upvote_ratio": 0.87, "score": 10, "url": "https://www.reddit.com/r/dataengineering/comments/1odlsyu/handson_iceberg_v3_tutorial/"}, {"id": "1odiglr", "name": "t3_1odiglr", "content": "HTAP isn't a new concept, it has been called out by Garnter as a trend already in 2014. Modern cloud platforms like Snowflake provide HTAP solutions like Unistore and there are other vendors such as Singlestore. Now I have seen that MariaDB announced a new solution called [MariaDB Exa](https://www.exasol.com/blog/mariadb-exa-announcement/) together with Exasol. So it looks like there is still appetite for new solutions. My question: do you see these kind of hybrid solutions in your daily job or are you rather building up your own stacks with proper pipelines between best of breed components?", "date_utc": 1761162419.0, "title": "Is HTAP the solution for combining OLTP and OLAP workloads?", "upvote_ratio": 0.94, "score": 14, "url": "https://www.reddit.com/r/dataengineering/comments/1odiglr/is_htap_the_solution_for_combining_oltp_and_olap/"}, {"id": "1odains", "name": "t3_1odains", "content": "Just finished a working version of a dockerized dataplatform using Ducklake! My friend has a startup and they had a need to display some data so I offered him that I could build something for them.\n\nThe idea was to use Superset, since that's what one of their analysts has used before. Superset seems to also have at least some kind of support for Ducklake, so I wanted to try that as well.\n\nSo I set up an EC2 where I pull a git repo and then spin up few docker compose services. First service is postgres that acts as a metadata for both Superset and Ducklake. Then Superset service spins up nginx and gunicorn that run the BI layer.\n\nActual ETL can be done anywhere on the EC2 (or Lambdas if you will) but basically I'm just pulling data from open source API's, doing a bit of transformation and then pushing the data to Ducklake. Storage is S3 and Ducklake handles the parquet files there.\n\nSuperset has access to the Ducklake metadata DB and therefore is able to access the data on S3.\n\nTo my surprise, this is working quite nicely. The only issue seems to be how Superset displays the schema of the Ducklake, as it shows all the secrets of the connection URI (:\n\nI don't want to publish the git repo as it's not very polished, but I just wanted to maybe raise discussion if anyone else has tried something similar before?\nThis sure was refreshing and different than my day to day job with big data.\n\nAnd if anyone has any questions regarding setting this up, I'm more than happy to help!", "date_utc": 1761144909.0, "title": "Ducklake on AWS", "upvote_ratio": 0.9, "score": 27, "url": "https://www.reddit.com/r/dataengineering/comments/1odains/ducklake_on_aws/"}, {"id": "1odgz38", "name": "t3_1odgz38", "content": "Hi \n\nI need some help. I have some sports data from different athletes, where I need to consider how and where we will analyse the data. They have data from training sessions the last couple of years in a database, and we have the API's. They want us to visualise the data and look for patterns and also make sure, that they can use, when we are done. We have around 60-100 hours to execute it. \n\nMy question is what platform should we use\n\n\\- Build a streamlit app? \n\n\\- Build a power BI dashboard? \n\n\\- Build it in Databricks\n\nAre there other ways to do it?  \n  \n They need to pay for hosting and operation, so we also need to consider the costs for them, since they don't have that much.  ", "date_utc": 1761159081.0, "title": "Help for hosting and operating sports data via API", "upvote_ratio": 0.94, "score": 14, "url": "https://www.reddit.com/r/dataengineering/comments/1odgz38/help_for_hosting_and_operating_sports_data_via_api/"}, {"id": "1odqg2u", "name": "t3_1odqg2u", "content": "How to get better at system design in data engineering? Are there any channels, books or websites(like leetcode) that I can look up? Thanks ", "date_utc": 1761182793.0, "title": "System design", "upvote_ratio": 0.72, "score": 3, "url": "https://www.reddit.com/r/dataengineering/comments/1odqg2u/system_design/"}, {"id": "1odpk4g", "name": "t3_1odpk4g", "content": "As per title. Most of the data I'm working with for this particular project involves ingesting data directly from \\*\\*xlsx\\*\\* files and there is a lot of information security concerns (eg. they have no API to expose the client data, they would much rather have an admin person do the exporting directly from the CRM portal manually).\n\n\n\nIn these cases, \n\n\n\n1) what are the modern practices for creating analytics tools? As in libraries, workflows, or pipelines. For user-side tools, would Jupyter notebooks be applicable or should it be a fully baked app (whatever tech stack that entails)? I am concerned about hardcoding certain graphing functions too early (losing flexibility). What is common industry practice?\n\n\n\n2) Is there a point in trying to get them to migrate over to PostGres or MySQL? My instinct is that I should just accept the xlsx file as input (maybe make suggestions on specific changes for the table format) but while I came in initially to help them automate and streamline, I feel I have more value add on the visualization front due to the heavily low-tech nature of the org.\n\n\n\nHelp?", "date_utc": 1761180242.0, "title": "Doing Analytics/Dashboards for Excel-Heavy workflows", "upvote_ratio": 0.76, "score": 4, "url": "https://www.reddit.com/r/dataengineering/comments/1odpk4g/doing_analyticsdashboards_for_excelheavy_workflows/"}, {"id": "1odgtlh", "name": "t3_1odgtlh", "content": "Our EMR (spark) cost crossed 100K annually. I want to start leveraging spot and reserve instances. How to get started and what type of instance should I choose for spot instances? Currently we are using on-demand r8g machines.", "date_utc": 1761158748.0, "title": "EMR cost optimization tips", "upvote_ratio": 1.0, "score": 10, "url": "https://www.reddit.com/r/dataengineering/comments/1odgtlh/emr_cost_optimization_tips/"}, {"id": "1odslm9", "name": "t3_1odslm9", "content": "Hi,there!\nI'm making OSS of visialization from SQL.\n\uff08Just SQL to any grid or table\uff09\nNow,I'll try to add feature.\nLet me know about your thoughts!\n", "date_utc": 1761189170.0, "title": "Making SQL to Viz tools", "upvote_ratio": 0.76, "score": 2, "url": "https://github.com/nkwork9999/sql2viz"}, {"id": "1od40jc", "name": "t3_1od40jc", "content": "Patterns of possible automated bot activity promoting several vendors across r/dataengineering and broader Reddit have been detected.\n\nEasy way to find dozens of bot accounts:  Find one shilling a bunch of tools then search these tools together.\n\nHere's an [example query](https://www.reddit.com/search/?q=airbyte+dreamfactory&type=comments&sort=new) or[ this one](https://www.reddit.com/search/?q=airbyte+windsor&type=comments&sort=new) which find **dozens of bot users and hundreds of comments**. When pasting these comments to an LLM it will immediately identify patterns and highlight which vendors are being shilled with what tactic.\n\nCommunity: **stay alert and report suspected bots. Tell your vendor if on the list that their tactics are backfiring.** When buying, consider vendor ethics, not just product features.\n\n# Consequences exist! All it takes some pissed off reports.\n\nLuckily astroturfing is illegal in all of the countries where these vendors are based.[ ](https://www.bloomberg.com/news/articles/2013-09-25/operation-clean-turf-and-the-war-on-fake-yelp-reviews)\n\n[Here's what happened in 2013 to vendors with deceptive practise in sting operation \"clean turf\".](https://www.bloomberg.com/news/articles/2013-09-25/operation-clean-turf-and-the-war-on-fake-yelp-reviews) Founders and their CEOS were publicly named and shamed in major news outlets, like The Guardian, for personally orchestrating the fraud. Individuals were personally fined and forced to sign legally binding \"assurance of discontinuance\", in some cases prohibiting them from starting companies again.\n\nFor the 19 companies, the founders/owners were forced to personally pay fines ranging from $2,500 to just under $100,000 and sign an \"Assurance of Discontinuance,\" legally binding them to stop astroturfing.\n\n# Reddit context\n\nA\u00a0[Reddit ban on AI bot research](https://www.reddit.com/r/Switzerland/comments/1kczxvw/reddit_bans_researchers_who_used_ai_bots_to/)\u00a0shows how seriously this is taken. If that's \"a highly unethical experiment\" then **doing it for money instead of science is so much worse.**", "date_utc": 1761126935.0, "title": "PSA: Coordinated astroturfing campaign using LLM\u2013driven bots to promote or manipulate SEO and public perception of several software vendors", "upvote_ratio": 0.93, "score": 47, "url": "https://www.reddit.com/r/dataengineering/comments/1od40jc/psa_coordinated_astroturfing_campaign_using/"}, {"id": "1od2x2k", "name": "t3_1od2x2k", "content": "I recently ran into all sorts of pain working directly with raw Parquet files for an analytics project  broken schemas, partial writes, and painfully slow scans.   \nThat experience made me realize something simple: Parquet is *just* a storage format. It\u2019s great at compression and column pruning, but that\u2019s where it ends. No ACID guarantees, no safe schema evolution, no time travel, and a whole lot of chaos when multiple jobs touch the same data.\n\nThen I explored open table formats like Apache Iceberg, Delta Lake, and Hudi  and it was like adding a missing layer of order on top  impressive is what they are bringing in \n\n* ACID transaction**s** through atomic metadata commits\n* Schema evolution without having to rewrite everything\n* for easy rollbacks and historical analysis we have Time travel \n* you can scan millions of files in milliseconds by Manifest indexing another cool thing \n* not to forget the hidden partitions \n\nIn practice, these features made a *huge* difference  reliable BI queries running on the same data as streaming ETL jobs, painless GDPR-style deletes, and background compaction that keeps things tidy.\n\nBut it does make you think  is that extra metadata layer really worth the added complexity?  \n Or can clever workarounds and tooling keep raw Parquet setups running just fine at scale?\n\nWrote a blog on this that i am sharing here looking forward to your thoughts ", "date_utc": 1761122771.0, "title": "Parquet vs. Open Table Formats: Worth the Metadata Overhead?", "upvote_ratio": 0.92, "score": 48, "url": "https://olake.io/blog/iceberg-vs-parquet-table-format-vs-file-format"}, {"id": "1odic7i", "name": "t3_1odic7i", "content": "I am confused about Astronomer cosmos CLI.  When I sign up for the tutorials on their website I get hounded by Sales ppl who go radio silent once they hear I am just a minion with no budget to purchase anything. \n\nSo I want to run my Dbt Core projects and it seems like everyone in the community uses Airflow for orchestration.  Is it possible or worthwhile to use AstroCli (free version) in Airflow in production or do you have to pay for using the product outside of the local host?   Does anyone see a benefit to using Astronomer over just Airflow? \n\nWhat do you think of the tool?  Or is it easier to just dbt in Snowflakes dbt projects??? \n\nSorry if this question is stupid, I just get confused by these softwares that are free and paid as to what is for what. ", "date_utc": 1761162143.0, "title": "Astronomer Cosmos CLI", "upvote_ratio": 0.78, "score": 5, "url": "https://www.reddit.com/r/dataengineering/comments/1odic7i/astronomer_cosmos_cli/"}, {"id": "1od6qy5", "name": "t3_1od6qy5", "content": "Had a situation last week where PII leaked into our analytics sandbox because manual masking missed a few fields. Took half a day to track down which tables were affected and get it sorted. Not the first time either. \n\nGot me thinking about how much time actually goes into just getting clean, compliant data into non-prod environments. \n\nEvery other thread here mentions dealing with inconsistent schemas, manual masking workflows, or data refreshes that break dev environments. \n\nFor those managing dev, staging, or analytics environments, how much of your week goes to this stuff vs actual engineering work? And has this got worse with AI projects? \n\nFeels like legacy data issues that teams ignored for years are suddenly critical because AI needs properly structured, clean data. \n\nCurious what your reality looks like. Are you automating this or still doing manual processes?", "date_utc": 1761135728.0, "title": "How much time are we actually losing provisioning non-prod data", "upvote_ratio": 0.94, "score": 23, "url": "https://www.reddit.com/r/dataengineering/comments/1od6qy5/how_much_time_are_we_actually_losing_provisioning/"}, {"id": "1odgd30", "name": "t3_1odgd30", "content": "Whether it's databricks, snowflake, etc.\n\nOf the platforms you use, what are the features that have actually made you more productive vs. being something that got you excited but didn't actually change how you do things much.", "date_utc": 1761157722.0, "title": "What Platforms Features have Made you a more productive DE", "upvote_ratio": 1.0, "score": 6, "url": "https://www.reddit.com/r/dataengineering/comments/1odgd30/what_platforms_features_have_made_you_a_more/"}, {"id": "1odcr8u", "name": "t3_1odcr8u", "content": "I have for years worked in different roles related to data. A loss of job recently as a data analyst got me thinking about what I really wanted. I started reading up on many different paths and chose Data Governance. I armed myself with the necessary certifications and started dipping my toe into the job market. When I look at the skills section, I meet most but not all requirements. The problem however is that most of these job descriptions ask for 5 to 10 years of experience in a data governance related role. If you work in this space, how did you get your foot in the door? ", "date_utc": 1761149836.0, "title": "How do you get your foot in the door for a role in data governance?", "upvote_ratio": 0.71, "score": 4, "url": "https://www.reddit.com/r/dataengineering/comments/1odcr8u/how_do_you_get_your_foot_in_the_door_for_a_role/"}, {"id": "1od55tg", "name": "t3_1od55tg", "content": "Because SQL Server is not possible to install and maybe you have other DDBB in Amazon or Oracle", "date_utc": 1761130937.0, "title": "What's the best database IDE for Mac?", "upvote_ratio": 0.87, "score": 15, "url": "https://www.reddit.com/r/dataengineering/comments/1od55tg/whats_the_best_database_ide_for_mac/"}, {"id": "1odmqvh", "name": "t3_1odmqvh", "content": "How do I set up secure way of accessing secrets in the DAGS, considering multiple teams will be working on their own Airflow Env.\nThese credentials must be accessed very securely. I know we can use secrets manager and call secrets using sdks like boto3 or something. Just want best possible way to handle this", "date_utc": 1761172595.0, "title": "Airflow secrets setup", "upvote_ratio": 0.5, "score": 0, "url": "https://www.reddit.com/r/dataengineering/comments/1odmqvh/airflow_secrets_setup/"}, {"id": "1odgfnc", "name": "t3_1odgfnc", "content": "", "date_utc": 1761157878.0, "title": "How to address query performance challenges in Snowflake", "upvote_ratio": 0.75, "score": 2, "url": "https://www.capitalone.com/software/blog/addressing-query-performance-challenges-snowflake/?utm_campaign=fordev&utm_source=reddit&utm_medium=social-organic"}, {"id": "1ocz7qu", "name": "t3_1ocz7qu", "content": "I recently joined my company, and they currently run dbt jobs using AWS Step Functions and a Fargate task that executes the project, and so on.\n\nHowever, I\u2019m not sure if this is the best approach to orchestrate dbt jobs. Another important point is that the company manages most workflows through events following a DDD (Domain-Driven Design) pattern.\n\nRight now, there\u2019s a case where a process depends on two different Step Functions before triggering another process.\nThe challenge is that these Step Functions run at different times and don\u2019t depend on each other.\nAdditionally, in the future, there might be other processes that depend on those same Step Functions, but not necessarily on this one\n\nIn my opinion, Airflow doesn\u2019t fit well here.\n\nWhat do you think would be a better way to manage these processes? Would it make sense to build something more custom for these types of cases", "date_utc": 1761108844.0, "title": "What is the best way to orchestrate dbt job in aws", "upvote_ratio": 0.8, "score": 11, "url": "https://www.reddit.com/r/dataengineering/comments/1ocz7qu/what_is_the_best_way_to_orchestrate_dbt_job_in_aws/"}, {"id": "1od2c18", "name": "t3_1od2c18", "content": "", "date_utc": 1761120465.0, "title": "Data Modeling for the Agentic Era: Semantics, Speed, and Stewardship", "upvote_ratio": 0.64, "score": 3, "url": "https://www.rilldata.com/blog/data-modeling-for-the-agentic-era-semantics-speed-and-stewardship"}, {"id": "1ocst3a", "name": "t3_1ocst3a", "content": "Hey all I could really use some career advice from this community.\n\nI was fortunate to land 2 offers in this market, but now I\u2019m struggling to make the right long term decision.\n\nI\u2019m finishing my Master\u2019s in Data Science next semester. I interned last summer at a big company and then started working in my first FT data role as a data analyst at a small company (I\u2019m about 6 months in). My goal is to eventually move into Data Science/ML maybe ML engineer and end up in big tech.\n\nOption A: Data Engineer I\n* Industry: Finance. This one pays $15k more. I\u2019ll be working with a smaller team and I\u2019d be the main technical person on the team. So no strong mentorship and I\u2019ll have the pressure to \u201cfigure it out\u201d on my own. \n\nOption B: Senior Data Analyst\n* Industry: retail at a large org.\n\nI\u2019m nervous about being the only engineer on a team this early in my career\u2026But I\u2019m also worried about not being technical enough as a data analyst and not being technical. \n\nWhat would you do in my shoes?\nGo hard into engineering now and level up fast even if it\u2019s stressful without much support? Or take the analyst role at a big company, build brand and transition later?\n\nWould appreciate any advice from people who\u2019ve been on either path.\n", "date_utc": 1761090065.0, "title": "Need advice choosing between Data engineer vs Sr Data analyst", "upvote_ratio": 0.79, "score": 15, "url": "https://www.reddit.com/r/dataengineering/comments/1ocst3a/need_advice_choosing_between_data_engineer_vs_sr/"}, {"id": "1ocy398", "name": "t3_1ocy398", "content": "", "date_utc": 1761105202.0, "title": "The Death of Thread Per Core", "upvote_ratio": 0.88, "score": 6, "url": "https://buttondown.com/jaffray/archive/the-death-of-thread-per-core/"}, {"id": "1od6jn2", "name": "t3_1od6jn2", "content": "I'm a Data Quality Analyst for a Public Sector company based in the UK\n\nWe're an MS Stack company and have decided to go down the route of Purview for Data Governance. Split down the middle I'm aligned with Data Quality/Health/Diagnosis etc and our IT team is looking after policies and governance.\n\nLooking at Purviews latest pricing model I've done about as much research as I can and trying to use Purviews Pricing Calculator but getting some crazy figures.\n\nIn our proof of concept task we have 31 assets (31 tables from a specific schema in Azure SQL DB) will be running a scan every week and will need to use the Standard SKU for Data Quality as I want our rules to be dynamic and reflect business logic. \n\nThis is where it gets tricky. Using AI I tried to figure out how many DGPU (Data Governance Processing Units) would be needed to do the math. This came out at 250 units which seems huge and reflected in the cost of \u00a315,000 a month.\n\nThis seems an insane cost considering it's a proof of concept with not very many assets which we plan on growing the size of the assets.\n\nHas anyone any experience with this and could possibly help out because I am losing the plot a bit.\n\n  \nThanks in advance", "date_utc": 1761135159.0, "title": "MS Purview Pricing", "upvote_ratio": 0.5, "score": 0, "url": "https://www.reddit.com/r/dataengineering/comments/1od6jn2/ms_purview_pricing/"}, {"id": "1ocqznw", "name": "t3_1ocqznw", "content": "Has anyone ever had any success using tools like DataBricks Lakebridge or Snowflake's SnowConvert to migrate Informatica powercenter ETL pipelines to another platform? I assume at best they \"kind of work sometimes for some things\" but am curious to hear anyone's actual experience with them in the wild.", "date_utc": 1761085359.0, "title": "Tools for automated migration away from Informatica", "upvote_ratio": 0.9, "score": 7, "url": "https://www.reddit.com/r/dataengineering/comments/1ocqznw/tools_for_automated_migration_away_from/"}, {"id": "1occxjl", "name": "t3_1occxjl", "content": "Classic story: you should not work directly in prod, but apply the best devops practices, develop data pipelines in a development environment, debug, deploy in pre-prod, test, then deploy in production.\n\nWhat about data analysts? data scientists? statisticians? ad-hoc reports?\n\nMost data books focus on the data engineering lifecycle, sometimes they talk about the \"Analytics sandbox\", but they rarely address heads-on the personas doing analytics work in production. Modern data platform allow the decoupling of compute and data, enabling workload isolation to allow users read-only access to production data without affecting production workloads. Other teams perform data replication from production to lower environments. There's also the \"blue-green development architecture\", with two systems with production data.\n\nHow are you dealing with users requesting production data?", "date_utc": 1761053214.0, "title": "Developing with production data: who and how?", "upvote_ratio": 0.89, "score": 26, "url": "https://www.reddit.com/r/dataengineering/comments/1occxjl/developing_with_production_data_who_and_how/"}, {"id": "1ocie74", "name": "t3_1ocie74", "content": "Hope y'all find it useful!", "date_utc": 1761065889.0, "title": "Our 7 Snowflake query optimization tips and why they work", "upvote_ratio": 0.82, "score": 12, "url": "https://blog.greybeam.ai/snowflake-query-optimization/"}, {"id": "1od0cqr", "name": "t3_1od0cqr", "content": "Does anyone have experience with jOOQ (https://github.com/jOOQ/jOOQ) as a transpiler between two different SQL dialects? We are searching for options in Java to run queries from other dialects on Exasol without the users having to rewrite them.", "date_utc": 1761112892.0, "title": "Anyone experienced with jOOQ as SQL transpiler?", "upvote_ratio": 0.5, "score": 0, "url": "https://www.reddit.com/r/dataengineering/comments/1od0cqr/anyone_experienced_with_jooq_as_sql_transpiler/"}, {"id": "1oc1w56", "name": "t3_1oc1w56", "content": "Our team has been running nightly batch ETL for years and it works fine, but product leadership keeps asking if we should move \u201ceverything\u201d to real-time. The argument is that fresher data could help dashboards and alerts, but honestly, I\u2019m not sure most of those use cases need second-by-second updates.\n\nWe\u2019ve done some early tests with Kafka and Debezium for CDC, but the overhead is real, more infrastructure, more monitoring, more cost. I\u2019m trying to figure out what the actual decision criteria should be.\n\nFor those who\u2019ve made the switch, what tipped the scale for you? Was it user demand, system design, or just scaling pain with batch jobs? And if you stayed with batch, how do you justify that choice when \u201creal-time\u201d sounds more exciting to leadership?", "date_utc": 1761015772.0, "title": "How do you decide when to move from batch jobs to real-time pipelines?", "upvote_ratio": 0.96, "score": 96, "url": "https://www.reddit.com/r/dataengineering/comments/1oc1w56/how_do_you_decide_when_to_move_from_batch_jobs_to/"}, {"id": "1ocvodq", "name": "t3_1ocvodq", "content": "https://preview.redd.it/h4k1298ikkwf1.png?width=1744&format=png&auto=webp&s=ebf666f0ad1f466edc7e8802e4087f65c35dfd4c\n\n[https://codepen.io/gangtao/full/raxdOOK](https://codepen.io/gangtao/full/raxdOOK) ", "date_utc": 1761098042.0, "title": "Data Streaming Delivery Semantics", "upvote_ratio": 0.67, "score": 1, "url": "https://www.reddit.com/r/dataengineering/comments/1ocvodq/data_streaming_delivery_semantics/"}, {"id": "1oca4c7", "name": "t3_1oca4c7", "content": "Hello,  \nI have worked for a few years as an analyst (self taught) and now I am trying to get into data engineering. I am trying to simply understand how to structure a DWH using\u00a0**medallion architecture**\u00a0(Bronze \u2192 Silver \u2192 Gold) across\u00a0**multiple environments**\u00a0(Dev / Test / Prod).\n\nNow, with the last company I worked with, they simply had two databases, staging, and production. Staging is basically the data lake and they transformed all the data to production. I understand this is not best practice.\n\nI thought if I wanted to have a proper structure in my DWH, I was thinking of this:\n\nDWH |\n\n\\-> DevDB -> BronzeSchema, SilverSchema, GoldSchema\n\n\\-> TestDB -> BronzeSchema, SilverSchema, GoldSchema\n\n\\-> ProdDB -> BronzeSchema, SilverSchema, GoldSchema\n\nWould you even create a bronze layer on dev and test DBs or not really? I mean it is just the raw data no?", "date_utc": 1761045229.0, "title": "Help a noob: CI/CD pipelines with medallion architecture", "upvote_ratio": 0.9, "score": 16, "url": "https://www.reddit.com/r/dataengineering/comments/1oca4c7/help_a_noob_cicd_pipelines_with_medallion/"}, {"id": "1ocksmo", "name": "t3_1ocksmo", "content": "Thoughts on using Synthetic Data for Projects ?\n\nI'm currently a DB Specialist with 3 YOE learning Spark, DBT, Python, Airflow and AWS to switch to DE roles.\n\nI\u2019d love some feedback on a portfolio project I\u2019m working on. It\u2019s basically a modernized spin on the kind of work I do at my job, a Transaction Data Platform with a multi-step ETL pipeline.\n\nQuick overview of setup:\n\nDB structure:\n\nDimensions = Bank -> Account -> Routing\n\nFact = Transactions -> Transaction\\_Steps\n\nHistory = Hist_Transactions -> Hist_Transaction_Steps (identical to fact tables, just one extra column) \n\nI mocked up 3 regions -> 3 banks per region -> 3 accounts per bank -> 702 unique directional routings.\n\nA Python script first assigns following parameters to each routing:\n\ntype (High Intensity/Frequency/Normal)\n\ncountry\\_code, region, cross\\_border\n\nbase\\_freq, base\\_amount, base\\_latency, base\\_success\n\nvolatility vars (freq/amount/latency/success)\n\nThen the synthesizer script uses above paramters to spit out 85k-135k records per day, and 5x times Transaction\\_Steps \n\nAnomaly engine randomly spikes volatility (50\u2013250x) \\~5 times a week for a random routing, the aim is (hopefully) the pipeline will detect the anomalies.\n\nPipeline workflow:\n\nBatch runs daily (simulating off business hours migration).\n\nEvery day data older than 1 month in live table is moved to history tables (partitioned by day and OLTP compressed) \n\nThen the partitions older than a month in history tables are exported to Parquet (maybe I'll create a Data lake or something) cold storage and stored. \n\nThe current day's transactions are transformed through DBT, to generate 12 marts, helping in anomaly detection and system monitoring \n\nA Great Expectation + Python layer takes care of data quality and Anomaly detection\n\nFinally for visualization and ease of discussion I'm generating a streamlit dashboard from above 12 marts.\n\nMain concerns/questions:\n\n1. Since this is just inspired by my current work (I didn\u2019t use real table names/logic, just the concept), should I be worried about IP/overlap ?\n2. I\u2019ve done a barebones version of this in shell+SQL, so I personally know business and technical requirements and possible issues in this project, it feels really straightforward.  Do you think this is a solid enough project to showcase for DE roles at product-based-companies / fintechs (0\u20133 YOE range)?\n3. Thoughts on using synthetic data? I\u2019ve tried to make it noisy and realistic, but since I\u2019ll always have control, I feel like I'm missing something critical that only shows up in real-world messy data?\n\nWould love any outside perspective\n\nThis would ideally be the portfolio project, and there's one more planned using spark where I'm just cleaning and merging Spotify datasets from different types (CSV, json, sqlite, parquet etc) from Kaggle, it's just a practice project to showcase spark understanding. \n\n**TLDR:**  \nBuilt a synthetic transaction pipeline (750k+ txns, 3.75M steps, anomaly injection, DBT marts, cold storage). Looking for feedback on:\n\n* IP concerns (inspired by work but no copied code/keywords)\n* Whether it\u2019s a strong enough DE project for Product Based Companies and Fintech. \n* Pros/cons of using synthetic vs real-world messy data", "date_utc": 1761071216.0, "title": "Thoughts on Using Synthetic Tabular data for DE projects ?", "upvote_ratio": 0.67, "score": 3, "url": "https://www.reddit.com/r/dataengineering/comments/1ocksmo/thoughts_on_using_synthetic_tabular_data_for_de/"}, {"id": "1oc3vf9", "name": "t3_1oc3vf9", "content": "Hi subreddit. \n\nI\u2019ve been tipping my toes back in the job search; one thing I see this round I didn\u2019t see 3 years ago is that Terraform/IaC is required by almost every job. \n\nThought I could get away without it - was invited for an interv for job, but then they cancelled due to lack of IaC experience. \n\nIs this really the common expectation now? I\u2019ll spend some time learning it but really suprised by this outcome. ", "date_utc": 1761022042.0, "title": "IaC a prerequisite for DE?", "upvote_ratio": 0.86, "score": 19, "url": "https://www.reddit.com/r/dataengineering/comments/1oc3vf9/iac_a_prerequisite_for_de/"}, {"id": "1oco9fs", "name": "t3_1oco9fs", "content": "Hi all,\n\nI have some room booking data I need to do some time-related calculations with using Power Bi.\n\n1st table has room bookings data with room name, meeting start date time, meeting end date time, snapshot\\_date, etc.\n\nAs part of my ETL I am already building the snapshot\\_date rows based on the meeting start date time and meeting end date time.\n\n2nd table has room occupancy data which has room name, start date time, stop date time and usage which are in hour buckets.\n\nI have a dim date table connected to snapshot\\_date in the room bookings table and start date time in the room occupancy table.\n\nQuestion is do I need to have my room bookings data at the same time granularity (hourly) as the room occupancy data to make life easier with time calculations moving forward.\n\nCheers", "date_utc": 1761078928.0, "title": "Date time granularity", "upvote_ratio": 0.5, "score": 0, "url": "https://www.reddit.com/r/dataengineering/comments/1oco9fs/date_time_granularity/"}, {"id": "1ocalca", "name": "t3_1ocalca", "content": "Has anyone tried using PR review tools like **CodeRabbit** or **Greptile** for data engineering workflows (dbt, Airflow, Snowflake, etc.)?\n\nIf anyone can share theier experience on if they can handle things like schema changes, query optimization, or data quality checks well, or if they\u2019re more tuned for general code reviews (which I m mostly expecting).", "date_utc": 1761046763.0, "title": "Any good PR review tools for data stacks?", "upvote_ratio": 0.75, "score": 4, "url": "https://www.reddit.com/r/dataengineering/comments/1ocalca/any_good_pr_review_tools_for_data_stacks/"}, {"id": "1ocery5", "name": "t3_1ocery5", "content": "This is a service that basically puts a read replica of an RDS streaming into your Redshift data warehouse. \n\nWe have this set up in our environment and it runs many critical systems. After the nightmares of yesterday I checked this morning after getting some complaints from unhappy users about stale data and our ZETL integrations appear to have disappeared entirely. I can see the data and it appears to have stopped updating coincident with yesterday's outage. Looks like I'll have to completely remake these. This is pretty irritating because I can't find any information anywhere from AWS about the outage having *deleted* this infrastructure.", "date_utc": 1761057629.0, "title": "Anyone else use AWS Redshift Zero-ETL in US-EAST-1?", "upvote_ratio": 0.75, "score": 4, "url": "https://www.reddit.com/r/dataengineering/comments/1ocery5/anyone_else_use_aws_redshift_zeroetl_in_useast1/"}, {"id": "1obfys7", "name": "t3_1obfys7", "content": "EDIT EDIT: This is a past event although it looks like there are still errors trickling in.  Leaving this up for a week and then potting it.\n\nEDIT: AWS now appears to be largely working.\n\nIn terms of possible root cases, as hypothesised by u/tiredITguy42:\n\n>So what most likely happened:\n\n>DNS entry from DynamoDB API was bad.\n\n>Services can't access DynamoDB\n\n>It seems AWS is string IAM rules in DynamoDB\n\n>Users can't access services as they can't get access to resources resolved.\n\n>It seems that systems with main operation in other regions were OK even if some are running stuff in us-east-1 as well. It seems that they maintained access to DynamoDB in their region, so they could resolve access to resources in us-east-1.\n\n>These are just pieces I put together, we need to wait for proper postmortem analysis.\n\nAs some of you can tell, AWS is [currently experiencing outages](https://www.bbc.co.uk/news/live/c5y8k7k6v1rt)\n\nIn order to keep the subreddit a bit cleaner, post your gripes, stories, theories, memes etc. into here.\n\nWe salute all those on call getting shouted at.\n\nhttps://preview.redd.it/1ljfaxkc19wf1.jpg?width=500&format=pjpg&auto=webp&s=997eccdafb24bf1378b07503a37112e434c4470d", "date_utc": 1760957754.0, "title": "[Megathread] AWS is on fire", "upvote_ratio": 0.97, "score": 281, "url": "https://www.reddit.com/r/dataengineering/comments/1obfys7/megathread_aws_is_on_fire/"}, {"id": "1obzvv0", "name": "t3_1obzvv0", "content": "Or something like 'mrt\\_<sql\\_file\\_name>'?\n\nWhy don't you name it into, for example, 'recruitment' for marts for recruitment team?\n\n", "date_utc": 1761010012.0, "title": "Quick dbt question, do you name your data marts schema 'marts'?", "upvote_ratio": 0.94, "score": 14, "url": "https://www.reddit.com/r/dataengineering/comments/1obzvv0/quick_dbt_question_do_you_name_your_data_marts/"}, {"id": "1oc1c98", "name": "t3_1oc1c98", "content": "I am looking for ideas to leverage my Python programming knowledge while creating ADF pipelines to build a traditional DWH. Both source and target are Azure SQL. I am very new to ADF as this will be the first project in ADF. The project timeline is very tight. I want to avoid as much UI part (drag and drop) as possible during development and rely more on Python scripts. Any suggestion will be greatly appreciated. Thanks.", "date_utc": 1761014169.0, "title": "Azure Data Factory pipelines in Python", "upvote_ratio": 0.81, "score": 6, "url": "https://www.reddit.com/r/dataengineering/comments/1oc1c98/azure_data_factory_pipelines_in_python/"}, {"id": "1obsrfm", "name": "t3_1obsrfm", "content": "currently manage a small data team for a stable, growing and relaxed company. It\u2019s somewhat cross functional but doesn\u2019t have a clear growth path forward in terms of position or comp. Also, I am probably 75% hands on DE and remainder is a cross of business strategy, PM and misc. Dept growth may be stagnant since the it\u2019s not a tech company.\n\nI have an offer from a non-FAANG, but top company in their industry for a team lead position. TC is ~50% more. Growth is more defined and I think could have a much higher comp ceiling.\n\nI\u2019ve been running the small company route for a while and have never done DE at scale for a company with the resources/need to use the big tech. Can\u2019t decide whether finally being thrown into an actual engineering env would be beneficial or unnecessary at this stage in my career.\n\nAnyone have any words of wisdom?", "date_utc": 1760992002.0, "title": "Small company head of dept or team lead at a dominant global company?", "upvote_ratio": 0.96, "score": 23, "url": "https://www.reddit.com/r/dataengineering/comments/1obsrfm/small_company_head_of_dept_or_team_lead_at_a/"}, {"id": "1ocaik3", "name": "t3_1ocaik3", "content": "Hey folks,\n\nI\u2019m currently using Dagster as the orchestrator in my team\u2019s data stack and I\u2019m considering incorporating sqlmesh as our transformation library. But, I can\u2019t really figure out a way to integrate my sqlmesh models with Dagster so that they show up as individual assets. Has anyone had any luck in achieving this ? How did you go about doing it ?", "date_utc": 1761046512.0, "title": "Integrating sqlmesh models with Dagster", "upvote_ratio": 0.57, "score": 1, "url": "https://www.reddit.com/r/dataengineering/comments/1ocaik3/integrating_sqlmesh_models_with_dagster/"}, {"id": "1oc6sq1", "name": "t3_1oc6sq1", "content": "Hey guys, Cannot determine primary keys in raw data as no column is unique and concatenation of columns too don\u2019t provide uniqueness even if I go by business logic and say these columns are pk I don\u2019t get uniqueness, I get many duplicate rows, any idea on how to approach this? I can\u2019t just remove those duplicates\n\nEDIT - I checked each column for uniqueness and concatenation of columns and checked their uniqueness by using distinct but nothing unique\nI got duplicates and then I hashed all the columns together and removed the duplicate hashed columns and now I'm only hashing ID columns as other columns can like time and date can be changed and got some unique combo of columns that can be pk, I hope this approach is good guys ", "date_utc": 1761032804.0, "title": "Cannot determine primary keys in raw data as no column is unique and concatenation of columns too don\u2019t provide uniqueness", "upvote_ratio": 0.58, "score": 3, "url": "https://www.reddit.com/r/dataengineering/comments/1oc6sq1/cannot_determine_primary_keys_in_raw_data_as_no/"}, {"id": "1obij50", "name": "t3_1obij50", "content": "Yeah, so basically that. WTF. That was my first, second, and third reaction when I started trying to understand watermarks in Apache Flink. \n\nSo I got together with a couple of colleagues and built flink-watermarks.wtf. \n\nIt's a 'scrollytelling' explainer of what watermarks in Apache Flink are, why they matter, and how to use them.\n\nTry it out: [https://flink-watermarks.wtf/](https://flink-watermarks.wtf/)", "date_utc": 1760965640.0, "title": "Flink Watermarks. WTF?", "upvote_ratio": 0.94, "score": 33, "url": "https://i.redd.it/51pn5j7tm9wf1.gif"}, {"id": "1obp5go", "name": "t3_1obp5go", "content": "\\[CLOSED, got enough interest and i will postback\\]  \nHey everyone,  \nwe are a small team building a data orchestrator and we have a dbt use case we would like to demo. We would like to meet someone using DBT at large scale and understand how you use dbt/ usecase and would like to demo our product to get your feedback  \n", "date_utc": 1760983662.0, "title": "Anyone who uses DBT at large scale? looking for feedback", "upvote_ratio": 0.66, "score": 11, "url": "https://www.reddit.com/r/dataengineering/comments/1obp5go/anyone_who_uses_dbt_at_large_scale_looking_for/"}, {"id": "1obo6mx", "name": "t3_1obo6mx", "content": "I have worked on some decent project pipelines with the stack of airflow, apache kafka, pyspark and snowflake. Looking for open-source projects to build my profile more and to build my portfolio.", "date_utc": 1760981105.0, "title": "Looking for open-source projects", "upvote_ratio": 0.86, "score": 10, "url": "https://www.reddit.com/r/dataengineering/comments/1obo6mx/looking_for_opensource_projects/"}, {"id": "1obmj9a", "name": "t3_1obmj9a", "content": "Hi,\n\nI\u2019m currently doing some research for my internship and one of my sub-questions is which of a data warehouse, data lake, or lakehouse fits in my use case. Instead of listing those three options every time, I\u2019d like to use an umbrella term, but I haven\u2019t found a widely used one across different sources. I tried a few suggested terms from chatgpt, but the results on Google weren\u2019t consistent, so I\u2019m not sure what the correct umbrella term is.\n\n", "date_utc": 1760976357.0, "title": "Umbrella word for datawarehouse, datalake and lakehouse?", "upvote_ratio": 0.91, "score": 8, "url": "https://www.reddit.com/r/dataengineering/comments/1obmj9a/umbrella_word_for_datawarehouse_datalake_and/"}, {"id": "1obnj8i", "name": "t3_1obnj8i", "content": "", "date_utc": 1760979283.0, "title": "Databases Without an OS? Meet QuinineHM and the New Generation of Data Software", "upvote_ratio": 0.7, "score": 5, "url": "https://dataware.dev/blog/databases-without-os-meet-quininehm.html"}, {"id": "1oboeyg", "name": "t3_1oboeyg", "content": "Hey everyone, I\u2019ve been working at a WITCH company for about a year now, as a Data Engineer. I\u2019ve been in the same project throughout, where I mainly handle support tasks\u2014 documentation, debugging, adding columns, updating views, ensuring data flow, and just ensuring everything runs smoothly. Essentially, I\u2019m not involved in any real development work like building pipelines or writing scripts.\n\nI\u2019ve seen a lot of posts about people in similar situations, like the one here [Need career advice \u2013 Am I a Data Engineer?](https://www.reddit.com/r/dataengineersindia/comments/1nf3h0u/need_career_advice_am_i_a_data_engineer_how_do_i/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button) , and it feels like I\u2019m in the same boat. The problem is, I\u2019ve been stuck in this role for a year, and I honestly feel like if I stay for another year or two, I won\u2019t learn anything new. I\u2019ve been trying to switch jobs and have been applying for the last 3-4 months, but most of the time, I don\u2019t even make it past the shortlisting stage. I don\u2019t blame the companies, to be honest\u2014I just don\u2019t have anything unique to show in my experience section. I\u2019ve worked on some personal projects, but that\u2019s about it.\n\nAnother challenge is the 90-day notice period. It\u2019s hard to move quickly when companies need immediate joiners, so I feel stuck in that regard too.\n\nI see two possible options right now:\n\nAsk to be released from my current project: But this is tricky. I\u2019m not sure if they\u2019ll even let me go (seniors have told that release is not provided easily), and if I do get released, I\u2019m worried the next project might be even worse. Plus, I don\u2019t know how long I\u2019d be able to stay on the bench, and that\u2019s also not ideal.\n\nResign and serve the notice period quietly: This comes with its own set of risks, mainly the uncertainty of not having a job while I keep applying. The idea of not having a stable income while job hunting scares me.\n\nSo, I guess I\u2019m at a crossroads. Has anyone been in a similar situation? How did you navigate it? Any advice on how I should proceed from here?   \n  \nI don't want to be stuck here in this swamp in the early stages of my career as it will also affect my future career path. I have been making few more personal projects to add but I still don't think that will be enough.\n\nAppreciate any insights or suggestions!\n\nTL/DR : I\u2019ve been working as a Data Engineer in a WITCH company for a year, mostly handling support tasks with no real development experience. I\u2019ve been trying to switch jobs for 3-4 months but keep getting rejected. I\u2019m stuck with a 90-day notice period and feel like I\u2019m not learning anything new. Should I ask to be released from my project (with no guarantee of better work) or resign and serve the notice period while job hunting (despite the risk of unemployment)?", "date_utc": 1760981730.0, "title": "[Need Career Advice] Stuck In WITCH Trap with no Real learning. What Should I Do?", "upvote_ratio": 0.73, "score": 5, "url": "https://www.reddit.com/r/dataengineering/comments/1oboeyg/need_career_advice_stuck_in_witch_trap_with_no/"}, {"id": "1obmg2l", "name": "t3_1obmg2l", "content": "Roaring-like bitset indexes are used in most OLAP databases (Lucene, Spark, ClickHouse, etc).  \n  \nI explored plausibly fast-to-decode compression schemes and found a couple BSP-based approaches which can half Roaring's size. The decode complexity is quite high so these will probably match (rather than beat) Roaring throughput on bitwise ops once tuned, but their might be some value for memory-constrained and disk/network-bound contexts.  \n  \nWith an alternative simpler compression scheme I was able to reduce size by 23%, and expect the throughput will beat Roaring once the implementation is further along.", "date_utc": 1760976122.0, "title": "BSP-inspired bitsets: 46% smaller than Roaring (but probably not faster)", "upvote_ratio": 0.81, "score": 3, "url": "https://github.com/ashtonsix/perf-portfolio/blob/main/bspx/README.ipynb"}, {"id": "1obddb7", "name": "t3_1obddb7", "content": "Hi everyone, I'm working in retail logistics. Obviously retail enterprises face unique challenges meeting data governance and compliance requirements, especially multi-channel sales and regional variations in regulations (we're based in US, btw). When your company goes through audits or compliance reviews, what processes or frameworks help you streamline governance, auditability, and consent management?\n\nIf there's a more relevant place to post this please let me know!", "date_utc": 1760944475.0, "title": "Data compliance & governance in Salesforce Retail environments?", "upvote_ratio": 0.88, "score": 12, "url": "https://www.reddit.com/r/dataengineering/comments/1obddb7/data_compliance_governance_in_salesforce_retail/"}, {"id": "1obtotd", "name": "t3_1obtotd", "content": "MDN seems to be the gold standard for web devs for gaining knowledge. Are there any similar websites for Data Engineers?", "date_utc": 1760994061.0, "title": "Is there a website like MDN for data engineers?", "upvote_ratio": 0.6, "score": 1, "url": "https://www.reddit.com/r/dataengineering/comments/1obtotd/is_there_a_website_like_mdn_for_data_engineers/"}, {"id": "1obisxn", "name": "t3_1obisxn", "content": "Hey folks, I need to pick your brains to brainstorm a potential solution to my problem.\n\nCurrent stack: SparkSQL (Databricks SQL), storage in Delta, modeling in dbt.\n\nI have a pipeline that generally works like this:\n\n    WITH a AS (SELECT * FROM table)\n    SELECT a.*, 'one' AS type\n    FROM a\n    \n    UNION ALL\n    \n    SELECT a.*, 'two' AS type\n    FROM a\n    \n    UNION ALL\n    \n    SELECT a.*, 'three' AS type\n    FROM a\n\nThe source table is partitioned on a column, let's say column \\`date\\`, and the output is stored also with partition column \\`date\\` (both with Delta). The transformation in the pipeline is just as simple as select one huge table, do broadcast joins with a couple small tables (I have made sure all joins are done as \\`BroadcastHashJoin\\`), and then project the DataFrame into multiple output legs.\n\nI had a few assumptions that turns out to be plain wrong, and this mistake really f\\*\\*ks up the performance.\n\n**Assumption 1:** I thought Spark will scan the table once, and just read it from cache for each of the projections. Turns out, Spark compiles the CTE into inline query and read the table thrice.\n\n**Assumption 2**: Because Spark read the table three times, and because Delta doesn't support bucketization, Spark distributes the partition for each projection leg without guarantee that rows that share the same \\`date\\` will end up in the same worker. The consequence of this is a massive shuffling at the end before writing the output to Delta, and this shuffle really kills the performance.\n\nI have been thinking about alternative solutions that involve switching stack/tools, e.g. use pySpark for a fine-grained control, or switch to vanilla Parquet to leverage the bucketization feature, but those options are not practical. Do you guys have any idea to satisfy the above two requirements: (a) scan table once, and (b) ensure partitions are distributed consistently to avoid any shuffling.", "date_utc": 1760966392.0, "title": "Help: Fine-grained Instructions on SparkSQL", "upvote_ratio": 0.81, "score": 3, "url": "https://www.reddit.com/r/dataengineering/comments/1obisxn/help_finegrained_instructions_on_sparksql/"}, {"id": "1obdp5t", "name": "t3_1obdp5t", "content": "Hey all. Do you experience some issues with AWS as well? It seems it might be down.\n\nIf it is down, we will have a wonderful day for sure (\\\\s).", "date_utc": 1760945687.0, "title": "Anyone experiencing issues with AWS right now", "upvote_ratio": 0.79, "score": 8, "url": "https://www.reddit.com/r/dataengineering/comments/1obdp5t/anyone_experiencing_issues_with_aws_right_now/"}, {"id": "1obf0uv", "name": "t3_1obf0uv", "content": "Entire AWS management console page down... that's a first...\n\nAnd of course it had to happen right before production deployment, congrats to all you people not on call I guess.\n\nhttps://preview.redd.it/nldday0gd8wf1.png?width=1504&format=png&auto=webp&s=c899e84f1e203ce27ab09122a45e696fb093f107\n\nhttps://preview.redd.it/3o1kwblhd8wf1.png?width=1100&format=png&auto=webp&s=b2bd02c5a78fc33d2ea3bbe9e76663a02b6883f7\n\n", "date_utc": 1760950540.0, "title": "AWS US East DynamoDB and pretty much everything else down...", "upvote_ratio": 0.7, "score": 5, "url": "https://www.reddit.com/r/dataengineering/comments/1obf0uv/aws_us_east_dynamodb_and_pretty_much_everything/"}, {"id": "1ob3566", "name": "t3_1ob3566", "content": "I have been trying Apache Superset for some time, and it does most of the job but also comes just short of what I need it to do. Things like:\n\n- Not straightforward to reuse the same dashboard with different source tables or views. \n- Supports cert auth for some DB connections but not others. Unless I am reading the docs wrong.\n\nWhat other alternatives are out there? I do not even need the fancy visualizations, just something that can do filtering and aggregation on the fly for display in tabular format.", "date_utc": 1760912903.0, "title": "What tools do you prefer to use for simple interactive dashboards?", "upvote_ratio": 1.0, "score": 31, "url": "https://www.reddit.com/r/dataengineering/comments/1ob3566/what_tools_do_you_prefer_to_use_for_simple/"}, {"id": "1oblnpe", "name": "t3_1oblnpe", "content": "So we've had a data architect for a while now who's been with the business a long time, and he's resigned so we're looking to replace him. I discovered that he has, for the most part, been copying and pasting data model designs from some confluence docs created in 2018 by someone else... so it's not a huge loss, but he does know the org's SAP implementation quite well.\n\n  \nI'm wondering... what am I looking for? What do I need? We don't need technical implementation help from a platform perspective, I think we just need someone mainly doing data modelling. I also want to steer clear of anyone wanting to create an up front enterprise data model.\n\n  \nWe're trying to design our data model iteratively, but carefully.", "date_utc": 1760973948.0, "title": "Advice on hiring a data architect?", "upvote_ratio": 0.5, "score": 0, "url": "https://www.reddit.com/r/dataengineering/comments/1oblnpe/advice_on_hiring_a_data_architect/"}, {"id": "1ob6d1n", "name": "t3_1ob6d1n", "content": "Looking for broad advice on what should data engineering teams be showing during demos to customers or stakeholders (KPIs, dashboards, metrics, reports, other?). My team doesn't have anything super time sensitive coming up, just wondering what reports/dashboards people recommend we invest time into creating and maintaining to show progress in our data engineering. We just want to get better at showing continuous progress to customer/stakeholders.\n\nI feel this is harder than for data scientists or analysts since they are a lot closer to the work that directly relates to \"the core business\".\n\nI have been reading into DORA metrics from software engineering as well, but I don't know if those are things we could share to show progress to stakeholders.", "date_utc": 1760921678.0, "title": "What to show during demo's?", "upvote_ratio": 1.0, "score": 9, "url": "https://www.reddit.com/r/dataengineering/comments/1ob6d1n/what_to_show_during_demos/"}, {"id": "1obdu67", "name": "t3_1obdu67", "content": "How would you design your pipelines for handling deduplicates before they move to your downstream?", "date_utc": 1760946158.0, "title": "How do you do a Dedup check in batch & steam?", "upvote_ratio": 0.67, "score": 2, "url": "https://www.reddit.com/r/dataengineering/comments/1obdu67/how_do_you_do_a_dedup_check_in_batch_steam/"}, {"id": "1oatq7o", "name": "t3_1oatq7o", "content": "Hey guys,\n\nI'm total beginner learning tools used data engineering and just started diving into orchestration , but I'm honestly so confused about which direction to go\n\ni saw people mentioning Airflow, Dagster, Prefect\n\nI figured \"okay, Airflow seems to be the most popular, let me start there.\" But then I went to actually set it up and now I'm even MORE confused...\n\n* First option: run it in a Python environment (seems simple enough?)\n* BUT WAIT - they say it's recommend using a Docker image instead\n* BUT WAIT AGAIN - there's this big caution message in the documentation saying you should really be using Kubernetes\n* OH AND ALSO - you can use some \"Astro CLI\" too?\n\nLike... which one am I actually supposed to using? Should I just pick one setup method and roll with it, or does the \"right\" choice actually matter?\n\nAlso, if Airflow is this complicated to even get started with, should I be looking at Dagster or Prefect instead as a beginner?\n\nWould really appreciate any guidance because i'm so lost and thanks in advance", "date_utc": 1760890615.0, "title": "Beginner Confused About Airflow Setup", "upvote_ratio": 0.95, "score": 29, "url": "https://www.reddit.com/r/dataengineering/comments/1oatq7o/beginner_confused_about_airflow_setup/"}, {"id": "1ob2shp", "name": "t3_1ob2shp", "content": "In this Meta data engineering [blog post](https://engineering.fb.com/2025/08/13/data-infrastructure/agentic-solution-for-warehouse-data-access/) it says, \"As part of its offline data systems, Meta operates a data warehouse that supports use cases across analytics, ML, and AI\".\n\n  \nI'm familiar with OLTP (Online Transaction Processing) and OLAP (Online Analytical Processing) data systems. What makes Meta's offline data system different than the average OLAP data system. E.g what makes a data warehouse online vs offline?", "date_utc": 1760912006.0, "title": "Confused by Offline Data Systems terminology", "upvote_ratio": 0.88, "score": 6, "url": "https://www.reddit.com/r/dataengineering/comments/1ob2shp/confused_by_offline_data_systems_terminology/"}, {"id": "1obdotn", "name": "t3_1obdotn", "content": "\u00a0We recently had to handle both batch and stream data for a fintec client. I set up Spark structured streaming on top of Delta Lake with Airflow scheduling. The tricky part was ensuring consistency between batch historical loads and realtime ingestion \n\nHad to tweak checkpointing and watermarks to avoid duplicates and late arrivals. Felt like juggling clocks and datasets at the same time. Anyone else run into weird late arrival issues with Spark streaming?\n\n", "date_utc": 1760945657.0, "title": "Designing a hybrid batch stream pipeline for fintech data", "upvote_ratio": 0.6, "score": 1, "url": "https://www.reddit.com/r/dataengineering/comments/1obdotn/designing_a_hybrid_batch_stream_pipeline_for/"}, {"id": "1oajs0h", "name": "t3_1oajs0h", "content": "Hey all,\n\nThis might be a very dumb or ignorant question from me who know very little about DevOps or best practices in DE but would be great if I can stand on the shoulders of giants!\n\nFor the background context, I'm working as a quant engineer at a company with about 400 employees total (60\\~80 IT staff, separate from our quant/data team which consists of 4 people, incl myself). Our team's trying to build out our analytics infrastructure and our IT department has set up **completely separate environments** for DEV, UAT, and PROD including:\n\n* Separate Snowflake accounts for each environment\n* Separate managed Airflow deployments for each environment\n* GitHub monorepo with protected branches (dev/uat/prod) for code (In fact, this is what I asked for. IT dept tried to setup polyrepo for n different projects but I refused)\n\nThis setup is causing major challenges or at least I do not understand how to:\n\n* As far as I am aware, zero copy cloning doesn't work across Snowflake accounts, making it impossible to easily copy production data to DEV for testing\n* We don't have dedicated DevOps people so setting up CI/CD workflows feels complicated\n* Testing ML pipelines is extremely difficult without realistic data given we cannot easily copy data from prod to dev account in Snowflake\n\nI've been reading through blogs & docs but I'm still confused about what's standard practice for this circumstance. I'd really appreciate some real-world insights from people who've been in similar situations.\n\n# This is my best attempt to distill the questions:\n\n* For a small team like ours (4 people handling all data work), is it common to have completely separate Snowflake accounts AND separate Airflow deployments for each environment? Or do most companies use a single Snowflake account with separate databases for DEV/UAT/PROD and a single Airflow instance with environment-specific configurations?\n* How do you handle testing with production-like data when you can't clone production data across accounts? For ML development especially, how do you validate models without using actual production data?\n* What's the practical workflow for promoting changes from DEV to UAT to PROD? We're using GitHub branches for each environment but I'm not sure how to structure the CI/CD process for both dbt models and Airflow DAGs without dedicated DevOps support\n* How do you handle environment-specific configurations in dbt and Airflow when they're completely separate deployments? Like, do you run Airflow & dbt in DEV environment to  generate data for validation and do it again across UAT & PROD? How does this work?\n\nAgain, I have tried my best to arcitulate the headaches that I am having and any practical advice would be super helpful. \n\nThanks in advance for any insights and enjoy your rest of Sunday!", "date_utc": 1760860056.0, "title": "Struggling with separate Snowflake and Airflow environments for DEV/UAT/PROD - how do others handle this?", "upvote_ratio": 0.98, "score": 43, "url": "https://www.reddit.com/r/dataengineering/comments/1oajs0h/struggling_with_separate_snowflake_and_airflow/"}, {"id": "1oafuv2", "name": "t3_1oafuv2", "content": "Hey r/dataengineering,\n\u200bMy company (consulting in europe) is giving me some time and an open budget to grab my next certification. I need your honest opinions on what's worth the time and money in today's market.\n\n\n\u200bMy Profile:\n\u200bStarted as: Data Analyst 5years ago (Power BI, SQL, Python).\n\u200bNow shifting into: Data Engineering (Fabric, dbt, Snowflake).\n\u200bGoal: Go deeper into proper DE work, (while keeping analytics sttenghts).\n\n\u200bCurrent Certs I've already passed:\n* \u200bPL-300 (Power BI)\n* \u200bDP-600 (fabric Analytics Engineer Associate)\n* \u200bPlus, the basic dbt and Databricks Foundations certs.\n\n\n\u200bSo, what's the next move?\n\u200bWhat serious, paid certification is the actual game-changer right now for staying competitive? \nShould I double down on a specific cloud (AWS/GCP/Azure DE path)? Focus on something like Databricks/snowflake/dbt ?\n\n\nI know certif are sometimes bullshiy, but I can't resist free time and free voucher :)\n\n\u200bHit me with your best recommendations !\n\nEdit: formating", "date_utc": 1760845899.0, "title": "Company is paying for my next DE cert. Which one to choose right now ?", "upvote_ratio": 0.89, "score": 41, "url": "https://www.reddit.com/r/dataengineering/comments/1oafuv2/company_is_paying_for_my_next_de_cert_which_one/"}, {"id": "1oaq471", "name": "t3_1oaq471", "content": "Before we begin, I'm a 17-year-old high school student in South Korea. I'm currently researching AI APIs for a personal project. Grok and Open AI are expensive, so I tried using the Gemini API in Google AI Studio. However, I can't use it in Korea because the minimum age requirement is 18. Then, I found the Hugging Face Inference API, but I can't find any reviews or detailed information about it in Korea, so I'm posting my questions here.\n\n1: Is this API free?\n\n2: If it's free, how many free requests can I make per day or month?\n\nThat's all. (I'm still learning, so I might be wrong.)", "date_utc": 1760881753.0, "title": "Questions about the hugging face inference API", "upvote_ratio": 1.0, "score": 6, "url": "https://www.reddit.com/r/dataengineering/comments/1oaq471/questions_about_the_hugging_face_inference_api/"}, {"id": "1oaqz6g", "name": "t3_1oaqz6g", "content": "Hi everyone , I've some bulleted queries regarding Open Telemetry tuning in production for ETL.\n1. What parameters to capture\n2. Sampling rate", "date_utc": 1760883970.0, "title": "OTel tuning", "upvote_ratio": 0.67, "score": 2, "url": "https://www.reddit.com/r/dataengineering/comments/1oaqz6g/otel_tuning/"}, {"id": "1oagoef", "name": "t3_1oagoef", "content": "currently i'm building data lakehouse using aws native services - glue, athena, lakeformation, etc.   \n\npreviously wihtin data lake, sensitive PII data was handling in redimentary way, wherein, static fields per datasets are maintained ,and regex based data masking/redaction in consumption layers. With new data flowing, handling newly ingested sensitive data is reactive.\n\nwith data lakehouse, as per my understanding PII handling would be done i a more elegant way as part of data governance strategy, and to some extent i've explored lakeformation , PII tagging, access control based on tags, etc.  however, i still have below gaps : \n\n* with medallian architecture, and incremental data flow, i'm i suppose to auto scan incremental data and tag them while data is moving from bronze to silver?\n* should the tagging be from silver layer onwards?\n* whats the best way to accurately scan/tag at scale - any llm/ml option\n* scanning incremental data given high volume, to be scalable, should it be separate to the actual data movement jobs?\n   * if kept separate , now should we still redact from silver and how to workout the sequence as tagging might happen layer to movement\n   * or should we rather go with dynamic masking , again whats the best technology for this\n\n  \nany suggestion/ideas are highly appreciated.\n\n", "date_utc": 1760848643.0, "title": "handling sensitive pii data in modern lakehouse built with AWS stack", "upvote_ratio": 0.82, "score": 7, "url": "https://www.reddit.com/r/dataengineering/comments/1oagoef/handling_sensitive_pii_data_in_modern_lakehouse/"}, {"id": "1oa0bi3", "name": "t3_1oa0bi3", "content": "Hi I\u2019m looking at table that is fairly large 20 billion rows.  Trying to join it against table with about 10 million rows. It is aggregate join that an accumulates pretty much all the rows in the bigger table using all rows in smaller table.  End result not that big. Maybe 1000 rows. \n\nWhat is strategy for such joins in database.  We have been using just a dedicated program written in c++ that just holds all that data in memory.  Downside is that it involves custom coding, no sql, just is implemented using vectors and hash tables. Other downside is if this server goes down it takes some time to reload all the data.  Also machine needs lots of ram.  Upside is the query is very fast.\n\nI understand a type of aggregate materialized view could be used.  But this doesn\u2019t seem to work if clauses added to where. Would work for a whole join though.\n\nWhat are best techniques for such joins or what end typically used ?\n\n", "date_utc": 1760805013.0, "title": "Best approach to large joins.", "upvote_ratio": 0.99, "score": 77, "url": "https://www.reddit.com/r/dataengineering/comments/1oa0bi3/best_approach_to_large_joins/"}, {"id": "1oabl3i", "name": "t3_1oabl3i", "content": "I am currently using big query salesforce data transfer service to ingest salesforce data - right now it is on a preview mode and only supports full refreshes\n\nGoogle is releasing incremental updates feature to the connector, which is the more efficient option\n\nProblem with salesforce data is the formula fields and how they\u2019re calculated on the go instead of storing actual data on the object \n\nI have a transaction data object with 446 fields and 183 of those fields are calculated/formula fields\n\nSome fields , like customer_address_street, is a formula field that references the customer object\n\nIf the address on the customer record on the customer object gets updated, the corresponding row(s) referencing the same customer on the transaction object will not get updated as the transaction row is not explicitly updated, and thus the systemmodstamp field remains unchanged\n\nIncremental refreshes wont capture this change of data and the transaction row from the transaction object will show the old address of the customer.\n\nHow are you currently handling this behaviour? Especially for objects with 183 formula fields, and more being added within the salesforce database?\n\nIdeally i want my salesforce data to refresh every 2 hours in the warehouse\n\n*For reference, i develop BI dashboards and i have very little experience in data engineering ", "date_utc": 1760832611.0, "title": "For anyone working with Salesforce data in BigQuery, how are you handling formula fields?", "upvote_ratio": 0.81, "score": 6, "url": "https://www.reddit.com/r/dataengineering/comments/1oabl3i/for_anyone_working_with_salesforce_data_in/"}, {"id": "1oai1xu", "name": "t3_1oai1xu", "content": "Hello!\n\nI am making some data models for marketing insights through social and web channels. Surely each of their APIs provide users with some useful default metrics\n\nBut I am curious tho if anyone here has the experience on building metrics that don't exists in the first place\n\nWhat important metrics have you built for social media and web analyses that are not provided by default?\n\nHow's that helping your analyst or scientist?", "date_utc": 1760853581.0, "title": "Important analytical models/metrics you have made for social media and web analyst", "upvote_ratio": 0.75, "score": 2, "url": "https://www.reddit.com/r/dataengineering/comments/1oai1xu/important_analytical_modelsmetrics_you_have_made/"}, {"id": "1oaa9jv", "name": "t3_1oaa9jv", "content": "Hey guys, looking for feedback on a potential setup. For context, we are a medium sized company and our data department consists of me, my boss and one other analyst. I'm the most technical one, the other two can connect to a database in Tableau and that's about it. I'm fairly comfortable writing Python scripts and SQL queries, but I am not a software engineer.\n\nWe currently have MS SQL Server on prem that was set up a decade ago and is reaching its end of life in terms of support. For ETL, we've been using Alteryx for about as long as that, and for reporting we have Tableau Server. We don't have that much data (550GB total), and we ingest about 50k rows an hour in batched CSV files that our vendors send us. This data is a little messy and needs to be cleaned up before a database can ingest it.\n\nWith the SQL Server getting old and our renewal conversations with Alteryx going extremely poorly, my boss has directed me to research options for replacing both, or scaling Alteryx down to just the last mile for Tableau Server. Our main purposes are 1) upgrade our data warehouse to something with as little maintenance as possible and 2) continue to serve our Tableau dashboards 3) make ad-hoc analysis in Tableau possible for my boss and the other analyst. Ideally, we'd keep our costs to under 70k a year.\n\nSo far I've played around with Databricks, Clickhouse, Prefect, Dagster, and have started doing the dbt fundementals courses to get a better idea of it. While I liked Databricks's unity catalog and time travel capabilities of delta tables, the price and computing power of spark seems like overkill for our purposes/size. It felt like I was spending a lot of time spinning up clusters and frivolously spending cash working out the syntax.\n\nClickhouse caught my eye since it promises fast query times, it is easy enough to set up and put together a sample pipeline together, and the cloud database offering seems cheaper than DBX. It's nice that dbt-core can be used with it as well, because just writing queries and views inside the cloud console there seems like it can get hairy and confusing really fast.\n\nSo far, I'm thinking that we can run local Python scripts for ingesting data into Clickhouse staging tables, then write views on top of those for the cleaner silver + gold tables and let Alteryx/analysts connect to those. The tricky part with CH is how it manages upserts/deletions behind the scenes, but I think with ReplacingMergeTrees and solid queries, we could get around those limitations. It's also less forgiving with schema drift and inferring data types.\n\nSo my questions are as follows:\n\n\n* Does my approach make sense? \n* Are there other products worth looking into for my use case?\n* How do you guys evaluate the feasibility of a setup when the tools are new to you?\n* Is Clickhouse in your experience a solid product that will be around for the next 5-10 years?\n\n\nThank you for your time.", "date_utc": 1760828927.0, "title": "Evaluating my proposed approach", "upvote_ratio": 0.63, "score": 2, "url": "https://www.reddit.com/r/dataengineering/comments/1oaa9jv/evaluating_my_proposed_approach/"}, {"id": "1o9xi5u", "name": "t3_1o9xi5u", "content": "I'm currently working on deploying an Apache Airflow project to AWS EC2 using Terraform, and I have a question about how to handle the deployment of the project files themselves. I understand how to use Terraform to provision the infrastructure, but I\u2019m not sure about the best way to automatically upload my entire Airflow project to the EC2 instance that Terraform creates. How do people typically handle this step?\n\nAdditionally, I\u2019d like to make the project more complete by adding a machine learning layer, but I\u2019m still exploring ideas. Do you have any suggestions for some ML projects using Reddit data? \n\nThank you in advance for your attention.", "date_utc": 1760798231.0, "title": "How to deploy airflow project on EC2 instance using Terraform.", "upvote_ratio": 0.74, "score": 7, "url": "https://www.reddit.com/r/dataengineering/comments/1o9xi5u/how_to_deploy_airflow_project_on_ec2_instance/"}, {"id": "1o9x20y", "name": "t3_1o9x20y", "content": "Hi everyone, what are your thoughts on Databricks genie? I am just worried about it hallucinating or my business team relying too much on gen AI. Do you guys use it and is it comparable to other products and platforms?? What would you recommend instead and what don\u2019t you like about it??", "date_utc": 1760797153.0, "title": "thoughts on databricks genie", "upvote_ratio": 0.76, "score": 6, "url": "https://www.reddit.com/r/dataengineering/comments/1o9x20y/thoughts_on_databricks_genie/"}, {"id": "1oa26wu", "name": "t3_1oa26wu", "content": "We have many adhoc scripts to run at our org like:\n\n1. postgres data insertions based on certain params\n\n2. s3 to postgres \n\n3. run certain data cleaning scripts\n\n  \nI am thinking to use dagster for this because I need to have some visibility into when the devs are running certain scripts, view logs, track them etc.\n\n  \nI am I in the right direction to think about using dagster ? or any other tool better suits this purpose ??\n\n  \n", "date_utc": 1760809507.0, "title": "What is the right tool for running adhoc scripts (with some visibility)", "upvote_ratio": 0.54, "score": 1, "url": "https://www.reddit.com/r/dataengineering/comments/1oa26wu/what_is_the_right_tool_for_running_adhoc_scripts/"}, {"id": "1o9q246", "name": "t3_1o9q246", "content": "Hey looking for some direction on Data factory extraction design patterns. Im new to the Data Engineering world but i come from infrastructure with experience standing Data factories and some simple pipelines. Last month we implemented a Databricks DLT Meta framework that we just scrapped and pivoted to a similar design that doesn't rely on all those onboarding ddl etc files. Now its just dlt pipelines perfoming ingestion based on inputs defined in asset bundle when ingesting. On the data factory side our whole extraction design is dependent on a metadata table in a SQL Server database. This is where i feel like this is a bad design concept to totally depend on a unsecured non version controlled table in a sql server database. That table get deleted or anyone with access doing anything malicious with that table we can't extract data from our sources. Is this a industry standard way of extracting data from sources? This feels very outdated and non scalable to me to have your entire data factory extraction design based on a sql table. We only have 240 tables currently but we are about to scale in December to 2000 and im not confident in that scaling at all. My concerns fall on deaf ears due to my co workers having 15+ years in data but primary using Talend not Data Factory and not using Databricks at all. Can someone please give me some insights on modern techniques if my suspicions are correct? ", "date_utc": 1760774743.0, "title": "Data Factory extraction techniques", "upvote_ratio": 0.9, "score": 14, "url": "https://www.reddit.com/r/dataengineering/comments/1o9q246/data_factory_extraction_techniques/"}, {"id": "1o957aj", "name": "t3_1o957aj", "content": "Am I crazy in thinking this doesn't represent \"open\" at all? ", "date_utc": 1760716681.0, "title": "Data infrastructure so \"open\" that there's only 1 box that isn't Fivetran...", "upvote_ratio": 0.96, "score": 257, "url": "https://i.redd.it/2w1fkqef2pvf1.png"}, {"id": "1o9uqkv", "name": "t3_1o9uqkv", "content": "Hi all,\n\nfollowing the discussion here:  \n[https://www.reddit.com/r/dataengineering/comments/1n7b1uw/steps\\_in\\_transforming\\_lake\\_swamp\\_to\\_lakehouse/](https://www.reddit.com/r/dataengineering/comments/1n7b1uw/steps_in_transforming_lake_swamp_to_lakehouse/)\n\nIve explained my boss that the solution is to create some kind of pipeline that:  \n1. model the data  \n2. transform it to tabular format (Iceberg)  \n3. save it as parquet with some metadata\n\nHe insist that its not correct - and there is much better and easy solution - which is to index all the data and create our own metadata files that will have the location of the files we are looking for (maybe like MongoDB)  \nanother aspect why he against the idea of table format is because all our testing pipeline is based on some kind of json format (we transform the raw json to our own msgpec model).\n\nhow can I deliver to him that we are getting all this indexing for free when we are using iceberg, and if we miss some indexing in his idea we will need to go over all the data again and again.\n\nThank (for his protection he has 0 background in DE) ", "date_utc": 1760791269.0, "title": "How to convince my boss that table is the way to go", "upvote_ratio": 0.7, "score": 7, "url": "https://www.reddit.com/r/dataengineering/comments/1o9uqkv/how_to_convince_my_boss_that_table_is_the_way_to/"}, {"id": "1o97xkg", "name": "t3_1o97xkg", "content": "For example, I\u2019ve noticed that an Eng department will have dedicated teams per product area/feature, i.e. multiple front end developers who only work on one part of the code base. More concretely, there may be one front end developer for marketing/onboarding, another for the customer facing app and maybe another for internal tools.\n\n\nEdit: I\u2019m just using the FE role as an example. In reality, it\u2019s actually a complete team \n\n\nHowever, the expectation is that one DE is responsible for all of the areas; understanding the data model, owning telemetry/product analytics, ensuring data quality, maintaining data pipelines, building the dw and finally either building charts or partnering with analytics/reporting on the BI. The point being that if one of these teams drops the ball, the blame still falls on the DE. \n\nI\u2019ve had this expectation everywhere I\u2019ve been. Some places are better than others in terms of how big the Data team can be and perhaps placing more responsibility on the downstream and upstream teams, but it\u2019s generally never a \u201cyou are only responsible for this area\u201d\n\nI\u2019m rambling a bit but hopefully you get the idea. Is it only my experience? Is it only a startup thing? I\u2019m curious to hear from others. ", "date_utc": 1760722855.0, "title": "Anyone feel like too much is expected of DEs (at small companies)", "upvote_ratio": 0.94, "score": 96, "url": "https://www.reddit.com/r/dataengineering/comments/1o97xkg/anyone_feel_like_too_much_is_expected_of_des_at/"}, {"id": "1o9gfu5", "name": "t3_1o9gfu5", "content": "This is a problem I\u2019ve been thinking about for quite some time, and I just can\u2019t wrap my head around it.\nIt\u2019s generally recommended to partition data by the time it lands in S3 (i.e., event processing time) so that your pipelines are easier to make idempotent and deterministic. That makes sense operationally \u2014 but it creates a disconnect because business users don\u2019t care about processing time; they care about event time.\nTo complicate things further, it\u2019s also recommended to keep your bronze layer append-only and handle deduplication downstream.\nSo, I have three main questions:\n1. How would you approach partitioning in the bronze layer under these constraints?\n2. How would you design an efficient deduplication view on top of the bronze layer, given that it can contain duplicates and the business only cares about the latest record?\n3. Given that there might be intermediary steps in between, like dbt transformations when going from bronze to gold. How do you partition data in each layer so that your pipeline can scale? \n\nIs achieving idempotentcy and deterministic behavior at scale a huge challenge? \n\nI would be grateful if there are any resources on it that you can point me towards too? \n\n", "date_utc": 1760743449.0, "title": "Late data arrival partitioning best practices", "upvote_ratio": 0.87, "score": 23, "url": "https://www.reddit.com/r/dataengineering/comments/1o9gfu5/late_data_arrival_partitioning_best_practices/"}, {"id": "1o96rrn", "name": "t3_1o96rrn", "content": "Curious what everyone's \"dream job\" looks like as a DE", "date_utc": 1760720245.0, "title": "If you could work as a DE anywhere, what company or industry would it be - and why?", "upvote_ratio": 0.92, "score": 54, "url": "https://www.reddit.com/r/dataengineering/comments/1o96rrn/if_you_could_work_as_a_de_anywhere_what_company/"}, {"id": "1o9gtyi", "name": "t3_1o9gtyi", "content": "Any recommendations for a course which teaches advanced and basic dimensional and fact modelling (kimball one preferably) \n\nPlease provide the one you have used and learnt from.", "date_utc": 1760744539.0, "title": "Courses for dim and fact modelling", "upvote_ratio": 0.89, "score": 16, "url": "https://www.reddit.com/r/dataengineering/comments/1o9gtyi/courses_for_dim_and_fact_modelling/"}, {"id": "1o9335d", "name": "t3_1o9335d", "content": "Hi everyone, I'm the only DE at a small startup, and this is my first DE job.\n\nCurrently, as engineers build features on our application, they occasionally modify the database by adding new columns or changing column data types, without informing me. Thus, inevitably, data gets dropped or removed and a critical part of our application no longer works. This leaves me completely reactive to urgent bugs.\n\nWhen I bring it up with management and our CTO, they said I should put in tests in the DB to keep track as engineers may forget. Intuitively, this doesn't feel like the right solution, but I'm open to suggestions for either technical or process implementations. \n\nStack: Postgres DB + python scripting to clean and add data to the DB.", "date_utc": 1760711914.0, "title": "Engineers modifying DB columns without informing others", "upvote_ratio": 0.94, "score": 64, "url": "https://www.reddit.com/r/dataengineering/comments/1o9335d/engineers_modifying_db_columns_without_informing/"}, {"id": "1o98a9f", "name": "t3_1o98a9f", "content": "Hello all,\n\nI've decided to swallow my dreams of data engineering as a profession and just enjoy it as a hobby. I'm disentangling my need for more work from my desire to work with more data.\n\nAnyone else out there in a different field that performs data engineering at home for the love of it? I have no shortage of project ideas that involve modeling, processing, verifying, and analyzing \"massive\" (relative to home lab - so not massive) amounts of data. At hyper laptop scale!\n\nTo kick off some discussion... What's your home data stack? How do you keep your costs down? What do you love about working with data that compels you to do it without being paid for it?\n\nI'm sporting pyspark (for initial processing), cuallee (for verification and quality control), and pandas (for actual analysis). I glue it together with Bash and Python scripts. Occasionally parts of the pipeline happen in Go or C when I need speed. For cloud, I know my way around AWS and GCP, but don't typically use them for home projects.\n\nTake care,  \nme (I swear).\n\nEdit: minor readability edit.", "date_utc": 1760723673.0, "title": "Embracing data engineering as a hobby", "upvote_ratio": 0.84, "score": 26, "url": "https://www.reddit.com/r/dataengineering/comments/1o98a9f/embracing_data_engineering_as_a_hobby/"}, {"id": "1oa7fk5", "name": "t3_1oa7fk5", "content": "I am completely new to this. I just switched from mathematics to data engineering and had my first job. I am wondering whether the job market of this particular profession is tough or not? The US and Europe are both of interest to me. ", "date_utc": 1760821674.0, "title": "Is the data engineering job market good?", "upvote_ratio": 0.46, "score": 0, "url": "https://www.reddit.com/r/dataengineering/comments/1oa7fk5/is_the_data_engineering_job_market_good/"}, {"id": "1o9h3xu", "name": "t3_1o9h3xu", "content": "Currently, I have the [acryl_datahub_dagster_plugin](https://docs.datahub.com/docs/lineage/dagster) working in my Dagster instance, so that all assets that Dagster materializes will automatically show up in my DataHub instance. And with any dbt models that materialize via Dagster, those too all show up in DataHub, including the table lineage of all of the models that were executed.\n\nBut has anyone else figured out how to automatically get the columns for each model to show up in DataHub? The above plugin doesn't seem to do that, but wasn't sure if anyone already figured out a trick to get Dagster to upload those models' columns for me?\n\nLooking at the [Important Capabilities](https://docs.datahub.com/docs/generated/ingestion/sources/dbt#important-capabilities) for dbt in DataHub, it states that Column-Level Lineage should be possible, but wasn't sure if there was an automated way of doing this via Dagster? Or would I have to get the `CLI based Ingestion` working instead, and then just run that each time I deploy my code?\n\n\n#NOTE: using `Dagster OSS` and `dbt core`", "date_utc": 1760745319.0, "title": "Dagster, dbt, and DataHub integration", "upvote_ratio": 0.68, "score": 4, "url": "https://www.reddit.com/r/dataengineering/comments/1o9h3xu/dagster_dbt_and_datahub_integration/"}, {"id": "1o81ydc", "name": "t3_1o81ydc", "content": "", "date_utc": 1760608338.0, "title": "Hard to swallow.....", "upvote_ratio": 0.99, "score": 4273, "url": "https://i.redd.it/edggfctg4gvf1.png"}, {"id": "1o9du1r", "name": "t3_1o9du1r", "content": "Hi everyone, I\u2019ve kicked off this open source project and I\u2019d love to have you all try it. Full disclosure, this is a personal solo project and I\u2019m releasing it under the MIT license so this is not a marketing post. \n\nIt\u2019s a python library that allows you to create unlimited synthetic tabular data for training AI models. It uses Gaussian Copula to learn from the seed data and produce realistic and believable copies. It\u2019s not just randomized noise so you\u2019re not going to have teens with high blood pressure in a medical dataset or toddlers with mortgages on a financial dataset.\n\nAdditionally, it generates a cryptographic proof with every synthesis using hashes and Merkle roots for auditing purposes.\n\nI\u2019d love your feedback and PRs if you\u2019re up for it!", "date_utc": 1760736616.0, "title": "Open source verifiable synthetic data library", "upvote_ratio": 1.0, "score": 4, "url": "https://github.com/VeriSynthAI/verisynth-core"}, {"id": "1o96izv", "name": "t3_1o96izv", "content": "Hi guys,\n\nI have a client db (mysql) with 3 tables of each 3M rows.\n\nThis tables are bloated with useless and incorrect data, and thus we need to clean it and remove some columns and then insert it in our db (postgres).\n\nRuns fine the first time on my colleague pc with 128GB of ram....\n\nI need to run this every night and can't use so much ram on the server since it's shared....\n\nI thought about comparing the 2 DBs and updating/inserting only the rows changed, but since the schema is not equal i can't to that directly.\n\nI even thought about hashing the records, but still schema not equal...\n\nThe only option i can think of, is to select only the common columns and create an hash on our 2nd DB  and then successively compare only the hash, but still need to calculate it on the fly ( can't modify client db).\n\nUsing the updated\\_at column is a no go since i saw it literally change every now and then on ALL the records.\n\nAny suggestion is appreciated.  \nThanks", "date_utc": 1760719696.0, "title": "Compare and update two different databases", "upvote_ratio": 0.86, "score": 5, "url": "https://www.reddit.com/r/dataengineering/comments/1o96izv/compare_and_update_two_different_databases/"}, {"id": "1o8yjpe", "name": "t3_1o8yjpe", "content": "Hey everyone, I have this one question maybe vague, but hope its ok to ask.... As there is a lot of boilerplate code around open telemetry, retries, DLQ's, scaling and overall code structure. How do you manage it from projects to projects.", "date_utc": 1760699868.0, "title": "How do you architect your boilerplate code over projects.", "upvote_ratio": 0.81, "score": 9, "url": "https://www.reddit.com/r/dataengineering/comments/1o8yjpe/how_do_you_architect_your_boilerplate_code_over/"}, {"id": "1o8vwac", "name": "t3_1o8vwac", "content": "Iceberg support is coming to Fluss in 0.8.0 - but I got my hands on the first demo (authored by Yuxia Luo and Mehul Batra) and recorded a video running it.\n\nWhat it means for Iceberg is that now we'll be able to use Fluss as a hot layer for sub-second latency of your Iceberg based Lakehouse and use Flink as the processing engine - and I'm hoping that more processing engines will integrate with Fluss eventually.\n\nFluss is a very young project, it was donated to Apache Software Foundation this summer, but there's already a first success story by Taobao.\n\nHave you head about the project? Does it look like something that might help in your environment?", "date_utc": 1760690138.0, "title": "Iceberg support in Apache Fluss - first demo", "upvote_ratio": 0.92, "score": 10, "url": "https://youtu.be/a6MG4f0Ko_g"}, {"id": "1o93iwl", "name": "t3_1o93iwl", "content": "Wondering if anybody has experienced this type of migration to Fabric. I have met with Microsoft numerous times and have not gotten a straight answer. \n\nFor a long time we have had the BI tool decoupled from the ETL/Warehouse and we are used to be able to refresh models and re-run ETL/Pipelines or scripts in the DB in parallel, the DW300c size warehouse is independent from the \"current\" Power BI capacity. we have a large number of users, and I'm really skeptical that a P1 (F64) capacity will suffice for all our data related activities.\n\nWhat has been your experience so far?  To me migrating the models/dashboards sounds straightforward but sticking everything in Fabric (all-in-one platform) sounds scary to me, I have not had the chance to POC it myself to discard the \"resource contention\" problem. We can scale up/down in Synapse without worrying if it's going to break any Power BI related activities.\n\nI decided to post it here because looking up online is just a bunch of consulting firms trying to sell the \"product\". I want the real thing  . Thanks for your time in advance!!!", "date_utc": 1760712919.0, "title": "Power BI + Azure Synapse to Fabric migration", "upvote_ratio": 0.67, "score": 2, "url": "https://www.reddit.com/r/dataengineering/comments/1o93iwl/power_bi_azure_synapse_to_fabric_migration/"}, {"id": "1o8td28", "name": "t3_1o8td28", "content": "Hi, how do u deal with Date columns which have valid dates before 1900-01-01? I have a Date column as Decimal(8, 0) which i want to convert to Date column, but a lot of the values are valid dates before 1900-01-01, which CH cant support, what do u do with this? Why is this even behavior?", "date_utc": 1760680390.0, "title": "ClickHouse Date and DateTime types", "upvote_ratio": 0.74, "score": 5, "url": "https://www.reddit.com/r/dataengineering/comments/1o8td28/clickhouse_date_and_datetime_types/"}, {"id": "1o93vum", "name": "t3_1o93vum", "content": "What features would you want in your Lakehouse catalog? What features you like in existing solutions?", "date_utc": 1760713697.0, "title": "Lakehouse Catalog Feature Dream List", "upvote_ratio": 0.5, "score": 0, "url": "https://www.reddit.com/r/dataengineering/comments/1o93vum/lakehouse_catalog_feature_dream_list/"}, {"id": "1o8yia8", "name": "t3_1o8yia8", "content": "Hey all, I wanted to check regarding a Data Engineer role in M&S Digital UK.\nWould love to know from people who\u2019ve been there in Data teams what\u2019s the culture like, how\u2019s the team, and what should I look forward to?", "date_utc": 1760699739.0, "title": "Any experiences with Marks and Spencer UK Digital (Data Engineer role)?", "upvote_ratio": 0.76, "score": 2, "url": "https://www.reddit.com/r/dataengineering/comments/1o8yia8/any_experiences_with_marks_and_spencer_uk_digital/"}, {"id": "1o8tn2c", "name": "t3_1o8tn2c", "content": "Hey\nI\u2019ve written a full Databricks Serverless blueprint on GCP (europe-west1) and would really appreciate your technical feedback and real-world insights.\nThe architecture includes:\n\u2022 1 single GCP project with 3 Databricks workspaces (dev / preprod / prod)\n\u2022 Unity Catalog for governance and environment isolation\n\u2022 GitHub Actions CI/CD (linting, testing, automated deploys, manual gate for prod)\n\u2022 Terraform for infra (buckets, workspaces, catalogs)\n\u2022 Databricks Workflows for serverless orchestration\n\u2022 A strong focus on security, governance, and FinOps (usage-based billing, auto-termination, tagging)\n\nDoes this setup look consistent with your Databricks/GCP best practices?\nAny real-world feedback on:\n\nrunning serverless compute in production,\n\nmanaging multi-environment governance with Unity Catalog,\n\nor building mature CI/CD with Databricks Asset Bundles?\n\nOpen to any critique or advice \nThanks", "date_utc": 1760681390.0, "title": "Databricks Serverless on GCP", "upvote_ratio": 0.86, "score": 5, "url": "https://www.reddit.com/r/dataengineering/comments/1o8tn2c/databricks_serverless_on_gcp/"}, {"id": "1o8yh1r", "name": "t3_1o8yh1r", "content": "Hi, so I am a fresher and I have been told to do a poc on reading iceberg tables using duckdb. Now I am using duckdb in python to read iceberg tables but so far my attempts have been unsuccessful as the code is not executing. I have tried using iceberg_scan method by creating a secret before that as I cannot provide my aws credentials like access_id_key, etc in my code (as it is a safety breach). I know there are other methods too like using the pyiceberg library in python but I was not able to understand how that works exactly.\nIf anyone has any suggestions or insights or any other methods that could work, please let me know, it would be a great help and I would really appreciate it.\nHope everyone\u2019s doing good:)\n\nEDIT- I was able to execute the code using iceberg_scan successfully without facing any errors.\nNow my senior said to look into using glue catalog for the same thing, if anyone has any suggestions for that, please let me know, thanks :)", "date_utc": 1760699625.0, "title": "Poc on using duckdb to read iceberg tables, and facing a problem with that (help!)", "upvote_ratio": 0.57, "score": 1, "url": "https://www.reddit.com/r/dataengineering/comments/1o8yh1r/poc_on_using_duckdb_to_read_iceberg_tables_and/"}, {"id": "1o99nzs", "name": "t3_1o99nzs", "content": "Is this of any value to anyone? i would love some people to test it. \n\nUses postgres and duckdb on the backend with php/htmx/alpinejs and c# on the backend\n\n  \n[https://instantrows.com](https://instantrows.com)", "date_utc": 1760726840.0, "title": "I built a tool- csv/parquet to API in 30 seconds?", "upvote_ratio": 0.41, "score": 0, "url": "https://v.redd.it/posk8rjrwpvf1"}, {"id": "1o8j9ct", "name": "t3_1o8j9ct", "content": "We put together a demo + guide for a code\u2011first, local-first CDC pipeline to ClickHouse using Debezium, Redpanda, and MooseStack as the dx/glue layer.   \n  \nWhat the demo shows:\n\n* Spin up ClickHouse, Postgres, Debeizum, and Redpanda locally in a single command\n* Pull Debezium managed Redpanda topics directly into code\n* Add stateless streaming transformations on the CDC payloads via Kafka consumer\n* Define/manage ClickHouse tables in code and use them as the sink for the CDC stream\n\nBlog: [https://www.fiveonefour.com/blog/cdc-postgres-to-clickhouse-debezium-drizzle](https://www.fiveonefour.com/blog/cdc-postgres-to-clickhouse-debezium-drizzle) \u2022 Repo:[ https://github.com/514-labs/debezium-cdc](https://github.com/514-labs/debezium-cdc)\n\n*(Disclosure: we work on MooseStack. ClickPipes is great for managed\u2014this is the code\u2011first path.)*    \n\n\nRight now the demo solely focuses on the local dev experience, looking for input from this community on best practices for running Debezium in production (operational patterns, scaling, schema evolution, failure recovery, etc.).", "date_utc": 1760651116.0, "title": "Code\u2011first Postgres\u2192ClickHouse CDC with Debezium + Redpanda + MooseStack (demo + write\u2011up)", "upvote_ratio": 0.7, "score": 5, "url": "https://github.com/514-labs/debezium-cdc"}, {"id": "1o80e49", "name": "t3_1o80e49", "content": "I'm the lead software engineer and architect at a very small startup, and have also thrown my hat into the ring to build business intelligence reports.\n\nThe platform is 100% AWS, so my  approach was AWS Glue to S3 and finally Quicksight. \n\nWe're at the point of scaling up, and I'm keen to understand where my current approach is going to fail.\n\nShould I continue on the current path or look into more specialized tools and workflows?\n\nCost is a factor, ao I can't just tell my boss I want to migrate the whole thing to Databricks.. I also don't have any specific data engineering experience, but have good SQL and general programming skills", "date_utc": 1760602162.0, "title": "Accidentally Data Engineer", "upvote_ratio": 0.94, "score": 86, "url": "https://www.reddit.com/r/dataengineering/comments/1o80e49/accidentally_data_engineer/"}, {"id": "1o8luf7", "name": "t3_1o8luf7", "content": "Hi!\n\nAssume I got a huge dataset of crawled internet webpages, and I'd like to transform them page by page doing some kind of filtration, pre-processing, tokenization etc. Let's say that original dataset is stored along some metainformation in form of parquet files in S3.\n\nComing from enterprises, I have some background in Apache ecosystem as well as some older Big Tech MapReduce-kinda data warehouses, so my first idea was to use Spark to define those transformations using some scala/python code and just deal with it in batch processing manner. But before doing it \"classic ETL-style\" way, I decided to check some more modern (trending?) data stacks that are out there.\n\nI learned about Modal. They seem to be claiming about revolutionizing data processing, but I am not sure how exactly the practical usecases of data processing are expressed in them. Therefore, a bunch of questions to the community:\n\n1. They don't provide a notion of \"dataframes\", nor know anything about my input datasets, thus I must be responsible for somehow partitioning of the input into chunks, right? Like, reading slices of parquet file if needed, or coalescing groups of parquet files together before running an actual distributed computation?\n\n2. What about fault-tolerance? Spark has implemented protocols for atomic output commit, how do you expose result of a distributed data processing atomically without producing garbage from restarted jobs when using Modal? Do I, again, implement this manually?\n\n3. Is there any kind of long-running data processing operation state snapshotting? (not saying about individual executors, but rather the application master) If I have a CPU intensive computation running for 24 hours and I close my laptop lid, or the initiator host dies some other way, am I automatically screwed?\n\n4. Are there tweaks like speculative execution, or at least a way how to control/abort individual function executions? It is always a pity to see how 99% of a job finished with high concurrency and last couple of tasks ended up on some faulty host and take eternity to finish. \n\n5. Since they are a cloud service - do you know about their actual scalability limits? I have a computation CPU cluster of \\~25k CPU cores in my company, do they have some comparable fleet? It would be quite stupid to hit into some limitation like \"no more than 1000 cpu cores per user unless you are an enterprise folk paying $20k/month just for a license\"... \n\n6. Also, them being non-opensource also makes it harder to understand what exactly happens under the hood, are there any open-source competitors to them? Or at least a way how to bring them on-premise to my company's fleet?\n\nAnd a more generic question \u2013 has any of you folks ever tried actually processing some huge datasets with them? Right now it looks more like a tool for smaller developer experiments, or for time-slicing GPUs for seconds, but not something that I would use to build a reliable data pipeline over. But maybe I am missing something. \n\nPS I was told Ray also became popular recently, and they seem to be open-source as well, so will check them later as well.", "date_utc": 1760657708.0, "title": "Using modal for transformation of a huge dataset", "upvote_ratio": 0.75, "score": 4, "url": "https://www.reddit.com/r/dataengineering/comments/1o8luf7/using_modal_for_transformation_of_a_huge_dataset/"}, {"id": "1o8tf72", "name": "t3_1o8tf72", "content": "Elusion v7.9.0 has a few additional features for filtering Plots. This time I'm highlighting filtering categorical data.\n\nWhen you click on a Bar, Pie, or Donut chart, you'll get cross-filtering.\n\nTo learn more, check out the GitHub repository: [https://github.com/DataBora/elusion](https://github.com/DataBora/elusion)\n\nhttps://preview.redd.it/0cuocl8d3mvf1.png?width=1106&format=png&auto=webp&s=0c064cd71955385c2e0970e84f66faa64e9244e2\n\n  \n", "date_utc": 1760680601.0, "title": "Elusion v7.9.0 has additional DASHBOARD features", "upvote_ratio": 0.5, "score": 0, "url": "https://www.reddit.com/r/dataengineering/comments/1o8tf72/elusion_v790_has_additional_dashboard_features/"}, {"id": "1o8d1i4", "name": "t3_1o8d1i4", "content": "Unless data is a top priority from your top management which means there will multiple teams having data folks - anayst, engineer, mle, data scientists etc\nOr, you are tech company which is truly data driven what\u2019s the point of working in data in small teams and companies where it is not the focus?\nCoz no one\u2019s looking at the dashboards being built, data pipelines optimized or even the business questions being answered using data. \nIt is my assumption but 80% of people working in data fall in the category where data is not a focus, it is a small team or some exec wanted to grow his team hence hired a data team.\nHow do you keep yourself motivated if no one uses what you build?\nI feel like a pivot to either SWE or a business role would make more sense as you are creating something that has utility in most companies.\n\nP.S : Frustrated small team DE", "date_utc": 1760636911.0, "title": "What\u2019s your motivation ?", "upvote_ratio": 0.85, "score": 9, "url": "https://www.reddit.com/r/dataengineering/comments/1o8d1i4/whats_your_motivation/"}, {"id": "1o87r49", "name": "t3_1o87r49", "content": "With Fivetrans getting dbt and Tobiko under it's belt, is there any other consolidation you'd guess is coming sooner or later?", "date_utc": 1760625139.0, "title": "Data Vendors Consolidation Speculation Thread", "upvote_ratio": 0.86, "score": 10, "url": "https://www.reddit.com/r/dataengineering/comments/1o87r49/data_vendors_consolidation_speculation_thread/"}, {"id": "1o8qtsx", "name": "t3_1o8qtsx", "content": "I have been trying to set up Airflow and Spark with Docker. Apparently, the easiest way would usually be to use the Bitnami Spark image. However, this image is no longer freely available, and I can't find any information online on how to properly set up Spark using the regular Spark image. Anyone have any idea on how to make it work with Airflow?", "date_utc": 1760671973.0, "title": "How am i supposed to set up an environment with Airflow and Spark?", "upvote_ratio": 0.6, "score": 1, "url": "https://www.reddit.com/r/dataengineering/comments/1o8qtsx/how_am_i_supposed_to_set_up_an_environment_with/"}, {"id": "1o8whgl", "name": "t3_1o8whgl", "content": "Hey everyone,\n\nI wanted to share a tool I\u2019ve been working on that\u2019s been a total game-changer for comparing product spec sheets.\n\nYou know the pain: downloading multiplePDFs from different vendors or manufacturers, opening each one, manually extracting specs, normalizing units, and then building a comparison table in Excel\u2026 takes hours (sometimes days).\n\nWell, I built something to solve exactly that problem:\n\n1.) Upload multiple PDFs at once.\n\n2.) Automatically extract key specs from each document.\n\n3.) Normalize units and field names across PDFs (so \u201cPower\u201d, \u201cWattage\u201d, and \u201cRated Output\u201d all align) \n\n4.)Generate a sortable, interactive comparison table\n\n5.)Export as CSV/Excel for easy sharing\n\nIt\u2019s designed for engineers, procurement teams, product managers, and anyone who deals with technical PDFs regularly.\n\nI want anyone who is interested and faces these problems regularly to help me validate this tool and comment \"interested\" and leave your opinions and feedback. ", "date_utc": 1760692428.0, "title": "Ai-based specsheet data extraction tool for products.", "upvote_ratio": 0.33, "score": 0, "url": "https://www.reddit.com/r/dataengineering/comments/1o8whgl/aibased_specsheet_data_extraction_tool_for/"}, {"id": "1o7vyxe", "name": "t3_1o7vyxe", "content": "I know this isn't \"directly\" related to data engineering, but I find myself constantly looking to visualize my data while I transform it. Whether part of an EDA process, inspection process, or something else.\n\n  \nI can't stand any of the existing tools, but curious to hear about what your favorite tools are, and why?\n\n  \nAlso, if there is something you would love to see, but doesn't exist, share it here too.", "date_utc": 1760585953.0, "title": "What is your favorite viz tool and why?", "upvote_ratio": 0.98, "score": 43, "url": "https://www.reddit.com/r/dataengineering/comments/1o7vyxe/what_is_your_favorite_viz_tool_and_why/"}, {"id": "1o88ocz", "name": "t3_1o88ocz", "content": "Hey folks! I\u2019m curious how teams actually build anomaly alerting for business metrics when there are many slices (e.g., country \\* prime entity \\* device type \\* app version).\n\nWhat I\u2019m exploring:  \nMAD/robust Z, STL/MSTL, Prophet/ETS, rolling windows, adaptive thresholds, alert grouping.\n\nOne thing I keep running into: the more \u201cadvanced\u201d the detector, the more false positives I get in practice. Ironically, a simple 3-sigma rule often ends up the most stable for us. If you\u2019ve been here too - what actually reduced noise without missing real incidents?", "date_utc": 1760627264.0, "title": "How do you build anomaly alerts for real business metrics with lots of slices?", "upvote_ratio": 1.0, "score": 4, "url": "https://www.reddit.com/r/dataengineering/comments/1o88ocz/how_do_you_build_anomaly_alerts_for_real_business/"}, {"id": "1o8jom8", "name": "t3_1o8jom8", "content": "I know this isn't a sub specifically for technical questions, but I'm really at a loss here. Any guidance would be greatly appreciated.\n\nDisclaimer that this problem is with delta-rs (in Python), not Delta Lake with Databricks.\n\nThe project is simple: We have a Delta table, and we want to update some records.\n\nThe solution: use the merge functionality.\n\n    dt = DeltaTable(\"./table\")\n    updates_df = get_updates()\n    \n    dt.merge(\n        updates_df,\n        predicate=(\n            \"target.pk       = source.pk\"\n            \"AND target.salt = source.salt\"\n            \"AND target.foo  = source.foo\"\n            \"AND target.bar != source.bar\"\n        ),\n        source_alias=\"source\",\n        target_alias=\"target\",\n    ).when_matched_update(\n        updates={\"bar\": \"source.bar\"}\n    ).execute()\n\nThe above code is essentially a simplified version of what I have, but all the core pieces are there. It's quite simple in general. The delta table in `./table` is very very large, but it is partitioned nicely with around 1M records per partition (salted to get the partitions balanced). Overall there's \\~2B records in there, while `updates_df` has 2M.\n\nThe problem is that the merge operation balloons memory **massively** for some reason. I was under the impression that working with partitions would drastically decrease the memory consumption, but no. It eventually OOMs, exceeding 380G. This doesn't make sense. Doing a join on the same column between the two tables with duckdb, I find that there would be \\~120k updates across 120 partitions (there are a little over 500 partitions). For one, duckdb can handle the join just fine, and two, it's working with such a small amount of updates. How is it using so much? The partitioned columns are `pk` and `salt`, which I am using in the predicate, so I don't think it has anything to do with lack of pruning.\n\nIf anyone has any experience with this or the solution is glaringly obvious (never used Delta before), then I'd love to hear your thoughts. Oh and if you're wondering why I don't use a more conventional solution for this - that's not my decision. And even if it were, now I'm just curious at this point.", "date_utc": 1760652154.0, "title": "Large memory consumption where it shouldn't be with delta-rs?", "upvote_ratio": 0.6, "score": 1, "url": "https://www.reddit.com/r/dataengineering/comments/1o8jom8/large_memory_consumption_where_it_shouldnt_be/"}, {"id": "1o7xl3r", "name": "t3_1o7xl3r", "content": "I have been in software leadership positions - VP at Small to medium company, and Director at a large company for last few years and have managed mostly web/mobile related projects and have a very strong hands on experience with architecture and coding in the same. During the time, I have also led some analytics teams which had reporting frameworks and most recently GenAI related projects. Have a good understanding of GenAI LLM integrations. I have basic understanding of models and model architecture but have a good handle on with the recent LLM integration/workflow frameworks like Langchain, Langtrace etc. \n\nCurrently, while looking for a change, I am seeing much more demand in Data which makes total sense to me with the direction industry is heading. I am wondering how should i get myself more framed as a Data engineering leader than the generic engineering leader role. I have done some LinkedIn basic trainings but seems like i will need a little more indepth knowledge as my past hands on experience has been in Java, nodejs and cloud native architectures. \n\nDo you folks have any recommendation on how should i get up to speed, is there a databricks or snowflake level certification which i go for to understand the basic concepts. I don't care whether i clear the exam or not but learning is going to be a key to me.", "date_utc": 1760591352.0, "title": "Data Engineering Playbook for a leader.", "upvote_ratio": 0.89, "score": 19, "url": "https://www.reddit.com/r/dataengineering/comments/1o7xl3r/data_engineering_playbook_for_a_leader/"}, {"id": "1o857sq", "name": "t3_1o857sq", "content": "General problem I'm been considering in the back of my head, when trying to figure out how to make some sort of interactive web UI for various language texts, and allow text annotation, and text editing (to progressively/slowly clean up the text mistakes over time, etc.). But in a way such a way that, if you or someone edits the text down the road, it won't mess up the annotations and stuff like that?\n\nI don't know much about linguistic annotation software (saw [this brief overview of some options](https://www.labellerr.com/blog/text-annotation-labeling-tools/#1-labellerr)), but what I've looked at so far are basically these:\n\n- [Perseus Greek Texts](https://www.perseus.tufts.edu/hopper/text?doc=Perseus%3atext%3a1999.01.0227) (click on individual words to lookup)\n- [Prodigy demo](https://demo.prodi.gy/?=null&view_id=pos_manual) (on of the text annotation tools I could quickly try in basic mode for free)\n- [Logeion](https://logeion.uchicago.edu/articulus) (double click to visit terms anywhere in the text)\n\nBut the general problem I'm getting stuck on in my head is what I was saying, here is a brief example to clarify:\n\n- Say we are working with the Bible text (bunch of books, divided into chapters, divided into verses)\n- The data model I'm considering at this point is a tree of JSON basically, `text_section` can be arbitrarily nested (bible -> book -> chapter), and then at the end are `text_span` in the children (verses here).\n- Say the Bible unicode text is super messy, random artifacts here and there, extra whitespace and punctuation in various spots, overall the text is 90% good quality but could use months or years of fine-tuned polish to clean it up and make it perfect. (Sefaria texts, open-source Hebrew texts, are super-super messy, tons of textual artifacts that could use some love to clean up and stuff eventually over time... for example.).\n- But say you can also annotate the text at any point, creating probably \"selection_ranges\" of text within or across verses, etc.. Then you can label or do whatever to add metadata to those ranges.\n\nProblem is:\n\n- Text is being cleaned up over say a couple years, a few minor tweaks every day.\n- Annotations are being added every day too.\n\nEdge-case is basically this:\n\n- Annotation is added on some selected text\n- Text gets edited (maybe user is not even aware of or focused on the annotation UI at this point, but under the hood the metadata is still there).\n- Editor removes some extra whitespace, and adds a missing word (as they found say by looking at a real manuscript scan).\n- Say the editor added `Newton` to `Isaac`, so whereas before it said `foo    bar <thing>Isaac</thing> ...   baz`, now it says `foo bar <thing>Isaac</thing> Newton baz`.\n- Now the annotation sort of changes meaning, and needs to be redone (this is a terrible example, I tried thinking of what my mind's stumbling on, but can't quite pin it down totally yet).\n- Should say `foo bar <thing>Isaac Newton</thing> baz` let's say (but the editor never sees anything annotation-wise...)\n\nBasically, trying to show that, the annotations can get messed up, and I don't see a systematic way to handle or resolve that if editing the text is also allowed.\n\nYou can imagine other cases where some annotation marks like a phrase or idiom, but then the editor comes and changes the idiom to be something totally different, or just partially different, whatever. Or splits the annotation somehow, etc..\n\nBasically, have apps or anyone figured out generally how to handle this general problem? How to not make it so when you edit, you have to just delete the annotations, but it somehow smart merges, or flags it for double-checking, etc.. Basically there is a lot to think through functionality-wise, and I'm not sure if it's already been done before. It's both a data-modeling problem, and a UI/UX problem. But mainly concerned about the technical data-modeling problem here.", "date_utc": 1760618838.0, "title": "How to implement text annotation and collaborative text editing at the same time?", "upvote_ratio": 0.72, "score": 3, "url": "https://www.reddit.com/r/dataengineering/comments/1o857sq/how_to_implement_text_annotation_and/"}, {"id": "1o7k8av", "name": "t3_1o7k8av", "content": "https://www.reuters.com/business/a16z-backed-data-firms-fivetran-dbt-labs-merge-all-stock-deal-2025-10-13/\n\nFirst they split off Fusion as proprietary and put dbt-core in maintenance mode, now they merged with Fivetran (which has no history of open). Not to mention SQLMesh which will probably get killed off.\n\nIs this the death of OSS dbt? ", "date_utc": 1760555558.0, "title": "Final nail in the coffin of OSS dbt", "upvote_ratio": 0.91, "score": 99, "url": "https://www.reddit.com/r/dataengineering/comments/1o7k8av/final_nail_in_the_coffin_of_oss_dbt/"}, {"id": "1o88lxr", "name": "t3_1o88lxr", "content": "Hello,\n\nI'm seeking help with a bad situation I have with Synapse + Azure storage (ADLS2).\n\nThe situation: I'm forced to use Synapse notebooks for certain data processing jobs; a couple of weeks ago I was asked to create a pipeline to download some financial data from a public repository and output it to Azure storage.\n\nSaid data is very small, a few Megabytes at most. So I first developed the script locally, used Polars for dataframe interface and once I verified everything worked, I put it online.\n\n# Edit\n\nApparently I failed to explain myself since nearly everyone who answered, implicitly thinks I'm an idiot, so while I'm not ruling that option out I'll just simplify:\n\n* I have some code that reads data from an online API and writes it somewhere.\n* The data is a few MBs.\n* I'm using Polars, not Pyspark\n* Locally it runs in one minute.\n* On Synapse it runs in 7 minutes.\n* Yes, I did account for pool spin up time, it takes 7 minutes after the pool is ready.\n* Synapse and storage account are in the same region.\n* I am FORCED to use Synapse notebooks by the organization I'm working for.\n* I don't have details about networking at the moment as I wasn't involved in the setup, I'd have to collect them.\n\nNow I understand that data transfer goes over the network, so it's gotta be slower than writing to disk, but what the fuck? 5 to 10 times slower is insane, for such a small amount of data.\n\nThis also makes me think that the Spark jobs that run in the same environment would be MUCH faster in a different setup.\n\nSo this said, the question is, is there anything I can do to speed up this shit?\n\n# Edit 2\n\nUnder suggestion of some of you I then profiled every component of the pipeline, which eventually confirmed the suspicion that the bottleneck is in the I/O part.\n\nHere's the relevant profiling results if anyone is interested:\n\n\n### local\n\n```\n_write_parquet:\n  Calls: 1713\n  Total: 52.5928s\n  Avg:   0.0307s\n  Min:   0.0003s\n  Max:   1.0037s\n\n_read_parquet (this is an extra step used for data quality check):\n  Calls: 1672\n  Total: 11.3558s\n  Avg:   0.0068s\n  Min:   0.0004s\n  Max:   0.1180s\n\ndownload_zip_data:\n  Calls: 22\n  Total: 44.7885s\n  Avg:   2.0358s\n  Min:   1.6840s\n  Max:   2.2794s\n\nunzip_data:\n  Calls: 22\n  Total: 1.7265s\n  Avg:   0.0785s\n  Min:   0.0577s\n  Max:   0.1197s\n\nread_csv:\n  Calls: 2074\n  Total: 17.9278s\n  Avg:   0.0086s\n  Min:   0.0004s\n  Max:   0.0410s\n\ntransform (includes read_csv time):\n  Calls: 846\n  Total: 20.2491s\n  Avg:   0.0239s\n  Min:   0.0012s\n  Max:   0.2056s\n```\n\n### synapse\n\n```\n_write_parquet:\n  Calls: 1713\n  Total: 848.2049s\n  Avg:   0.4952s\n  Min:   0.0428s\n  Max:   15.0655s\n\n_read_parquet:\n  Calls: 1672\n  Total: 346.1599s\n  Avg:   0.2070s\n  Min:   0.0649s\n  Max:   10.2942s\n\ndownload_zip_data:\n  Calls: 22\n  Total: 14.9234s\n  Avg:   0.6783s\n  Min:   0.6343s\n  Max:   0.7172s\n\nunzip_data:\n  Calls: 22\n  Total: 5.8338s\n  Avg:   0.2652s\n  Min:   0.2044s\n  Max:   0.3539s\n\nread_csv:\n  Calls: 2074\n  Total: 70.8785s\n  Avg:   0.0342s\n  Min:   0.0012s\n  Max:   0.2519s\n\ntransform (includes read_csv time):\n  Calls: 846\n  Total: 82.3287s\n  Avg:   0.0973s\n  Min:   0.0037s\n  Max:   1.0253s\n```\n\n### context:\n\n`_write_parquet`: writes to local storage or adls.\n\n`_read_parquet`: reads from local storage or adls.\n\n`download_zip_data`: downloads the data from the public source to a local `/tmp/data` directory. Same code for both environments.\n\n`unzip_data`: unpacks the content of downloaded zips under the same local directory. The content is a bunch of CSV files. Same code for both environments.\n\n`read_csv`: Reads the CSV data from local `/tmp/data`. Same code for both environments.\n\n`transform`: It calls `read_csv` several times so the actual wall time of just the transformation is its total minus the total time of read_csv. Same code for both environments.\n\n\\---\n\nold message:\n\n>!~~The problem was in the run times. For the same exact code and data:~~!<\n\n* >!~~Locally, writing data to disk, took about 1 minute~~!<\n* >!~~On Synapse notebook, writing data to ADLS2 took about 7 minutes~~!<\n\n>!~~Later on I had to add some data quality checks to this code and the situation became even worse:~~!<\n\n* >!~~Locally only took 2 minutes.~~!<\n* >!~~On Synapse notebook, it took 25 minutes.~~!<\n\n>!~~Remember, we're talking about a FEW Megabytes of data. Under suggestion of my team lead I tried to change destination an used a blob storage of premium tier (this one in the red).~~!<\n\n>!~~It did have some improvements, but only went down to about 10 minutes run (vs again the 2 mins local).~~!<", "date_utc": 1760627109.0, "title": "Is Azure blob storage slow as fuck?", "upvote_ratio": 0.5, "score": 0, "url": "https://www.reddit.com/r/dataengineering/comments/1o88lxr/is_azure_blob_storage_slow_as_fuck/"}, {"id": "1o8ejxi", "name": "t3_1o8ejxi", "content": "", "date_utc": 1760640242.0, "title": "Practical Guide to Semantic Layers: Your MCP-Powered AI Analyst (Part 2)", "upvote_ratio": 0.5, "score": 0, "url": "https://open.substack.com/pub/rasmusengelbrecht/p/practical-guide-to-semantic-layers-34d?r=1gpztg&utm_medium=ios"}, {"id": "1o7xer0", "name": "t3_1o7xer0", "content": "", "date_utc": 1760590744.0, "title": "7 Best Free Data Engineering Courses", "upvote_ratio": 0.79, "score": 11, "url": "https://www.mltut.com/best-free-data-engineering-courses/"}, {"id": "1o8apr1", "name": "t3_1o8apr1", "content": "So the idea is that we get weather data with reference time and forecast time with a frequency of 6 hours and customer data with a frequency of 15 minutes. Consider also that there 5 weather data sources and many customers i.e. 100.\nThere are some options I have thought of:\n1. Storing as parquet files in gcs in a hive structure bucket/customer_id/source/year/month/day/hour. With duckDB on top to query these files.\n2. Postgres with a single table hash partiotioned by customer id with fields: reference time, forecast time, customer id, nwp source, features as JSON.\nHaving difficulties in wrapping up my head over the pros and cons of these options. Any suggestions would be helpful.", "date_utc": 1760631843.0, "title": "How would you handle nwp data and customer data both time series with different frequencies for a data warehouse?", "upvote_ratio": 0.67, "score": 1, "url": "https://www.reddit.com/r/dataengineering/comments/1o8apr1/how_would_you_handle_nwp_data_and_customer_data/"}, {"id": "1o8208j", "name": "t3_1o8208j", "content": "Hi I'm Steve.\n\nOur organization running dbt-core locally and want to move it into production\n\nWe already use Airflow on Kubernetes and CI/CD vis GitHub Actions.\n\nCurious what others do - run dbt inside Airflow DAGs? or just let CI/CD handle it separately?\n\nAny pros/cons you've seen in production?\n\nAdditional.\n\nWe are using...\n\nApache Airflow 2.7.3 (running in Kubernetes)\n\ndbt-core 1.9.1 (Just test, run in local environment)\n\nAnd we have two repositories:\n\n* One for Apache Airflow DAGs\n* One for dbt-core\n\nWould you recommend we have to integrate them or keeping seperate?\n\nI wish you guys help us :)", "date_utc": 1760608534.0, "title": "Expanding a local dbt-core project to production \u2014 should I integrate with Airflow or rely on CI/CD + Pre-Prod?", "upvote_ratio": 0.75, "score": 4, "url": "https://www.reddit.com/r/dataengineering/comments/1o8208j/expanding_a_local_dbtcore_project_to_production/"}, {"id": "1o84cs6", "name": "t3_1o84cs6", "content": "hey guys! so, i have an assessment to do in the next 4 days regarding a job position, for a junior data engineer role. i\u2019ve never had to do one so idk what is the best place to find material to study and train\ndo you guys recommend anything? any website or material? i believe the test will be focused on pyspark and sql\nps.: i\u2019m more interested in websites with practical questions and answers than theory :)", "date_utc": 1760616392.0, "title": "data engineering test", "upvote_ratio": 0.5, "score": 0, "url": "https://www.reddit.com/r/dataengineering/comments/1o84cs6/data_engineering_test/"}, {"id": "1o7eicq", "name": "t3_1o7eicq", "content": "I have 15 years of experience as a Data Engineer, mostly in investment banking, working with ETL pipelines, Snowflake, SQL, Spark, Python, and Shell scripting.\n\nLately, my role has shifted more toward strategy and less hands-on engineering. While my firm is modernizing its data stack, I find that the type of work I\u2019m doing no longer aligns with where I want to grow technically.\n\nI realize the job market is competitive, and I haven\u2019t applied for any roles in the past five years, which feels daunting. I also worry that my hands-on skills are getting rusty, as I often rely on tools like Copilot to assist with development.\n\nQuestions:\n\n1. What emerging tools or skills should I focus on to stay relevant as a senior data engineer in 2025\u201326?\n\n\n2. How do you recommend practicing technical skills and market readiness after being out of the job market for a while?\n\n\nAny advice from fellow senior data engineers or those in banking/finance tech would be greatly appreciated!", "date_utc": 1760542896.0, "title": "Looking for Advice to Stay Relevant technically as a Senior Data Engineer", "upvote_ratio": 0.97, "score": 80, "url": "https://www.reddit.com/r/dataengineering/comments/1o7eicq/looking_for_advice_to_stay_relevant_technically/"}, {"id": "1o88lg6", "name": "t3_1o88lg6", "content": "Hi, I'm new to databricks (Please go easy) and  i'm trying to implement an ETL pipeline for data coming from different sources for end users in our company. Although new data comes in the Azure SQL Database daily basis (we anticipate 10 GB approximately of data on the weekly basis).\n\n  \nWe get also get Files in Landing Zone (ADLS Gen2) on weekly basis (Upto 50 GB).\n\nNow we need to process all of this data weekly.  Currently, i have come up with this medallion architecture:\n\n**Landing to Bronze:**\n\n**-> data in azure sql source**\n\n\t\\-> Using ADF to copy the files from azure sql (multiple database instances) to bronze. \n\n\t\\-> We have a configuration file from which we know, what is the database, table, the load type (full load/incremental), datasource\n\n\t\\-> We process the data accordingly and also have an audit table where the watermark for tables with incremental load is maintained\n\n\t\\-> Creating delta tables on the bronze (the tables here contain the data source and timestamp columns as well)\n\n**-> data in landing zone**\n\n\\-> using autoloader to copy the files from landing zone to bronze\n\n\t\\-> Landing zone uses a fairly nested structure (files arriving weekly).\t\n\n\\-> Also fetching ICD Codes from athena and saving then to bronze.\n\n\\-> We create delta tables in the bronze layer.\n\n**Silver:**\n\n\\-> From bronze, we read the data into silver. This is incremental using MERGE UPSERT (Is there a better approach)\n\n\\-> We apply Common Data Model in the Silver Layer and Type SCD 2 for dimension tables. Here \n\n\\-> We do the quality checks as well. On failures we halt the pipeline as the data quality is critical to the end user.\n\n\\-> We are also get the data dictionary so schema evolution is handled by using a custom schema registry and compare the current infered schema with the latest schema version we are maintaining. All of these come under the data quality checks. If anyone fail, we send email.\n\n\\-> The schema is checked for the raw files we receive in the ADLS2 Landing Zone.\n\n**Gold:**\n\n\\-> Data is loaded from silver to Gold Layer with predefined data model \n\n\n\nPlease tell me what changes i can make in this approach?", "date_utc": 1760627076.0, "title": "Suggestion needed with Medallion Architecture", "upvote_ratio": 0.56, "score": 1, "url": "https://www.reddit.com/r/dataengineering/comments/1o88lg6/suggestion_needed_with_medallion_architecture/"}, {"id": "1o88isc", "name": "t3_1o88isc", "content": "", "date_utc": 1760626910.0, "title": "Every company is a data company, but most don't know where to start", "upvote_ratio": 0.6, "score": 1, "url": "https://taleshape.com/blog/getting-started-building-a-data-platform/"}, {"id": "1o85f8n", "name": "t3_1o85f8n", "content": "", "date_utc": 1760619397.0, "title": "Intelligent Applications: The Next Step in Data-Driven Decision-Making", "upvote_ratio": 0.43, "score": 0, "url": "https://news.sap.com/2025/10/intelligent-applications-data-driven-decision-making/"}, {"id": "1o7tcyz", "name": "t3_1o7tcyz", "content": "Hi everyone! I'm doing data collection for a class, and it would be amazing if you guys could fill this out for me! (it's anonymous). Thank you so much!!!\n\n[https://forms.gle/zjFdkprPyFWv5Utx6](https://forms.gle/zjFdkprPyFWv5Utx6)", "date_utc": 1760578227.0, "title": "Data Collecting", "upvote_ratio": 0.88, "score": 6, "url": "https://www.reddit.com/r/dataengineering/comments/1o7tcyz/data_collecting/"}, {"id": "1o83jwp", "name": "t3_1o83jwp", "content": "I run a huge data product that gives information on the revenue and susbcriber numbers of most major video services in the world (e.g. Netflix) on a by-country basis.\n\ncurrently this is split across 14 siloed Google Sheets , that are largely not linked with eachother (except for some core demographic data which all points to another single sheet). There are 2 Google Sheets for each of the 7 global regions we cover, with a tab for every country in that region, as well as summary tabs for every data point.\n\nthis seems like a crazily inefficient way to run a model this size but I don't have a background in data and am unsure how I could improve the process. any ideas? could learning SQL (or anything else) help me? ", "date_utc": 1760613908.0, "title": "Best way to run a detailed global market model - Google Sheets?", "upvote_ratio": 0.4, "score": 0, "url": "https://www.reddit.com/r/dataengineering/comments/1o83jwp/best_way_to_run_a_detailed_global_market_model/"}]